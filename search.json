[{"path":"https://stochastictree.github.io/stochtree-r/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 stochtree authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Bayesian-Supervised-Learning.html","id":"simulation","dir":"Articles","previous_headings":"Demo 1: Step Function","what":"Simulation","title":"Bayesian Supervised Learning in StochTree","text":", generate data simple step function.","code":"# Generate the data n <- 500 p_x <- 10 snr <- 3 X <- matrix(runif(n*p_x), ncol = p_x) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- sd(f_XW) / snr y <- f_XW + rnorm(n, 0, 1)*noise_sd  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] W_test <- NULL W_train <- NULL y_test <- y[test_inds] y_train <- y[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Bayesian-Supervised-Learning.html","id":"warmstart","dir":"Articles","previous_headings":"Demo 1: Step Function > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"first simulate ensemble model \\(y \\mid X\\) using “warm-start” initialization samples (Hahn (2023)). default stochtree. Inspect initial XBART “warm-start” samples   Inspect BART samples initialized XBART warm-start","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_model_warmstart <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test, leaf_model = 0,      num_trees = 100, num_gfr = num_gfr, num_burnin = num_burnin,      num_mcmc = num_mcmc, sample_sigma = T, sample_tau = T ) plot(bart_model_warmstart$sigma2_samples[1:num_gfr], ylab=\"sigma^2\") plot(rowMeans(bart_model_warmstart$yhat_test[,1:num_gfr]), y_test, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(bart_model_warmstart$sigma2_samples[(num_gfr + 1):num_samples], ylab=\"sigma^2\") plot(rowMeans(bart_model_warmstart$yhat_test[,(num_gfr + 1):num_samples]), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Bayesian-Supervised-Learning.html","id":"bart-mcmc-without-warmstart","dir":"Articles","previous_headings":"Demo 1: Step Function > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin.","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_model_root <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test, leaf_model = 0,      num_trees = 100, num_gfr = num_gfr, num_burnin = num_burnin,      num_mcmc = num_mcmc, sample_sigma = T, sample_tau = T ) plot(bart_model_root$sigma2_samples[(num_burnin + 1):num_samples], ylab=\"sigma^2\") plot(rowMeans(bart_model_root$yhat_test[,(num_burnin + 1):num_samples]), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Bayesian-Supervised-Learning.html","id":"simulation-1","dir":"Articles","previous_headings":"Demo 2: Partitioned Linear Model","what":"Simulation","title":"Bayesian Supervised Learning in StochTree","text":", generate data simple partitioned linear model.","code":"# Generate the data n <- 500 p_x <- 10 p_w <- 1 snr <- 3 X <- matrix(runif(n*p_x), ncol = p_x) W <- matrix(runif(n*p_w), ncol = p_w) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5*W[,1]) ) noise_sd <- sd(f_XW) / snr y <- f_XW + rnorm(n, 0, 1)*noise_sd  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] W_test <- W[test_inds,] W_train <- W[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Bayesian-Supervised-Learning.html","id":"warmstart-1","dir":"Articles","previous_headings":"Demo 2: Partitioned Linear Model > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"first simulate ensemble model \\(y \\mid X\\) using “warm-start” initialization samples (Hahn (2023)). default stochtree. Inspect initial XBART “warm-start” samples   Inspect BART samples initialized XBART warm-start","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_model_warmstart <- stochtree::bart(     X_train = X_train, W_train = W_train, y_train = y_train,      X_test = X_test, W_test = W_test, leaf_model = 1,      num_trees = 100, num_gfr = num_gfr, num_burnin = num_burnin,      num_mcmc = num_mcmc, sample_sigma = T, sample_tau = T ) plot(bart_model_warmstart$sigma2_samples[1:num_gfr], ylab=\"sigma^2\") plot(rowMeans(bart_model_warmstart$yhat_test[,1:num_gfr]), y_test, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(bart_model_warmstart$sigma2_samples[(num_gfr + 1):num_samples], ylab=\"sigma^2\") plot(rowMeans(bart_model_warmstart$yhat_test[,(num_gfr + 1):num_samples]), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Bayesian-Supervised-Learning.html","id":"bart-mcmc-without-warmstart-1","dir":"Articles","previous_headings":"Demo 2: Partitioned Linear Model > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin.","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_model_root <- stochtree::bart(     X_train = X_train, W_train = W_train, y_train = y_train,      X_test = X_test, W_test = W_test, leaf_model = 1,      num_trees = 100, num_gfr = num_gfr, num_burnin = num_burnin,      num_mcmc = num_mcmc, sample_sigma = T, sample_tau = T ) plot(bart_model_root$sigma2_samples[(num_burnin + 1):num_samples], ylab=\"sigma^2\") plot(rowMeans(bart_model_root$yhat_test[,(num_burnin + 1):num_samples]), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"demo-1-nonlinear-outcome-model-heterogeneous-treatment-effect","dir":"Articles","previous_headings":"","what":"Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process Hahn, Murray, Carvalho (2020): \\[\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= 1 + g(X) + 6 \\lvert X_3 - 1 \\rvert\\\\ \\tau(X) &= 1 + 2 X_2 X_4\\\\ g(X) &= \\mathbb{}(X_5=1) \\times 2 - \\mathbb{}(X_5=2) \\times 1 - \\mathbb{}(X_5=3) \\times 4\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim \\text{Bernoulli}(1/2)\\\\ X_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\ \\end{aligned} \\end{equation*}\\]","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"simulation","dir":"Articles","previous_headings":"Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=T)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"warmstart","dir":"Articles","previous_headings":"Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model \\(y \\mid X\\) using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_gfr+1):num_samples plot(rowMeans(bcf_model_warmstart$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_warmstart$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.98"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"bart-mcmc-without-warmstart","dir":"Articles","previous_headings":"Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_burnin+1):num_samples plot(rowMeans(bcf_model_root$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_root$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.98"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"demo-2-linear-outcome-model-heterogeneous-treatment-effect","dir":"Articles","previous_headings":"","what":"Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process Hahn, Murray, Carvalho (2020): \\[\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= 1 + g(X) + 6 X_1 X_3\\\\ \\tau(X) &= 1 + 2 X_2 X_4\\\\ g(X) &= \\mathbb{}(X_5=1) \\times 2 - \\mathbb{}(X_5=2) \\times 1 - \\mathbb{}(X_5=3) \\times 4\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim \\text{Bernoulli}(1/2)\\\\ X_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\ \\end{aligned} \\end{equation*}\\]","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"simulation-1","dir":"Articles","previous_headings":"Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=T)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu2(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"warmstart-1","dir":"Articles","previous_headings":"Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model \\(y \\mid X\\) using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_gfr+1):num_samples plot(rowMeans(bcf_model_warmstart$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_warmstart$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.73"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"bart-mcmc-without-warmstart-1","dir":"Articles","previous_headings":"Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_burnin+1):num_samples plot(rowMeans(bcf_model_root$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_root$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.95"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"demo-3-linear-outcome-model-homogeneous-treatment-effect","dir":"Articles","previous_headings":"","what":"Demo 3: Linear Outcome Model, Homogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process Hahn, Murray, Carvalho (2020): \\[\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= 1 + g(X) + 6 X_1 X_3\\\\ \\tau(X) &= 3\\\\ g(X) &= \\mathbb{}(X_5=1) \\times 2 - \\mathbb{}(X_5=2) \\times 1 - \\mathbb{}(X_5=3) \\times 4\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim \\text{Bernoulli}(1/2)\\\\ X_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\ \\end{aligned} \\end{equation*}\\]","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"simulation-2","dir":"Articles","previous_headings":"Demo 3: Linear Outcome Model, Homogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=T)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu2(X) tau_x <- tau1(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"warmstart-2","dir":"Articles","previous_headings":"Demo 3: Linear Outcome Model, Homogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model \\(y \\mid X\\) using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_gfr+1):num_samples plot(rowMeans(bcf_model_warmstart$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_warmstart$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 1"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"bart-mcmc-without-warmstart-2","dir":"Articles","previous_headings":"Demo 3: Linear Outcome Model, Homogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_burnin+1):num_samples plot(rowMeans(bcf_model_root$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_root$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 1"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"demo-4-nonlinear-outcome-model-heterogeneous-treatment-effect","dir":"Articles","previous_headings":"","what":"Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process: \\[\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= \\begin{cases} -1.1 & \\text{ } X_1 > X_2\\\\ 0.9 & \\text{ } X_1 \\leq X_2 \\end{cases}\\\\ \\tau(X) &= \\frac{1}{1+\\exp(-X_3)} + \\frac{X_2}{10}\\\\ \\pi(X) &= \\Phi\\left(\\mu(X)\\right)\\\\ Z &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right)\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim N\\left(X_2,1\\right)\\\\ \\end{aligned} \\end{equation*}\\]","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"simulation-3","dir":"Articles","previous_headings":"Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 1000 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- rnorm(n,x2,1) X <- cbind(x1,x2,x3,x4) p <- ncol(X) mu <- function(x) {-1*(x[,1]>(x[,2])) + 1*(x[,1]<(x[,2])) - 0.1} tau <- function(x) {1/(1 + exp(-x[,3])) + x[,2]/10} mu_x <- mu(X) tau_x <- tau(X) pi_x <- pnorm(mu_x) Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x sigma <- diff(range(mu_x + tau_x*pi))/8 y <- E_XZ + sigma*rnorm(n)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"warmstart-3","dir":"Articles","previous_headings":"Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model \\(y \\mid X\\) using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_gfr+1):num_samples plot(rowMeans(bcf_model_warmstart$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_warmstart$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 1"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Causal-Inference.html","id":"bart-mcmc-without-warmstart-3","dir":"Articles","previous_headings":"Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, feature_types = c(0,0,0,1,1),      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F ) sample_inds <- (num_burnin+1):num_samples plot(rowMeans(bcf_model_root$mu_hat_test[,sample_inds]), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test[,sample_inds]), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed)),                   max(c(bcf_model_root$sigma2_samples[sample_inds], sigma_observed))) plot(bcf_model_root$sigma2_samples[sample_inds], ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.995"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"Prototype Interface in StochTree","text":"functions bart() bcf() provide simple performant interfaces supervised learning / causal inference, stochtree also offers access many “low-level” data structures typically implemented C++. low-level interface designed performance even simplicity — rather intent provide “prototype” interface C++ code doesn’t require modifying C++. illustrate prototype interface might useful, consider classic BART algorithm:    INPUT: \\(y\\), \\(X\\), \\(\\tau\\), \\(\\nu\\), \\(\\lambda\\), \\(\\alpha\\), \\(\\beta\\)    OUTPUT: \\(m\\) samples decision forest \\(k\\) trees global variance parameter \\(\\sigma^2\\)    Initialize \\(\\sigma^2\\) via default data-dependent calibration exercise    Initialize “forest 0” \\(k\\) trees single root node, referring tree \\(j\\)’s prediction vector \\(f_{0,j}\\)    Compute residual \\(r = y - \\sum_{j=1}^k f_{0,j}\\)    \\(\\) \\(\\left\\{1,\\dots,m\\right\\}\\):       Initialize forest \\(\\) forest \\(-1\\)       \\(j\\) \\(\\left\\{1,\\dots,k\\right\\}\\):          Add predictions tree \\(j\\) residual: \\(r = r + f_{,j}\\)          Update tree \\(j\\) via Metropolis-Hastings \\(r\\) \\(X\\) data tree priors depending (\\(\\tau\\), \\(\\sigma^2\\), \\(\\alpha\\), \\(\\beta\\))          Sample leaf node parameters tree \\(j\\) via Gibbs (leaf node prior \\(N\\left(0,\\tau\\right)\\))          Subtract (updated) predictions tree \\(j\\) residual: \\(r = r - f_{,j}\\)       Sample \\(\\sigma^2\\) via Gibbs (prior \\(IG(\\nu/2,\\nu\\lambda/2)\\)) algorithm conceptually simple, much core computation carried low-level languages C C++ tree data structure. result, changes algorithm, supporting heteroskedasticity (Pratola et al. (2020)), categorical outcomes (Murray (2021)) causal effect estimation (Hahn, Murray, Carvalho (2020)) require modifying low-level code. prototype interface exposes core components loop R level, thus making possible interchange C++ computation steps like “update tree \\(j\\) via Metropolis-Hastings” R computation custom variance model, user-specified additive mean model components, . begin, load stochtree package","code":"library(stochtree)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"simulation","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Simulation","title":"Prototype Interface in StochTree","text":"Simulate simple partitioned linear model","code":"# Generate the data n <- 500 p_X <- 10 p_W <- 1 X <- matrix(runif(n*p_X), ncol = p_X) W <- matrix(runif(n*p_W), ncol = p_W) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-3*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-1*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (1*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3*W[,1]) ) y <- f_XW + rnorm(n, 0, 1)  # Standardize outcome y_bar <- mean(y) y_std <- sd(y) resid <- (y-y_bar)/y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"sampling","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Sampling","title":"Prototype Interface in StochTree","text":"Set parameters inform forest variance parameter samplers Initialize R-level access C++ classes needed sample model Prepare run sampler Run grow--root sampler “warm-start” BART Pick last GFR forest (associated global variance / leaf scale parameters) MCMC sampler Predict rescale samples","code":"alpha <- 0.9 beta <- 1.25 min_samples_leaf <- 1 num_trees <- 100 cutpoint_grid_size = 100 global_variance_init = 1. tau_init = 0.5 leaf_prior_scale = matrix(c(tau_init), ncol = 1) nu <- 4 lambda <- 0.5 a_leaf <- 2. b_leaf <- 0.5 leaf_regression <- T feature_types <- as.integer(rep(0, p_X)) # 0 = numeric var_weights <- rep(1/p_X, p_X) # Data if (leaf_regression) {     forest_dataset <- createForestDataset(X, W)     outcome_model_type <- 1 } else {     forest_dataset <- createForestDataset(X)     outcome_model_type <- 0 } outcome <- createOutcome(resid)  # Random number generator (std::mt19937) rng <- createRNG()  # Sampling data structures forest_model <- createForestModel(forest_dataset, feature_types,                                    num_trees, n, alpha, beta, min_samples_leaf)  # Container of forest samples if (leaf_regression) {     forest_samples <- createForestContainer(num_trees, 1, F) } else {     forest_samples <- createForestContainer(num_trees, 1, T) } num_warmstart <- 10 num_mcmc <- 100 num_samples <- num_warmstart + num_mcmc global_var_samples <- c(global_variance_init, rep(0, num_samples)) leaf_scale_samples <- c(tau_init, rep(0, num_samples)) for (i in 1:num_warmstart) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          global_var_samples[i], cutpoint_grid_size, gfr = T     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1] } for (i in (num_warmstart+1):num_samples) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          global_var_samples[i], cutpoint_grid_size, gfr = F     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1] } # Forest predictions preds <- forest_samples$predict(forest_dataset)*y_std + y_bar  # Global error variance sigma_samples <- sqrt(global_var_samples)*y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"results","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Results","title":"Prototype Interface in StochTree","text":"Inspect initial samples obtained via “grow--root” (Hahn (2023))   Inspect BART samples obtained “warm-starting”","code":"plot(sigma_samples[1:num_warmstart], ylab=\"sigma\") plot(rowMeans(preds[,1:num_warmstart]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(sigma_samples[(num_warmstart+1):num_samples], ylab=\"sigma\") plot(rowMeans(preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"demo-2-causal-inference","dir":"Articles","previous_headings":"","what":"Demo 2: Causal Inference","title":"Prototype Interface in StochTree","text":"show implement Bayesian Causal Forest (BCF) model Hahn, Murray, Carvalho (2020) using stochtree’s prototype API, including demoing non-trivial sampling step done R level.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"background","dir":"Articles","previous_headings":"Demo 2: Causal Inference","what":"Background","title":"Prototype Interface in StochTree","text":"supervised learning case previous demo conceptually simple, motivate causal effect estimation task additional notation. Let \\(y\\) refer continuous outcome interest, \\(Z\\) refer binary treatment, \\(X\\) set covariates may influence \\(Y\\), \\(Z\\), . \\(X\\) exhaustive set covariates influence \\(Z\\) \\(Y\\), can specific \\(Y\\) terms causal model (see example Pearl (2009)) \\[\\begin{equation*} \\begin{aligned} Y &= F(Z, X, \\epsilon_Y) \\end{aligned} \\end{equation*}\\] \\(\\epsilon_Y\\) outcome specific random noise \\(F\\) function generates \\(Y\\) (many cases, \\(F\\) can thought inverse CDF conditional \\(X\\) \\(Z\\)). “potential outcomes” (see Imbens Rubin (2015)) can recovered \\(Y^1 = F(1, X, \\epsilon_Y)\\) \\(Y^0 = F(0, X, \\epsilon_Y)\\). causal outcome model can decomposed “mean” “error” terms \\[\\begin{equation*} \\begin{aligned} Y &= \\mu(X) + Z\\tau(X) + \\left[\\eta(X) + Z\\delta(X)\\right]\\\\ \\mu(X) &= \\mathbb{E}_{\\epsilon_Y}\\left[F(0, X, \\epsilon_Y)\\right]\\\\ \\tau(X) &= \\mathbb{E}_{\\epsilon_Y}\\left[F(1, X, \\epsilon_Y) - F(0, X, \\epsilon_Y)\\right]\\\\ \\eta(X) &= F(0, X, \\epsilon_Y) - \\mathbb{E}_{\\epsilon_Y}\\left[F(0, X, \\epsilon_Y)\\right]\\\\ \\delta(X) &= F(1, X, \\epsilon_Y) - F(0, X, \\epsilon_Y) - \\mathbb{E}_{\\epsilon_Y}\\left[F(1, X, \\epsilon_Y) - F(0, X, \\epsilon_Y)\\right] \\end{aligned} \\end{equation*}\\] \\(\\tau(X)\\) precisely conditional average treatment effect (CATE) estimand. Unfortunately, functional form \\(F\\) unavailable analysis, \\(\\tau(X)\\) derived. flexible, regularized nonparametrics enter picture, aim estimate \\(\\mu(X)\\) \\(\\tau(X)\\) data.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"bayesian-causal-forest-bcf","dir":"Articles","previous_headings":"Demo 2: Causal Inference > Background","what":"Bayesian Causal Forest (BCF)","title":"Prototype Interface in StochTree","text":"BCF estimates \\(\\mu(X)\\) \\(\\tau(X)\\) using separate BART forests term. Furthermore, rather rely common implicit coding \\(Z\\) 0 control observations 1 treated observations, consider coding control observations parameter \\(b_0\\) treated observations parameter \\(b_1\\). Placing \\(N(0,1/2)\\) prior \\(b_z\\), essentially redefines outcome model \\[\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) f(Z) + \\epsilon\\\\ f(Z) &= b_0(1-Z) + b_1 Z\\\\ \\epsilon &\\sim N\\left(0, \\sigma^2\\right)\\\\ b_0, b_1 &\\sim N\\left(0, 1/2\\right) \\end{aligned} \\end{equation*}\\] Updating \\(b_z\\) requires additional Gibbs step, derive . Conditioning sampled forests \\(\\mu\\) \\(\\tau\\), essentially regressing \\(y - \\mu(Z)\\) \\(\\left[(1-Z)\\tau(X), Z\\tau(X)\\right]\\) closed form posterior \\[\\begin{equation*} \\begin{aligned} b_0 \\mid y, X, \\mu,\\tau &\\sim N\\left(\\frac{s_{y\\tau,0}}{s_{\\tau\\tau,0} + 2\\sigma^2}, \\frac{\\sigma^2}{s_{\\tau\\tau,0} + 2\\sigma^2}\\right)\\\\ b_1 \\mid y, X, \\mu,\\tau &\\sim N\\left(\\frac{s_{y\\tau,1}}{s_{\\tau\\tau,1} + 2\\sigma^2}, \\frac{\\sigma^2}{s_{\\tau\\tau,1} + 2\\sigma^2}\\right) \\end{aligned} \\end{equation*}\\] \\(s_{y\\tau,z} = \\sum_{: Z_i = z} (y_i - \\mu(X_i))\\tau(X_i)\\) \\(s_{\\tau\\tau,z} = \\sum_{: Z_i = z} \\tau(X_i)\\tau(X_i)\\).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"simulation-1","dir":"Articles","previous_headings":"Demo 2: Causal Inference","what":"Simulation","title":"Prototype Interface in StochTree","text":"simulated causal DGP mirrors nonlinear, heterogeneous treatment effect DGP presented Hahn, Murray, Carvalho (2020).","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=T)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 4 y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr)  # Standardize outcome y_bar <- mean(y) y_std <- sd(y) resid <- (y-y_bar)/y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"sampling-1","dir":"Articles","previous_headings":"Demo 2: Causal Inference","what":"Sampling","title":"Prototype Interface in StochTree","text":"Set parameters inform forest variance parameter samplers Prepare run sampler (now must specify initial values \\(b_0\\) \\(b_1\\), choose -1/2 1/2 instead 0 1). Initialize R-level access C++ classes needed sample model Run grow--root sampler “warm-start” BART, also updating adaptive coding parameter \\(b_0\\) \\(b_1\\) Pick last GFR forest (associated global variance / leaf scale parameters) MCMC sampler Predict rescale samples","code":"# Mu forest alpha_mu <- 0.95 beta_mu <- 2.0 min_samples_leaf_mu <- 5 num_trees_mu <- 250 cutpoint_grid_size_mu = 100 tau_init_mu = 1/num_trees_mu leaf_prior_scale_mu = matrix(c(tau_init_mu), ncol = 1) a_leaf_mu <- 3. b_leaf_mu <- var(resid)/(num_trees_mu) leaf_regression_mu <- F sigma_leaf_mu <- var(resid)/(num_trees_mu) current_leaf_scale_mu <- as.matrix(sigma_leaf_mu)  # Tau forest alpha_tau <- 0.25 beta_tau <- 3.0 min_samples_leaf_tau <- 5 num_trees_tau <- 50 cutpoint_grid_size_tau = 100 a_leaf_tau <- 3. b_leaf_tau <- var(resid)/(2*num_trees_tau) leaf_regression_tau <- T sigma_leaf_tau <- var(resid)/(2*num_trees_tau) current_leaf_scale_tau <- as.matrix(sigma_leaf_tau)  # Common parameters nu <- 3 sigma2hat <- (sigma(lm(resid~X)))^2 quantile_cutoff <- 0.9 if (is.null(lambda)) {     lambda <- (sigma2hat*qgamma(1-quantile_cutoff,nu))/nu } sigma2 <- sigma2hat current_sigma2 <- sigma2 # Sampling composition num_gfr <- 20 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc  # Sigma^2 samples global_var_samples <- rep(0, num_samples)  # Adaptive coding parameter samples b_0_samples <- rep(0, num_samples) b_1_samples <- rep(0, num_samples) b_0 <- -0.5 b_1 <- 0.5 current_b_0 <- b_0 current_b_1 <- b_1 tau_basis <- (1-Z)*current_b_0 + Z*current_b_1 # Data X_mu <- cbind(X, pi_x) X_tau <- X feature_types <- c(0,0,0,1,1) feature_types_mu <- as.integer(c(feature_types,0)) feature_types_tau <- as.integer(feature_types) variable_weights_mu = rep(1/ncol(X_mu), ncol(X_mu)) variable_weights_tau = rep(1/ncol(X_tau), ncol(X_tau)) forest_dataset_mu <- createForestDataset(X_mu) forest_dataset_tau <- createForestDataset(X_tau, tau_basis) outcome <- createOutcome(resid)  # Random number generator (std::mt19937) rng <- createRNG()  # Sampling data structures forest_model_mu <- createForestModel(     forest_dataset_mu, feature_types_mu, num_trees_mu, nrow(X_mu),      alpha_mu, beta_mu, min_samples_leaf_mu ) forest_model_tau <- createForestModel(     forest_dataset_tau, feature_types_tau, num_trees_tau, nrow(X_tau),      alpha_tau, beta_tau, min_samples_leaf_tau )  # Container of forest samples forest_samples_mu <- createForestContainer(num_trees_mu, 1, T) forest_samples_tau <- createForestContainer(num_trees_tau, 1, F)  # Initialize the leaves of each tree in the prognostic forest forest_samples_mu$set_root_leaves(0, mean(resid) / num_trees_mu) forest_samples_mu$update_residual(     forest_dataset_mu, outcome, forest_model_mu, F, 0, F )  # Initialize the leaves of each tree in the treatment effect forest forest_samples_tau$set_root_leaves(0, 0.) forest_samples_tau$update_residual(     forest_dataset_tau, outcome, forest_model_tau, T, 0, F ) if (num_gfr > 0){     for (i in 1:num_gfr) {         # Sample the prognostic forest         forest_model_mu$sample_one_iteration(             forest_dataset_mu, outcome, forest_samples_mu, rng,              feature_types_mu, 0, current_leaf_scale_mu, variable_weights_mu,              current_sigma2, cutpoint_grid_size, gfr = T, pre_initialized = T         )                  # Sample variance parameters (if requested)         global_var_samples[i] <- sample_sigma2_one_iteration(             outcome, rng, nu, lambda         )         current_sigma2 <- global_var_samples[i]          # Sample the treatment forest         forest_model_tau$sample_one_iteration(             forest_dataset_tau, outcome, forest_samples_tau, rng,              feature_types_tau, 1, current_leaf_scale_tau, variable_weights_tau,              current_sigma2, cutpoint_grid_size, gfr = T, pre_initialized = T         )                  # Sample adaptive coding parameters         mu_x_raw <- forest_samples_mu$predict_raw_single_forest(forest_dataset_mu, i-1)         tau_x_raw <- forest_samples_tau$predict_raw_single_forest(forest_dataset_tau, i-1)         s_tt0 <- sum(tau_x_raw*tau_x_raw*(Z==0))         s_tt1 <- sum(tau_x_raw*tau_x_raw*(Z==1))         partial_resid_mu <- resid - mu_x_raw         s_ty0 <- sum(tau_x_raw*partial_resid_mu*(Z==0))         s_ty1 <- sum(tau_x_raw*partial_resid_mu*(Z==1))         current_b_0 <- rnorm(1, (s_ty0/(s_tt0 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt0 + 2*current_sigma2)))         current_b_1 <- rnorm(1, (s_ty1/(s_tt1 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt1 + 2*current_sigma2)))         tau_basis <- (1-Z)*current_b_0 + Z*current_b_1         forest_dataset_tau$update_basis(tau_basis)         b_0_samples[i] <- current_b_0         b_1_samples[i] <- current_b_1                  # Sample variance parameters (if requested)         global_var_samples[i] <- sample_sigma2_one_iteration(outcome, rng, nu, lambda)         current_sigma2 <- global_var_samples[i]     } } if (num_burnin + num_mcmc > 0) {     for (i in (num_gfr+1):num_samples) {         # Sample the prognostic forest         forest_model_mu$sample_one_iteration(             forest_dataset_mu, outcome, forest_samples_mu, rng, feature_types_mu,              0, current_leaf_scale_mu, variable_weights_mu, current_sigma2,              cutpoint_grid_size, gfr = F, pre_initialized = T         )                  # Sample global variance parameter         global_var_samples[i] <- sample_sigma2_one_iteration(outcome, rng, nu, lambda)         current_sigma2 <- global_var_samples[i]          # Sample the treatment forest         forest_model_tau$sample_one_iteration(             forest_dataset_tau, outcome, forest_samples_tau, rng, feature_types_tau,              1, current_leaf_scale_tau, variable_weights_tau, current_sigma2,              cutpoint_grid_size, gfr = F, pre_initialized = T         )                  # Sample coding parameters         mu_x_raw <- forest_samples_mu$predict_raw_single_forest(forest_dataset_mu, i-1)         tau_x_raw <- forest_samples_tau$predict_raw_single_forest(forest_dataset_tau, i-1)         s_tt0 <- sum(tau_x_raw*tau_x_raw*(Z==0))         s_tt1 <- sum(tau_x_raw*tau_x_raw*(Z==1))         partial_resid_mu <- resid - mu_x_raw         s_ty0 <- sum(tau_x_raw*partial_resid_mu*(Z==0))         s_ty1 <- sum(tau_x_raw*partial_resid_mu*(Z==1))         current_b_0 <- rnorm(1, (s_ty0/(s_tt0 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt0 + 2*current_sigma2)))         current_b_1 <- rnorm(1, (s_ty1/(s_tt1 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt1 + 2*current_sigma2)))         tau_basis <- (1-Z)*current_b_0 + Z*current_b_1         forest_dataset_tau$update_basis(tau_basis)         b_0_samples[i] <- current_b_0         b_1_samples[i] <- current_b_1          # Sample global variance parameter         global_var_samples[i] <- sample_sigma2_one_iteration(outcome, rng, nu, lambda)         current_sigma2 <- global_var_samples[i]     } } # Forest predictions mu_hat <- forest_samples_mu$predict(forest_dataset_mu)*y_std + y_bar tau_hat_raw <- forest_samples_tau$predict_raw(forest_dataset_tau) tau_hat <- t(t(tau_hat_raw) * (b_1_samples - b_0_samples))*y_std y_hat <- mu_hat + tau_hat * as.numeric(Z)  # Global error variance sigma2_samples <- global_var_samples*(y_std^2)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Prototype-Interface.html","id":"results-1","dir":"Articles","previous_headings":"Demo 2: Causal Inference","what":"Results","title":"Prototype Interface in StochTree","text":"Inspect XBART results    Inspect warm start BART results    Inspect “adaptive coding” parameters \\(b_0\\) \\(b_1\\).","code":"plot(sigma2_samples[1:num_gfr], ylab=\"sigma^2\") plot(rowMeans(mu_hat[,1:num_gfr]), mu_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"prognostic term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(tau_hat[,1:num_gfr]), tau_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"treatment effect term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) mean((rowMeans(tau_hat[,1:num_gfr]) - tau_x)^2) #> [1] 0.93167 plot(sigma_samples[(num_gfr+1):num_samples], ylab=\"sigma^2\") plot(rowMeans(mu_hat[,(num_gfr+1):num_samples]), mu_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"prognostic term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(tau_hat[,(num_gfr+1):num_samples]), tau_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"treatment effect term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) mean((rowMeans(tau_hat[,(num_gfr+1):num_samples]) - tau_x)^2) #> [1] 0.4200534 plot(b_0_samples, col = \"blue\", ylab = \"Coding parameter draws\",       ylim = c(min(min(b_0_samples), min(b_1_samples)), max(max(b_0_samples), max(b_1_samples)))) points(b_1_samples, col = \"orange\") legend(\"topleft\", legend = c(\"b_0\", \"b_1\"), col = c(\"blue\", \"orange\"), pch = c(1,1))"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Drew Herren. Author, maintainer. Richard Hahn. Author. Jared Murray. Author. Carlos Carvalho. Author. Jingyu . Author.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Herren D, Hahn R, Murray J, Carvalho C, J (2024). stochtree: Stochastic tree ensembles (XBART BART) supervised learning causal inference. R package version 0.0.0.9000, https://stochastictree.github.io/stochtree-r/.","code":"@Manual{,   title = {stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference},   author = {Drew Herren and Richard Hahn and Jared Murray and Carlos Carvalho and Jingyu He},   year = {2024},   note = {R package version 0.0.0.9000},   url = {https://stochastictree.github.io/stochtree-r/}, }"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference","text":"package can installed R via","code":"remotes::install_github(\"StochasticTree/stochtree-r\")"},{"path":"https://stochastictree.github.io/stochtree-r/reference/BART.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the BART algorithm for supervised learning. — bart","title":"Run the BART algorithm for supervised learning. — bart","text":"Run BART algorithm supervised learning.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/BART.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the BART algorithm for supervised learning. — bart","text":"","code":"bart(   X_train,   y_train,   W_train = NULL,   X_test = NULL,   W_test = NULL,   feature_types = rep(0, ncol(X_train)),   variable_weights = rep(1/ncol(X_train), ncol(X_train)),   cutpoint_grid_size = 100,   tau_init = NULL,   alpha = 0.95,   beta = 2,   min_samples_leaf = 5,   leaf_model = 0,   nu = 3,   lambda = NULL,   a_leaf = 3,   b_leaf = NULL,   q = 0.9,   sigma2_init = NULL,   num_trees = 200,   num_gfr = 5,   num_burnin = 0,   num_mcmc = 100,   sample_sigma = T,   sample_tau = T,   random_seed = -1 )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/BART.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the BART algorithm for supervised learning. — bart","text":"X_train Covariates used split trees ensemble. y_train Outcome modeled ensemble. W_train (Optional) Bases used define regression model y ~ W leaf regression tree. default, BART assumes constant leaf node parameters, implicitly regressing constant basis ones (.e. y ~ 1). X_test (Optional) Test set covariates used define \"sample\" evaluation data. W_test (Optional) Test set bases used define \"sample\" evaluation data. test set optional, structure provided test set must match training set (.e. X_train W_train provided, test set must consist X_test W_test number columns). feature_types Vector length ncol(X_train) indicating \"type\" covariates (0 = numeric, 1 = ordered categorical, 2 = unordered categorical). Default: rep(0,ncol(X_train)). variable_weights Vector length ncol(X_train) indicating \"weight\" placed variable sampling purposes. Default: rep(1/ncol(X_train),ncol(X_train)). cutpoint_grid_size Maximum size \"grid\" potential cutpoints consider. Default: 100. tau_init Starting value leaf node scale parameter. Calibrated internally 1/num_trees set . alpha Prior probability splitting tree depth 0. Tree split prior combines alpha beta via alpha*(1+node_depth)^-beta. beta Exponent decreases split probabilities nodes depth > 0. Tree split prior combines alpha beta via alpha*(1+node_depth)^-beta. min_samples_leaf Minimum allowable size leaf, terms training samples. Default: 5. leaf_model Integer indicating leaf model, 0 = constant Gaussian prior, 1 = univariate regression Gaussian prior, 2 = multivariate regression Gaussian prior. W_train ignored set 0. Default: 0. nu Shape parameter IG(nu, nu*lambda) global error variance model. Default: 3. lambda Component scale parameter IG(nu, nu*lambda) global error variance prior. specified, calibrated Sparapani et al (2021). a_leaf Shape parameter IG(a_leaf, b_leaf) leaf node parameter variance model. Default: 3. b_leaf Scale parameter IG(a_leaf, b_leaf) leaf node parameter variance model. Calibrated internally 0.5/num_trees set . q Quantile used calibrated lambda Sparapani et al (2021). Default: 0.9. sigma2_init Starting value global variance parameter. Calibrated internally Sparapani et al (2021) set . num_trees Number trees ensemble. Default: 200. num_gfr Number \"warm-start\" iterations run using grow--root algorithm (Hahn, 2021). Default: 5. num_burnin Number \"burn-\" iterations MCMC sampler. Default: 0. num_mcmc Number \"retained\" iterations MCMC sampler. Default: 100. sample_sigma Whether update sigma^2 global error variance parameter based IG(nu, nu*lambda). Default: T. sample_tau Whether update tau leaf scale variance parameter based IG(a_leaf, b_leaf). set true leaf_model=2. Default: T. random_seed Integer parameterizing C++ random number generator. specified, C++ random number generator seeded according std::random_device.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/BART.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the BART algorithm for supervised learning. — bart","text":"List sampling outputs wrapper around sampled forests (can used -memory prediction new data, serialized JSON disk).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/BART.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run the BART algorithm for supervised learning. — bart","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train, X_test = X_test, leaf_model = 0) # plot(rowMeans(bart_model$yhat_test), y_test, xlab = \"predicted\", ylab = \"actual\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"Persists C++ random number generator throughout R session ensure reproducibility given random seed. seed provided, C++ random number generator initialized using std::random_device.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"rng_ptr External pointer C++ std::mt19937 class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"CppRNG$new()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"Create new CppRNG object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"","code":"CppRNG$new(random_seed = -1)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"random_seed (Optional) random seed sampling","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"new CppRNG object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataset used to sample a forest — ForestDataset","title":"Dataset used to sample a forest — ForestDataset","text":"dataset consists three matrices / vectors: covariates, bases, variance weights. basis vector variance weights optional.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Dataset used to sample a forest — ForestDataset","text":"data_ptr External pointer C++ ForestDataset class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Dataset used to sample a forest — ForestDataset","text":"ForestDataset$new() ForestDataset$update_basis() ForestDataset$num_observations() ForestDataset$num_covariates() ForestDataset$num_basis() ForestDataset$has_basis() ForestDataset$has_variance_weights()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Dataset used to sample a forest — ForestDataset","text":"Create new ForestDataset object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$new(covariates, basis = NULL, variance_weights = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataset used to sample a forest — ForestDataset","text":"covariates Matrix covariates basis (Optional) Matrix bases used define leaf regression variance_weights (Optional) Vector observation-specific variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"new ForestDataset object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-update-basis-","dir":"Reference","previous_headings":"","what":"Method update_basis()","title":"Dataset used to sample a forest — ForestDataset","text":"Update basis matrix dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$update_basis(basis)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataset used to sample a forest — ForestDataset","text":"basis Updated matrix bases used define leaf regression","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-num-observations-","dir":"Reference","previous_headings":"","what":"Method num_observations()","title":"Dataset used to sample a forest — ForestDataset","text":"Return number observations ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$num_observations()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"Observation count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-num-covariates-","dir":"Reference","previous_headings":"","what":"Method num_covariates()","title":"Dataset used to sample a forest — ForestDataset","text":"Return number covariates ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$num_covariates()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"Covariate count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-num-basis-","dir":"Reference","previous_headings":"","what":"Method num_basis()","title":"Dataset used to sample a forest — ForestDataset","text":"Return number bases ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$num_basis()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"Basis count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-has-basis-","dir":"Reference","previous_headings":"","what":"Method has_basis()","title":"Dataset used to sample a forest — ForestDataset","text":"Whether dataset basis matrix","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$has_basis()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"True basis matrix loaded, false otherwise","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-has-variance-weights-","dir":"Reference","previous_headings":"","what":"Method has_variance_weights()","title":"Dataset used to sample a forest — ForestDataset","text":"Whether dataset variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$has_variance_weights()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"True variance weights loaded, false otherwise","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that defines and samples a forest model — ForestModel","title":"Class that defines and samples a forest model — ForestModel","text":"Hosts C++ data structures needed sample ensemble decision trees, exposes functionality run forest sampler (using either MCMC grow--root algorithm).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that defines and samples a forest model — ForestModel","text":"tracker_ptr External pointer C++ ForestTracker class tree_prior_ptr External pointer C++ TreePrior class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that defines and samples a forest model — ForestModel","text":"ForestModel$new() ForestModel$sample_one_iteration()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that defines and samples a forest model — ForestModel","text":"Create new ForestModel object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that defines and samples a forest model — ForestModel","text":"","code":"ForestModel$new(   forest_dataset,   feature_types,   num_trees,   n,   alpha,   beta,   min_samples_leaf )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that defines and samples a forest model — ForestModel","text":"forest_dataset ForestDataset object, used initialize forest sampling data structures feature_types Feature types (integers 0 = numeric, 1 = ordered categorical, 2 = unordered categorical) num_trees Number trees forest sampled n Number observations forest_dataset alpha Root node split probability tree prior beta Depth prior penalty tree prior min_samples_leaf Minimum number samples tree leaf","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that defines and samples a forest model — ForestModel","text":"new ForestModel object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"method-sample-one-iteration-","dir":"Reference","previous_headings":"","what":"Method sample_one_iteration()","title":"Class that defines and samples a forest model — ForestModel","text":"Run single iteration forest sampling algorithm (MCMC GFR)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that defines and samples a forest model — ForestModel","text":"","code":"ForestModel$sample_one_iteration(   forest_dataset,   residual,   forest_samples,   rng,   feature_types,   leaf_model_int,   leaf_model_scale,   variable_weights,   global_scale,   cutpoint_grid_size = 500,   gfr = T,   pre_initialized = F )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that defines and samples a forest model — ForestModel","text":"forest_dataset Dataset used sample forest residual Outcome used sample forest forest_samples Container forest samples rng Wrapper around C++ random number generator feature_types Vector specifying type p covariates forest_dataset (0 = numeric, 1 = ordered categorical, 2 = unordered categorical) leaf_model_int Integer specifying leaf model type (0 = constant leaf, 1 = univariate leaf regression, 2 = multivariate leaf regression) leaf_model_scale Scale parameter used leaf node model (q x q matrix q dimensionality basis >1 leaf_model_int = 2) variable_weights Vector specifying sampling probability p covariates forest_dataset global_scale Global variance parameter cutpoint_grid_size (Optional) Number unique cutpoints consider (default: 500, currently used GFR = TRUE) gfr (Optional) Whether forest sampled using \"grow--root\" (GFR) algorithm pre_initialized (Optional) Whether leaves pre-initialized outside sampling loop (samples drawn). multi-forest implementations like BCF, true, though single-forest supervised learning implementation, can let C++ initialization. Default: F.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that stores draws from an random ensemble of decision trees — ForestSamples","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Wrapper around C++ container tree ensembles","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_container_ptr External pointer C++ ForestContainer class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"ForestSamples$new() ForestSamples$predict() ForestSamples$predict_raw() ForestSamples$predict_raw_single_forest() ForestSamples$set_root_leaves() ForestSamples$update_residual() ForestSamples$save_json() ForestSamples$load_json() ForestSamples$num_samples() ForestSamples$output_dimension()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Create new ForestContainer object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$new(num_trees, output_dimension = 1, is_leaf_constant = F)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"num_trees Number trees output_dimension Dimensionality outcome model is_leaf_constant Whether leaf constant","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"new ForestContainer object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Predict every tree ensemble every sample forest_dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$predict(forest_dataset)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_dataset ForestDataset R class","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"matrix predictions many rows forest_dataset many columns samples ForestContainer","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-predict-raw-","dir":"Reference","previous_headings":"","what":"Method predict_raw()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Predict \"raw\" leaf values (without multiplied basis) every tree ensemble every sample forest_dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$predict_raw(forest_dataset)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_dataset ForestDataset R class","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Array predictions observation forest_dataset sample ForestSamples class prediction dimensionality forests' leaf model. case constant leaf model univariate leaf regression, array two-dimensional (number observations, number forest samples). case multivariate leaf regression, array three-dimension (number observations, leaf model dimension, number samples).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-predict-raw-single-forest-","dir":"Reference","previous_headings":"","what":"Method predict_raw_single_forest()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Predict \"raw\" leaf values (without multiplied basis) specific forest every sample forest_dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$predict_raw_single_forest(forest_dataset, forest_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_dataset ForestDataset R class forest_num Index forest sample within container","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"matrix predictions many rows forest_dataset many columns samples ForestContainer","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-set-root-leaves-","dir":"Reference","previous_headings":"","what":"Method set_root_leaves()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Set constant predicted value every tree ensemble. Stops program tree root node.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$set_root_leaves(forest_num, leaf_value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_num Index forest sample within container. leaf_value Constant leaf value(s) fixed tree ensemble indexed forest_num. Can either single number vector, depending forest's leaf dimension.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-update-residual-","dir":"Reference","previous_headings":"","what":"Method update_residual()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Updates residual based predictions forest","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$update_residual(   dataset,   outcome,   forest_model,   requires_basis,   forest_num,   add )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"dataset ForestDataset object storing covariates bases given forest outcome Outcome object storing residuals updated based forest predictions forest_model ForestModel object storing tracking structures used training / sampling requires_basis Whether forest requires basis prediction forest_num Index forest used update residuals add Whether forest predictions added subtracted residuals","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-save-json-","dir":"Reference","previous_headings":"","what":"Method save_json()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Store trees metadata ForestDataset class json file","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$save_json(json_filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_filename Name output json file (must end \".json\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-load-json-","dir":"Reference","previous_headings":"","what":"Method load_json()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Load trees metadata ensemble json file. Note trees metadata already present ForestDataset class overwritten.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$load_json(json_filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_filename Name model input json file (must end \".json\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-num-samples-","dir":"Reference","previous_headings":"","what":"Method num_samples()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Return number samples ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$num_samples()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Sample count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-output-dimension-","dir":"Reference","previous_headings":"","what":"Method output_dimension()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Return output dimension trees ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$output_dimension()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Leaf node parameter size","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":null,"dir":"Reference","previous_headings":"","what":"Outcome / partial residual used to sample an additive model. — Outcome","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"outcome class wrapper around vector (mutable) outcomes ML tasks (supervised learning, causal inference). additive tree ensemble sampled, outcome used sample specific model term \"partial residual\" consisting outcome minus predictions every model term (trees, group random effects, etc...).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"data_ptr External pointer C++ Outcome class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Outcome$new()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Create new Outcome object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"","code":"Outcome$new(outcome)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"outcome Vector outcome values","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"new Outcome object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"Run Bayesian Causal Forest (BCF) algorithm regularized causal effect estimation.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"","code":"bcf(   X_train,   Z_train,   y_train,   pi_train = NULL,   X_test = NULL,   Z_test = NULL,   pi_test = NULL,   feature_types = rep(0, ncol(X_train)),   cutpoint_grid_size = 100,   sigma_leaf_mu = NULL,   sigma_leaf_tau = NULL,   alpha_mu = 0.95,   alpha_tau = 0.25,   beta_mu = 2,   beta_tau = 3,   min_samples_leaf_mu = 5,   min_samples_leaf_tau = 5,   nu = 3,   lambda = NULL,   a_leaf_mu = 3,   a_leaf_tau = 3,   b_leaf_mu = NULL,   b_leaf_tau = NULL,   q = 0.9,   sigma2 = NULL,   num_trees_mu = 250,   num_trees_tau = 50,   num_gfr = 5,   num_burnin = 0,   num_mcmc = 100,   sample_sigma_global = T,   sample_sigma_leaf_mu = T,   sample_sigma_leaf_tau = T,   propensity_covariate = \"mu\",   adaptive_coding = T,   b_0 = -0.5,   b_1 = 0.5,   random_seed = -1 )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"X_train Covariates used split trees ensemble. Z_train Vector (continuous binary) treatment assignments. y_train Outcome modeled ensemble. pi_train (Optional) Vector propensity scores. provided, estimated data. X_test (Optional) Test set covariates used define \"sample\" evaluation data. Z_test (Optional) Test set (continuous binary) treatment assignments. pi_test (Optional) Vector propensity scores. provided, estimated data. feature_types Vector length ncol(X_train) indicating \"type\" covariates (0 = numeric, 1 = ordered categorical, 2 = unordered categorical). Default: rep(0,ncol(X_train)). cutpoint_grid_size Maximum size \"grid\" potential cutpoints consider. Default: 100. sigma_leaf_mu Starting value leaf node scale parameter prognostic forest. Calibrated internally 2/num_trees_mu set . sigma_leaf_tau Starting value leaf node scale parameter treatment effect forest. Calibrated internally 1/num_trees_tau set . alpha_mu Prior probability splitting tree depth 0 prognostic forest. Tree split prior combines alpha beta via alpha*(1+node_depth)^-beta. Default: 0.95. alpha_tau Prior probability splitting tree depth 0 treatment effect forest. Tree split prior combines alpha beta via alpha*(1+node_depth)^-beta. Default: 0.25. beta_mu Exponent decreases split probabilities nodes depth > 0 prognostic forest. Tree split prior combines alpha beta via alpha*(1+node_depth)^-beta. Default: 2.0. beta_tau Exponent decreases split probabilities nodes depth > 0 treatment effect forest. Tree split prior combines alpha beta via alpha*(1+node_depth)^-beta. Default: 3.0. min_samples_leaf_mu Minimum allowable size leaf, terms training samples, prognostic forest. Default: 5. min_samples_leaf_tau Minimum allowable size leaf, terms training samples, treatment effect forest. Default: 5. nu Shape parameter IG(nu, nu*lambda) global error variance model. Default: 3. lambda Component scale parameter IG(nu, nu*lambda) global error variance prior. specified, calibrated Sparapani et al (2021). a_leaf_mu Shape parameter IG(a_leaf, b_leaf) leaf node parameter variance model prognostic forest. Default: 3. a_leaf_tau Shape parameter IG(a_leaf, b_leaf) leaf node parameter variance model treatment effect forest. Default: 3. b_leaf_mu Scale parameter IG(a_leaf, b_leaf) leaf node parameter variance model prognostic forest. Calibrated internally 0.5/num_trees set . b_leaf_tau Scale parameter IG(a_leaf, b_leaf) leaf node parameter variance model treatment effect forest. Calibrated internally 0.5/num_trees set . q Quantile used calibrated lambda Sparapani et al (2021). Default: 0.9. sigma2 Starting value global variance parameter. Calibrated internally Sparapani et al (2021) set . num_trees_mu Number trees prognostic forest. Default: 200. num_trees_tau Number trees treatment effect forest. Default: 50. num_gfr Number \"warm-start\" iterations run using grow--root algorithm (Hahn, 2021). Default: 5. num_burnin Number \"burn-\" iterations MCMC sampler. Default: 0. num_mcmc Number \"retained\" iterations MCMC sampler. Default: 100. sample_sigma_global Whether update sigma^2 global error variance parameter based IG(nu, nu*lambda). Default: T. sample_sigma_leaf_mu Whether update sigma_leaf_mu leaf scale variance parameter prognostic forest based IG(a_leaf_mu, b_leaf_mu). Default: T. sample_sigma_leaf_tau Whether update sigma_leaf_tau leaf scale variance parameter treatment effect forest based IG(a_leaf_tau, b_leaf_tau). Default: T. propensity_covariate Whether include propensity score covariate either forests. Enter \"none\" neither, \"mu\" prognostic forest, \"tau\" treatment forest, \"\" forests. \"none\" propensity score provided, estimated (X_train, Z_train) using xgboost. Default: \"mu\". adaptive_coding Whether use \"adaptive coding\" scheme binary treatment variable coded manually (0,1) (-1,1) learned via parameters b_0 b_1 attach outcome model [b_0 (1-Z) + b_1 Z] tau(X). ignored Z binary. Default: T. b_0 Initial value \"control\" group coding parameter. ignored Z binary. Default: -0.5. b_1 Initial value \"treatment\" group coding parameter. ignored Z binary. Default: 0.5. random_seed Integer parameterizing C++ random number generator. specified, C++ random number generator seeded according std::random_device.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"List sampling outputs wrapper around sampled forests (can used -memory prediction new data, serialized JSON disk).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=T)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 4 y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,                   X_test = X_test, Z_test = Z_test, pi_test = pi_test) # plot(rowMeans(bcf_model$mu_hat_test), mu_test, xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") # abline(0,1,col=\"red\",lty=3,lwd=3) # plot(rowMeans(bcf_model$tau_hat_test), tau_test, xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a container of forest samples — createForestContainer","title":"Create a container of forest samples — createForestContainer","text":"Create container forest samples","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a container of forest samples — createForestContainer","text":"","code":"createForestContainer(num_trees, output_dimension = 1, is_leaf_constant = F)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a container of forest samples — createForestContainer","text":"num_trees Number trees output_dimension Dimensionality outcome model is_leaf_constant Whether leaf constant","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a container of forest samples — createForestContainer","text":"ForestSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a forest dataset object — createForestDataset","title":"Create a forest dataset object — createForestDataset","text":"Create forest dataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a forest dataset object — createForestDataset","text":"","code":"createForestDataset(covariates, basis = NULL, variance_weights = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a forest dataset object — createForestDataset","text":"covariates Matrix covariates basis (Optional) Matrix bases used define leaf regression variance_weights (Optional) Vector observation-specific variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a forest dataset object — createForestDataset","text":"ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a forest model object — createForestModel","title":"Create a forest model object — createForestModel","text":"Create forest model object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a forest model object — createForestModel","text":"","code":"createForestModel(   forest_dataset,   feature_types,   num_trees,   n,   alpha,   beta,   min_samples_leaf )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a forest model object — createForestModel","text":"forest_dataset ForestDataset object, used initialize forest sampling data structures feature_types Feature types (integers 0 = numeric, 1 = ordered categorical, 2 = unordered categorical) num_trees Number trees forest sampled n Number observations forest_dataset alpha Root node split probability tree prior beta Depth prior penalty tree prior min_samples_leaf Minimum number samples tree leaf","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a forest model object — createForestModel","text":"ForestModel object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an outcome object — createOutcome","title":"Create an outcome object — createOutcome","text":"Create outcome object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an outcome object — createOutcome","text":"","code":"createOutcome(outcome)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an outcome object — createOutcome","text":"outcome Vector outcome values","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an outcome object — createOutcome","text":"Outcome object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an R class that wraps a C++ random number generator — createRNG","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"Create R class wraps C++ random number generator","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"","code":"createRNG(random_seed = -1)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"random_seed (Optional) random seed sampling","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"CppRng object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict from a sampled BART model on new data — predict.bartmodel","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"Predict sampled BART model new data","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"","code":"# S3 method for bartmodel predict(bart, X_test, W_test = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"bart Object type bart containing draws regression forest associated sampling outputs. X_test Covariates used determine tree leaf predictions observation. W_test (Optional) Bases used prediction (e.g. dot product leaf values). Default: NULL.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"Matrix predictions nrow(X_test) rows bart$num_samples columns","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train, leaf_model = 0) yhat_test <- predict(bart_model, X_test) # plot(rowMeans(yhat_test), y_test, xlab = \"predicted\", ylab = \"actual\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict from a sampled BCF model on new data — predict.bcf","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"Predict sampled BCF model new data","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"","code":"# S3 method for bcf predict(bcf, X_test, Z_test, pi_test = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"bcf Object type bcf containing draws Bayesian causal forest model associated sampling outputs. X_test Covariates used determine tree leaf predictions observation. Z_test Treatments used prediction. pi_test (Optional) Propensities used prediction. Default: NULL.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"List three nrow(X_test) bcf$num_samples matrices: prognostic function estimates, treatment effect estimates outcome predictions.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=T)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 4 y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = F)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train) preds <- predict(bcf_model, X_test, Z_test, pi_test) # plot(rowMeans(preds$mu_hat), mu_test, xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") # abline(0,1,col=\"red\",lty=3,lwd=3) # plot(rowMeans(preds$tau_hat), tau_test, xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_sigma2_one_iteration.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample one iteration of the global variance model — sample_sigma2_one_iteration","title":"Sample one iteration of the global variance model — sample_sigma2_one_iteration","text":"Sample one iteration global variance model","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_sigma2_one_iteration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample one iteration of the global variance model — sample_sigma2_one_iteration","text":"","code":"sample_sigma2_one_iteration(residual, rng, nu, lambda)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_sigma2_one_iteration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample one iteration of the global variance model — sample_sigma2_one_iteration","text":"residual Outcome class rng C++ random number generator nu Global variance shape parameter lambda Constitutes scale parameter global variance along nu (.e. scale nu*lambda)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_tau_one_iteration.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","title":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","text":"Sample one iteration leaf parameter variance model (univariate basis constant leaf!)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_tau_one_iteration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","text":"","code":"sample_tau_one_iteration(forest_samples, rng, a, b, sample_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_tau_one_iteration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","text":"forest_samples Container forest samples rng C++ random number generator Leaf variance shape parameter b Leaf variance scale parameter sample_num Sample index","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/stochtree-package.html","id":null,"dir":"Reference","previous_headings":"","what":"stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference — stochtree-package","title":"stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference — stochtree-package","text":"Stochastic tree ensembles (XBART BART) supervised learning causal inference","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/stochtree-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference — stochtree-package","text":"Maintainer: Drew Herren drewherrenopensource@gmail.com (ORCID) Authors: Richard Hahn Jared Murray Carlos Carvalho Jingyu ","code":""}]
