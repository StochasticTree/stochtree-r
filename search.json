[{"path":"https://stochastictree.github.io/stochtree-r/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 stochtree authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"simulation","dir":"Articles","previous_headings":"Demo 1: Step Function","what":"Simulation","title":"Bayesian Supervised Learning in StochTree","text":", generate data simple step function.","code":"# Generate the data n <- 500 p_x <- 10 snr <- 3 X <- matrix(runif(n*p_x), ncol = p_x) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- sd(f_XW) / snr y <- f_XW + rnorm(n, 0, 1)*noise_sd  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- NULL W_train <- NULL y_test <- y[test_inds] y_train <- y[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"warmstart","dir":"Articles","previous_headings":"Demo 1: Step Function > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"first sample ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Hahn (2023)). default stochtree. Inspect MCMC samples","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T, num_trees_mean = 100) bart_model_warmstart <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(bart_model_warmstart$sigma2_global_samples, ylab=\"sigma^2\") abline(h=noise_sd^2,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_warmstart$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"bart-mcmc-without-warmstart","dir":"Articles","previous_headings":"Demo 1: Step Function > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"Next, sample ensemble model without warm-start initialization. Inspect MCMC samples","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T, num_trees_mean = 100) bart_model_root <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,     params = bart_params ) plot(bart_model_root$sigma2_global_samples, ylab=\"sigma^2\") abline(h=noise_sd^2,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_root$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"simulation-1","dir":"Articles","previous_headings":"Demo 2: Partitioned Linear Model","what":"Simulation","title":"Bayesian Supervised Learning in StochTree","text":", generate data simple partitioned linear model.","code":"# Generate the data n <- 500 p_x <- 10 p_w <- 1 snr <- 3 X <- matrix(runif(n*p_x), ncol = p_x) W <- matrix(runif(n*p_w), ncol = p_w) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5*W[,1]) ) noise_sd <- sd(f_XW) / snr y <- f_XW + rnorm(n, 0, 1)*noise_sd  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- W[test_inds,] W_train <- W[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"warmstart-1","dir":"Articles","previous_headings":"Demo 2: Partitioned Linear Model > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"first sample ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Hahn (2023)). default stochtree. Inspect MCMC samples","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T, num_trees_mean = 100) bart_model_warmstart <- stochtree::bart(     X_train = X_train, W_train = W_train, y_train = y_train, X_test = X_test, W_test = W_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(bart_model_warmstart$sigma2_global_samples, ylab=\"sigma^2\") abline(h=noise_sd^2,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_warmstart$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"bart-mcmc-without-warmstart-1","dir":"Articles","previous_headings":"Demo 2: Partitioned Linear Model > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"Next, sample ensemble model without warm-start initialization. Inspect BART samples burnin.","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T, num_trees_mean = 100) bart_model_root <- stochtree::bart(     X_train = X_train, W_train = W_train, y_train = y_train, X_test = X_test, W_test = W_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(bart_model_root$sigma2_global_samples, ylab=\"sigma^2\") abline(h=noise_sd^2,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_root$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"simulation-2","dir":"Articles","previous_headings":"Demo 3: Partitioned Linear Model with Random Effects","what":"Simulation","title":"Bayesian Supervised Learning in StochTree","text":", generate data simple partitioned linear model additive random effect structure.","code":"# Generate the data n <- 500 p_x <- 10 p_w <- 1 snr <- 3 X <- matrix(runif(n*p_x), ncol = p_x) W <- matrix(runif(n*p_w), ncol = p_w) group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-5, -3, 5, 3), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5*W[,1]) ) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) noise_sd <- sd(f_XW) / snr y <- f_XW + rfx_term + rnorm(n, 0, 1)*noise_sd  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- W[test_inds,] W_train <- W[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"warmstart-2","dir":"Articles","previous_headings":"Demo 3: Partitioned Linear Model with Random Effects > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"first sample ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Hahn (2023)). default stochtree. Inspect MCMC samples","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T, num_trees_mean = 100) bart_model_warmstart <- stochtree::bart(     X_train = X_train, W_train = W_train, y_train = y_train, group_ids_train = group_ids_train,      rfx_basis_train = rfx_basis_train, X_test = X_test, W_test = W_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(bart_model_warmstart$sigma2_global_samples, ylab=\"sigma^2\") abline(h=noise_sd^2,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_warmstart$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/BayesianSupervisedLearning.html","id":"bart-mcmc-without-warmstart-2","dir":"Articles","previous_headings":"Demo 3: Partitioned Linear Model with Random Effects > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Bayesian Supervised Learning in StochTree","text":"Next, sample ensemble model without warm-start initialization. Inspect MCMC samples","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T, num_trees_mean = 100) bart_model_root <- stochtree::bart(     X_train = X_train, W_train = W_train, y_train = y_train, group_ids_train = group_ids_train,      rfx_basis_train = rfx_basis_train, X_test = X_test, W_test = W_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(bart_model_root$sigma2_global_samples, ylab=\"sigma^2\") abline(h=noise_sd^2,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_root$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"demo-1-nonlinear-outcome-model-heterogeneous-treatment-effect","dir":"Articles","previous_headings":"Binary Treatment","what":"Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process Hahn, Murray, Carvalho (2020): y=μ(X)+τ(X)Z+ϵϵ∼N(0,σ2)μ(X)=1+g(X)+6|X3−1|τ(X)=1+2X2X4g(X)=𝕀(X5=1)×2−𝕀(X5=2)×1−𝕀(X5=3)×4sμ=𝕍(μ(X))π(X)=0.8ϕ(3μ(X)sμ)−X12+2U+120X1,X2,X3∼N(0,1)X4∼Bernoulli(1/2)X5∼Categorical(1/3,1/3,1/3)U∼Uniform(0,1)Z∼Bernoulli(π(X))\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= 1 + g(X) + 6 \\lvert X_3 - 1 \\rvert\\\\ \\tau(X) &= 1 + 2 X_2 X_4\\\\ g(X) &= \\mathbb{}(X_5=1) \\times 2 - \\mathbb{}(X_5=2) \\times 1 - \\mathbb{}(X_5=3) \\times 4\\\\ s_{\\mu} &= \\sqrt{\\mathbb{V}(\\mu(X))}\\\\ \\pi(X) &= 0.8 \\phi\\left(\\frac{3\\mu(X)}{s_{\\mu}}\\right) - \\frac{X_1}{2} + \\frac{2U+1}{20}\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim \\text{Bernoulli}(1/2)\\\\ X_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\ U &\\sim \\text{Uniform}\\left(0,1\\right)\\\\ Z &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right) \\end{aligned} \\end{equation*}","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"simulation","dir":"Articles","previous_headings":"Binary Treatment > Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart","dir":"Articles","previous_headings":"Binary Treatment > Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.88"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"bart-mcmc-without-warmstart","dir":"Articles","previous_headings":"Binary Treatment > Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_root$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples, sigma_observed)),                   max(c(bcf_model_root$sigma2_samples, sigma_observed))) plot(bcf_model_root$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.88"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"demo-2-linear-outcome-model-heterogeneous-treatment-effect","dir":"Articles","previous_headings":"Binary Treatment","what":"Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process Hahn, Murray, Carvalho (2020): y=μ(X)+τ(X)Z+ϵϵ∼N(0,σ2)μ(X)=1+g(X)+6X1X3τ(X)=1+2X2X4g(X)=𝕀(X5=1)×2−𝕀(X5=2)×1−𝕀(X5=3)×4sμ=𝕍(μ(X))π(X)=0.8ϕ(3μ(X)sμ)−X12+2U+120X1,X2,X3∼N(0,1)X4∼Bernoulli(1/2)X5∼Categorical(1/3,1/3,1/3)U∼Uniform(0,1)Z∼Bernoulli(π(X))\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= 1 + g(X) + 6 X_1 X_3\\\\ \\tau(X) &= 1 + 2 X_2 X_4\\\\ g(X) &= \\mathbb{}(X_5=1) \\times 2 - \\mathbb{}(X_5=2) \\times 1 - \\mathbb{}(X_5=3) \\times 4\\\\ s_{\\mu} &= \\sqrt{\\mathbb{V}(\\mu(X))}\\\\ \\pi(X) &= 0.8 \\phi\\left(\\frac{3\\mu(X)}{s_{\\mu}}\\right) - \\frac{X_1}{2} + \\frac{2U+1}{20}\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim \\text{Bernoulli}(1/2)\\\\ X_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\ U &\\sim \\text{Uniform}\\left(0,1\\right)\\\\ Z &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right) \\end{aligned} \\end{equation*}","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"simulation-1","dir":"Articles","previous_headings":"Binary Treatment > Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu2(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart-1","dir":"Articles","previous_headings":"Binary Treatment > Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.82"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"bart-mcmc-without-warmstart-1","dir":"Articles","previous_headings":"Binary Treatment > Demo 2: Linear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_root$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples, sigma_observed)),                   max(c(bcf_model_root$sigma2_samples, sigma_observed))) plot(bcf_model_root$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.92"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"demo-3-linear-outcome-model-homogeneous-treatment-effect","dir":"Articles","previous_headings":"Binary Treatment","what":"Demo 3: Linear Outcome Model, Homogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process Hahn, Murray, Carvalho (2020): y=μ(X)+τ(X)Z+ϵϵ∼N(0,σ2)μ(X)=1+g(X)+6X1X3τ(X)=3g(X)=𝕀(X5=1)×2−𝕀(X5=2)×1−𝕀(X5=3)×4sμ=𝕍(μ(X))π(X)=0.8ϕ(3μ(X)sμ)−X12+2U+120X1,X2,X3∼N(0,1)X4∼Bernoulli(1/2)X5∼Categorical(1/3,1/3,1/3)U∼Uniform(0,1)Z∼Bernoulli(π(X))\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= 1 + g(X) + 6 X_1 X_3\\\\ \\tau(X) &= 3\\\\ g(X) &= \\mathbb{}(X_5=1) \\times 2 - \\mathbb{}(X_5=2) \\times 1 - \\mathbb{}(X_5=3) \\times 4\\\\ s_{\\mu} &= \\sqrt{\\mathbb{V}(\\mu(X))}\\\\ \\pi(X) &= 0.8 \\phi\\left(\\frac{3\\mu(X)}{s_{\\mu}}\\right) - \\frac{X_1}{2} + \\frac{2U+1}{20}\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim \\text{Bernoulli}(1/2)\\\\ X_5 &\\sim \\text{Categorical}(1/3,1/3,1/3)\\\\ U &\\sim \\text{Uniform}\\left(0,1\\right)\\\\ Z &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right) \\end{aligned} \\end{equation*}","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"simulation-2","dir":"Articles","previous_headings":"Binary Treatment > Demo 3: Linear Outcome Model, Homogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu2(X) tau_x <- tau1(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart-2","dir":"Articles","previous_headings":"Binary Treatment > Demo 3: Linear Outcome Model, Homogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 1"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"bart-mcmc-without-warmstart-2","dir":"Articles","previous_headings":"Binary Treatment > Demo 3: Linear Outcome Model, Homogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_root$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples, sigma_observed)),                   max(c(bcf_model_root$sigma2_samples, sigma_observed))) plot(bcf_model_root$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 1"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"demo-4-nonlinear-outcome-model-heterogeneous-treatment-effect","dir":"Articles","previous_headings":"Binary Treatment","what":"Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process: y=μ(X)+τ(X)Z+ϵϵ∼N(0,σ2)μ(X)={−1.1 ifX1>X20.9 ifX1≤X2τ(X)=11+exp(−X3)+X210π(X)=Φ(μ(X))Z∼Bernoulli(π(X))X1,X2,X3∼N(0,1)X4∼N(X2,1)\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= \\begin{cases} -1.1 & \\text{ } X_1 > X_2\\\\ 0.9 & \\text{ } X_1 \\leq X_2 \\end{cases}\\\\ \\tau(X) &= \\frac{1}{1+\\exp(-X_3)} + \\frac{X_2}{10}\\\\ \\pi(X) &= \\Phi\\left(\\mu(X)\\right)\\\\ Z &\\sim \\text{Bernoulli}\\left(\\pi(X)\\right)\\\\ X_1,X_2,X_3 &\\sim N\\left(0,1\\right)\\\\ X_4 &\\sim N\\left(X_2,1\\right)\\\\ \\end{aligned} \\end{equation*}","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"simulation-3","dir":"Articles","previous_headings":"Binary Treatment > Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 1000 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- rnorm(n,x2,1) X <- cbind(x1,x2,x3,x4) p <- ncol(X) mu <- function(x) {-1*(x[,1]>(x[,2])) + 1*(x[,1]<(x[,2])) - 0.1} tau <- function(x) {1/(1 + exp(-x[,3])) + x[,2]/10} mu_x <- mu(X) tau_x <- tau(X) pi_x <- pnorm(mu_x) Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x sigma <- diff(range(mu_x + tau_x*pi))/8 y <- E_XZ + sigma*rnorm(n) X <- as.data.frame(X)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart-3","dir":"Articles","previous_headings":"Binary Treatment > Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.695"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"bart-mcmc-without-warmstart-3","dir":"Articles","previous_headings":"Binary Treatment > Demo 4: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 100 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_root$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples, sigma_observed)),                   max(c(bcf_model_root$sigma2_samples, sigma_observed))) plot(bcf_model_root$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.95"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"demo-5-nonlinear-outcome-model-heterogeneous-treatment-effect-with-additive-random-effects","dir":"Articles","previous_headings":"Binary Treatment","what":"Demo 5: Nonlinear Outcome Model, Heterogeneous Treatment Effect with Additive Random Effects","title":"Causal Machine Learning in StochTree","text":"augment simulated example Demo 1 additive random effect structure show bcf() function can estimate incorporate effects forest sampling procedure.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"simulation-4","dir":"Articles","previous_headings":"Binary Treatment > Demo 5: Nonlinear Outcome Model, Heterogeneous Treatment Effect with Additive Random Effects","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw augmented “demo 1” DGP","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart-4","dir":"Articles","previous_headings":"Binary Treatment > Demo 5: Nonlinear Outcome Model, Heterogeneous Treatment Effect with Additive Random Effects > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"simulate “warm-start” model (running root-MCMC BART random effects simply matter modifying code snippet setting num_gfr <- 0 num_mcmc > 0). Inspect BART samples initialized XBART warm-start      Examine test set interval coverage clear causal inference much difficult presence strong covariate-dependent prognostic effects strong group-level random effects. sense, proper prior calibration three μ\\mu, τ\\tau random effects models crucial.","code":"num_gfr <- 100 num_burnin <- 0 num_mcmc <- 500 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$y_hat_test), y_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Outcome\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$rfx_preds_test), rfx_term_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Random effects terms\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ-rfx_term) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.8"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"demo-6-nonlinear-outcome-model-heterogeneous-treatment-effect-using-different-features-in-the-prognostic-and-treatment-forests","dir":"Articles","previous_headings":"Binary Treatment","what":"Demo 6: Nonlinear Outcome Model, Heterogeneous Treatment Effect using Different Features in the Prognostic and Treatment Forests","title":"Causal Machine Learning in StochTree","text":", consider case might prefer use subset covariates treatment effect forest. might want ? Well, many cases plausible covariates (example age, income, etc…) influence outcome interest causal problem, moderate treatment effect. case, ’d need include variables prognostic forest deconfounding don’t necessarily need include treatment effect forest.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"simulation-5","dir":"Articles","previous_headings":"Binary Treatment > Demo 6: Nonlinear Outcome Model, Heterogeneous Treatment Effect using Different Features in the Prognostic and Treatment Forests","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw modified “demo 1” DGP","code":"mu <- function(x) {1+g(x)+x[,1]*x[,3]-x[,2]+3*x[,3]} tau <- function(x) {1 - 2*x[,1] + 2*x[,2] + 1*x[,1]*x[,2]} n <- 500 snr <- 4 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) x6 <- rnorm(n) x7 <- rnorm(n) x8 <- rnorm(n) x9 <- rnorm(n) x10 <- rnorm(n) X <- cbind(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10) p <- ncol(X) mu_x <- mu(X) tau_x <- tau(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE)  # Split data into test and train sets test_set_pct <- 0.5 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"mcmc-full-covariate-set-in-taux","dir":"Articles","previous_headings":"Binary Treatment > Demo 6: Nonlinear Outcome Model, Heterogeneous Treatment Effect using Different Features in the Prognostic and Treatment Forests > Sampling and Analysis","what":"MCMC, full covariate set in τ(X)\\tau(X)","title":"Causal Machine Learning in StochTree","text":"simulate model original MCMC sampler, using covariates prognostic (μ(X)\\mu(X)) treatment effect (τ(X)\\tau(X)) forests. Inspect burned-samples     Examine test set interval coverage test set RMSE","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_mcmc <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_mcmc$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_mcmc$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_mcmc$y_hat_test), y_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Outcome\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_mcmc$sigma2_samples, sigma_observed)),                   max(c(bcf_model_mcmc$sigma2_samples, sigma_observed))) plot(bcf_model_mcmc$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_mcmc$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_mcmc$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.76 test_tau_mean <- rowMeans(bcf_model_mcmc$tau_hat_test) sqrt(mean((test_tau_mean - tau_test)^2)) #> [1] 1.875399 test_outcome_mean <- rowMeans(bcf_model_mcmc$y_hat_test) sqrt(mean((test_outcome_mean - y_test)^2)) #> [1] 2.02139"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"mcmc-covariate-subset-in-taux","dir":"Articles","previous_headings":"Binary Treatment > Demo 6: Nonlinear Outcome Model, Heterogeneous Treatment Effect using Different Features in the Prognostic and Treatment Forests > Sampling and Analysis","what":"MCMC, covariate subset in τ(X)\\tau(X)","title":"Causal Machine Learning in StochTree","text":"simulate model original MCMC sampler, using covariate X1X_1 treatment effect forest. Inspect BART samples     Examine test set interval coverage test set RMSE","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F, keep_vars_tau = c(\"x1\",\"x2\")) bcf_model_mcmc <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_mcmc$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_mcmc$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_mcmc$y_hat_test), y_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Outcome\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_mcmc$sigma2_samples, sigma_observed)),                   max(c(bcf_model_mcmc$sigma2_samples, sigma_observed))) plot(bcf_model_mcmc$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_mcmc$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_mcmc$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.744 test_mean <- rowMeans(bcf_model_mcmc$tau_hat_test) sqrt(mean((test_mean - tau_test)^2)) #> [1] 1.580901 test_outcome_mean <- rowMeans(bcf_model_mcmc$y_hat_test) sqrt(mean((test_outcome_mean - y_test)^2)) #> [1] 2.076354"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart-full-covariate-set-in-taux","dir":"Articles","previous_headings":"Binary Treatment > Demo 6: Nonlinear Outcome Model, Heterogeneous Treatment Effect using Different Features in the Prognostic and Treatment Forests > Sampling and Analysis","what":"Warmstart, full covariate set in τ(X)\\tau(X)","title":"Causal Machine Learning in StochTree","text":"simulate model warm-start sampler, using covariates prognostic (μ(X)\\mu(X)) treatment effect (τ(X)\\tau(X)) forests. Inspect BART samples initialized XBART warm-start     Examine test set interval coverage test set RMSE","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$y_hat_test), y_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Outcome\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.756 test_tau_mean <- rowMeans(bcf_model_warmstart$tau_hat_test) sqrt(mean((tau_test - test_tau_mean)^2)) #> [1] 1.935506 test_outcome_mean <- rowMeans(bcf_model_warmstart$y_hat_test) sqrt(mean((y_test - test_outcome_mean)^2)) #> [1] 1.919096"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart-covariate-subset-in-taux","dir":"Articles","previous_headings":"Binary Treatment > Demo 6: Nonlinear Outcome Model, Heterogeneous Treatment Effect using Different Features in the Prognostic and Treatment Forests > Sampling and Analysis","what":"Warmstart, covariate subset in τ(X)\\tau(X)","title":"Causal Machine Learning in StochTree","text":"simulate model warm-start sampler, using covariate X1X_1 treatment effect forest. Inspect BART samples initialized XBART warm-start     Examine test set interval coverage test set RMSE","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F, keep_vars_tau = c(\"x1\",\"x2\")) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$y_hat_test), y_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Outcome\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.8 test_tau_mean <- rowMeans(bcf_model_warmstart$tau_hat_test) sqrt(mean((tau_test - test_tau_mean)^2)) #> [1] 1.482847 test_outcome_mean <- rowMeans(bcf_model_warmstart$y_hat_test) sqrt(mean((y_test - test_outcome_mean)^2)) #> [1] 1.821532"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"demo-1-nonlinear-outcome-model-heterogeneous-treatment-effect-1","dir":"Articles","previous_headings":"Continuous Treatment","what":"Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect","title":"Causal Machine Learning in StochTree","text":"consider following data generating process: y=μ(X)+τ(X)Z+ϵϵ∼N(0,σ2)μ(X)=1+2X1−𝟙(X2<0)×4+𝟙(X2≥0)×4+3(|X3|−2π)τ(X)=1+2X4X1,X2,X3,X4,X5∼N(0,1)U∼Uniform(0,1)π(X)=μ(X)−12+4(U−12)Z∼𝒩(π(X),1)\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) Z + \\epsilon\\\\ \\epsilon &\\sim N\\left(0,\\sigma^2\\right)\\\\ \\mu(X) &= 1 + 2 X_1 - \\mathbb{1}\\left(X_2 < 0\\right) \\times 4 + \\mathbb{1}\\left(X_2 \\geq 0\\right) \\times 4 + 3 \\left(\\lvert X_3 \\rvert - \\sqrt{\\frac{2}{\\pi}} \\right)\\\\ \\tau(X) &= 1 + 2 X_4\\\\ X_1,X_2,X_3,X_4,X_5 &\\sim N\\left(0,1\\right)\\\\ U &\\sim \\text{Uniform}\\left(0,1\\right)\\\\ \\pi(X) &= \\frac{\\mu(X) - 1}{2} + 4 \\left(U - \\frac{1}{2}\\right)\\\\ Z &\\sim \\mathcal{N}\\left(\\pi(X), 1\\right) \\end{aligned} \\end{equation*}","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"simulation-6","dir":"Articles","previous_headings":"Continuous Treatment > Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect","what":"Simulation","title":"Causal Machine Learning in StochTree","text":"draw DGP defined ","code":"n <- 500 snr <- 3 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- rnorm(n) x5 <- rnorm(n) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- 1 + 2*x1 - 4*(x2 < 0) + 4*(x2 >= 0) + 3*(abs(x3) - sqrt(2/pi)) tau_x <- 1 + 2*x4 u <- runif(n) pi_x <- ((mu_x-1)/4) + 4*(u-0.5) Z <- pi_x + rnorm(n,0,1) E_XZ <- mu_x + Z*tau_x y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"warmstart-5","dir":"Articles","previous_headings":"Continuous Treatment > Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"Warmstart","title":"Causal Machine Learning in StochTree","text":"first simulate ensemble model y∣Xy \\mid X using “warm-start” initialization samples (Krantsevich, , Hahn (2023)). default stochtree. Inspect BART samples initialized XBART warm-start    Examine test set interval coverage","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_warmstart <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_warmstart$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_warmstart$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_warmstart$sigma2_samples, sigma_observed)),                   max(c(bcf_model_warmstart$sigma2_samples, sigma_observed))) plot(bcf_model_warmstart$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_warmstart$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.77"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CausalInference.html","id":"bart-mcmc-without-warmstart-4","dir":"Articles","previous_headings":"Continuous Treatment > Demo 1: Nonlinear Outcome Model, Heterogeneous Treatment Effect > Sampling and Analysis","what":"BART MCMC without Warmstart","title":"Causal Machine Learning in StochTree","text":"Next, simulate ensemble model without warm-start initialization. Inspect BART samples burnin    Examine test set interval coverage","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 1000 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model_root <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params ) plot(rowMeans(bcf_model_root$mu_hat_test), mu_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(rowMeans(bcf_model_root$tau_hat_test), tau_test,       xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") abline(0,1,col=\"red\",lty=3,lwd=3) sigma_observed <- var(y-E_XZ) plot_bounds <- c(min(c(bcf_model_root$sigma2_samples, sigma_observed)),                   max(c(bcf_model_root$sigma2_samples, sigma_observed))) plot(bcf_model_root$sigma2_samples, ylim = plot_bounds,       ylab = \"sigma^2\", xlab = \"Sample\", main = \"Global variance parameter\") abline(h = sigma_observed, lty=3, lwd = 3, col = \"blue\") test_lb <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.025) test_ub <- apply(bcf_model_root$tau_hat_test, 1, quantile, 0.975) cover <- (     (test_lb <= tau_x[test_inds]) &      (test_ub >= tau_x[test_inds]) ) mean(cover) #> [1] 0.86"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"Custom Sampling Routines in StochTree","text":"functions bart() bcf() provide simple performant interfaces supervised learning / causal inference, stochtree also offers access many “low-level” data structures typically implemented C++. low-level interface designed performance even simplicity — rather intent provide “prototype” interface C++ code doesn’t require modifying C++. illustrate prototype interface might useful, consider classic BART algorithm:    INPUT: yy, XX, τ\\tau, ν\\nu, λ\\lambda, α\\alpha, β\\beta    OUTPUT: mm samples decision forest kk trees global variance parameter σ2\\sigma^2    Initialize σ2\\sigma^2 via default data-dependent calibration exercise    Initialize “forest 0” kk trees single root node, referring tree jj’s prediction vector f0,jf_{0,j}    Compute residual r=y−∑j=1kf0,jr = y - \\sum_{j=1}^k f_{0,j}    iiIN {1,…,m}\\left\\{1,\\dots,m\\right\\}:       Initialize forest ii forest −1i-1       jjIN {1,…,k}\\left\\{1,\\dots,k\\right\\}:          Add predictions tree jj residual: r=r+fi,jr = r + f_{,j}          Update tree jj via Metropolis-Hastings rr XX data tree priors depending (τ\\tau, σ2\\sigma^2, α\\alpha, β\\beta)          Sample leaf node parameters tree jj via Gibbs (leaf node prior N(0,τ)N\\left(0,\\tau\\right))          Subtract (updated) predictions tree jj residual: r=r−fi,jr = r - f_{,j}       Sample σ2\\sigma^2 via Gibbs (prior IG(ν/2,νλ/2)IG(\\nu/2,\\nu\\lambda/2)) algorithm conceptually simple, much core computation carried low-level languages C C++ tree data structure. result, changes algorithm, supporting heteroskedasticity (Pratola et al. (2020)), categorical outcomes (Murray (2021)) causal effect estimation (Hahn, Murray, Carvalho (2020)) require modifying low-level code. prototype interface exposes core components loop R level, thus making possible interchange C++ computation steps like “update tree jj via Metropolis-Hastings” R computation custom variance model, user-specified additive mean model components, . begin, load stochtree package","code":"library(stochtree)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"simulation","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Simulation","title":"Custom Sampling Routines in StochTree","text":"Simulate simple partitioned linear model","code":"# Generate the data n <- 500 p_X <- 10 p_W <- 1 X <- matrix(runif(n*p_X), ncol = p_X) W <- matrix(runif(n*p_W), ncol = p_W) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-3*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-1*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (1*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3*W[,1]) ) y <- f_XW + rnorm(n, 0, 1)  # Standardize outcome y_bar <- mean(y) y_std <- sd(y) resid <- (y-y_bar)/y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"sampling","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Sampling","title":"Custom Sampling Routines in StochTree","text":"Set parameters inform forest variance parameter samplers Initialize R-level access C++ classes needed sample model Prepare run sampler Run grow--root sampler “warm-start” BART Pick last GFR forest (associated global variance / leaf scale parameters) MCMC sampler Predict rescale samples","code":"alpha <- 0.9 beta <- 1.25 min_samples_leaf <- 1 max_depth <- 10 num_trees <- 100 cutpoint_grid_size = 100 global_variance_init = 1. tau_init = 0.5 leaf_prior_scale = matrix(c(tau_init), ncol = 1) nu <- 4 lambda <- 0.5 a_leaf <- 2. b_leaf <- 0.5 leaf_regression <- T feature_types <- as.integer(rep(0, p_X)) # 0 = numeric var_weights <- rep(1/p_X, p_X) # Data if (leaf_regression) {     forest_dataset <- createForestDataset(X, W)     outcome_model_type <- 1 } else {     forest_dataset <- createForestDataset(X)     outcome_model_type <- 0 } outcome <- createOutcome(resid)  # Random number generator (std::mt19937) rng <- createRNG()  # Sampling data structures forest_model <- createForestModel(forest_dataset, feature_types,                                    num_trees, n, alpha, beta,                                    min_samples_leaf, max_depth)  # Container of forest samples if (leaf_regression) {     forest_samples <- createForestContainer(num_trees, 1, F) } else {     forest_samples <- createForestContainer(num_trees, 1, T) } num_warmstart <- 10 num_mcmc <- 100 num_samples <- num_warmstart + num_mcmc global_var_samples <- c(global_variance_init, rep(0, num_samples)) leaf_scale_samples <- c(tau_init, rep(0, num_samples)) for (i in 1:num_warmstart) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, global_var_samples[i], cutpoint_grid_size, gfr = T     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1] } for (i in (num_warmstart+1):num_samples) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, global_var_samples[i], cutpoint_grid_size, gfr = F     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1] } # Forest predictions preds <- forest_samples$predict(forest_dataset)*y_std + y_bar  # Global error variance sigma_samples <- sqrt(global_var_samples)*y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"results","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Results","title":"Custom Sampling Routines in StochTree","text":"Inspect initial samples obtained via “grow--root” (Hahn (2023))   Inspect BART samples obtained “warm-starting”","code":"plot(sigma_samples[1:num_warmstart], ylab=\"sigma\") plot(rowMeans(preds[,1:num_warmstart]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(sigma_samples[(num_warmstart+1):num_samples], ylab=\"sigma\") plot(rowMeans(preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"demo-2-supervised-learning-with-additive-random-effects","dir":"Articles","previous_headings":"","what":"Demo 2: Supervised Learning with Additive Random Effects","title":"Custom Sampling Routines in StochTree","text":"build example add simple “random effects” structure: every observation either group 1 group 2 random group intercept (simulated quite strong, underscoring need random effects modeling).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"simulation-1","dir":"Articles","previous_headings":"Demo 2: Supervised Learning with Additive Random Effects","what":"Simulation","title":"Custom Sampling Routines in StochTree","text":"Simulate partitioned linear model simple additive group random effect structure","code":"# Generate the data n <- 500 p_X <- 10 p_W <- 1 X <- matrix(runif(n*p_X), ncol = p_X) W <- matrix(runif(n*p_W), ncol = p_W) group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- c(-5, 5) rfx_basis <- rep(1, n) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-3*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-1*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (1*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3*W[,1]) ) rfx_term <- rfx_coefs[group_ids] * rfx_basis y <- f_XW + rfx_term + rnorm(n, 0, 1)  # Standardize outcome y_bar <- mean(y) y_std <- sd(y) resid <- (y-y_bar)/y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"sampling-1","dir":"Articles","previous_headings":"Demo 2: Supervised Learning with Additive Random Effects","what":"Sampling","title":"Custom Sampling Routines in StochTree","text":"Set parameters inform forest variance parameter samplers Set parameters inform random effects samplers Initialize R-level access C++ classes needed sample model Prepare run sampler Run grow--root sampler “warm-start” BART Pick last GFR forest (associated global variance / leaf scale parameters) MCMC sampler Predict rescale samples","code":"alpha <- 0.9 beta <- 1.25 min_samples_leaf <- 1 max_depth <- 10 num_trees <- 100 cutpoint_grid_size = 100 global_variance_init = 1. tau_init = 0.5 leaf_prior_scale = matrix(c(tau_init), ncol = 1) nu <- 4 lambda <- 0.5 a_leaf <- 2. b_leaf <- 0.5 leaf_regression <- T feature_types <- as.integer(rep(0, p_X)) # 0 = numeric var_weights <- rep(1/p_X, p_X) alpha_init <- c(1) xi_init <- matrix(c(1,1),1,2) sigma_alpha_init <- matrix(c(1),1,1) sigma_xi_init <- matrix(c(1),1,1) sigma_xi_shape <- 1 sigma_xi_scale <- 1 # Data if (leaf_regression) {     forest_dataset <- createForestDataset(X, W)     outcome_model_type <- 1 } else {     forest_dataset <- createForestDataset(X)     outcome_model_type <- 0 } outcome <- createOutcome(resid)  # Random number generator (std::mt19937) rng <- createRNG()  # Sampling data structures forest_model <- createForestModel(forest_dataset, feature_types,                                    num_trees, n, alpha, beta,                                    min_samples_leaf, max_depth)  # Container of forest samples if (leaf_regression) {     forest_samples <- createForestContainer(num_trees, 1, F) } else {     forest_samples <- createForestContainer(num_trees, 1, T) }  # Random effects dataset rfx_basis <- as.matrix(rfx_basis) group_ids <- as.integer(group_ids) rfx_dataset <- createRandomEffectsDataset(group_ids, rfx_basis)  # Random effects details num_groups <- length(unique(group_ids)) num_components <- ncol(rfx_basis)  # Random effects tracker rfx_tracker <- createRandomEffectsTracker(group_ids)  # Random effects model rfx_model <- createRandomEffectsModel(num_components, num_groups) rfx_model$set_working_parameter(alpha_init) rfx_model$set_group_parameters(xi_init) rfx_model$set_working_parameter_cov(sigma_alpha_init) rfx_model$set_group_parameter_cov(sigma_xi_init) rfx_model$set_variance_prior_shape(sigma_xi_shape) rfx_model$set_variance_prior_scale(sigma_xi_scale)  # Random effect samples rfx_samples <- createRandomEffectSamples(num_components, num_groups, rfx_tracker) num_warmstart <- 10 num_mcmc <- 100 num_samples <- num_warmstart + num_mcmc global_var_samples <- c(global_variance_init, rep(0, num_samples)) leaf_scale_samples <- c(tau_init, rep(0, num_samples)) for (i in 1:num_warmstart) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, global_var_samples[i], cutpoint_grid_size, gfr = T     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1]          # Sample random effects model     rfx_model$sample_random_effect(rfx_dataset, outcome, rfx_tracker, rfx_samples, global_var_samples[i+1], rng) } for (i in (num_warmstart+1):num_samples) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, global_var_samples[i], cutpoint_grid_size, gfr = F     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1]          # Sample random effects model     rfx_model$sample_random_effect(rfx_dataset, outcome, rfx_tracker, rfx_samples, global_var_samples[i+1], rng) } # Forest predictions forest_preds <- forest_samples$predict(forest_dataset)*y_std + y_bar  # Random effects predictions rfx_preds <- rfx_samples$predict(group_ids, rfx_basis)*y_std  # Overall predictions preds <- forest_preds + rfx_preds  # Global error variance sigma_samples <- sqrt(global_var_samples)*y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"results-1","dir":"Articles","previous_headings":"Demo 2: Supervised Learning with Additive Random Effects","what":"Results","title":"Custom Sampling Routines in StochTree","text":"Inspect initial samples obtained via grow--root additive random effects model   Inspect BART samples obtained “warm-starting” plus additive random effects model   Now inspect samples BART forest alone (without considering random effect predictions)","code":"plot(sigma_samples[1:num_warmstart], ylab=\"sigma\") plot(rowMeans(preds[,1:num_warmstart]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(sigma_samples[(num_warmstart+1):num_samples], ylab=\"sigma\") plot(rowMeans(preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(forest_preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"demo-3-supervised-learning-with-additive-multi-component-random-effects","dir":"Articles","previous_headings":"","what":"Demo 3: Supervised Learning with Additive Multi-Component Random Effects","title":"Custom Sampling Routines in StochTree","text":"build example, case allowing random intercept regression coefficient (pre-specified basis) group (1 2).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"simulation-2","dir":"Articles","previous_headings":"Demo 3: Supervised Learning with Additive Multi-Component Random Effects","what":"Simulation","title":"Custom Sampling Routines in StochTree","text":"Simulate partitioned linear model simple additive group random effect structure","code":"# Generate the data n <- 500 p_X <- 10 p_W <- 1 X <- matrix(runif(n*p_X), ncol = p_X) W <- matrix(runif(n*p_W), ncol = p_W) group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-5, -3, 5, 3), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-3*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-1*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (1*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3*W[,1]) ) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- f_XW + rfx_term + rnorm(n, 0, 1)  # Standardize outcome y_bar <- mean(y) y_std <- sd(y) resid <- (y-y_bar)/y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"sampling-2","dir":"Articles","previous_headings":"Demo 3: Supervised Learning with Additive Multi-Component Random Effects","what":"Sampling","title":"Custom Sampling Routines in StochTree","text":"Set parameters inform forest variance parameter samplers Set parameters inform random effects samplers Initialize R-level access C++ classes needed sample model Prepare run sampler Run grow--root sampler “warm-start” BART Pick last GFR forest (associated global variance / leaf scale parameters) MCMC sampler Predict rescale samples","code":"alpha <- 0.9 beta <- 1.25 min_samples_leaf <- 1 max_depth <- 10 num_trees <- 100 cutpoint_grid_size = 100 global_variance_init = 1. tau_init = 0.5 leaf_prior_scale = matrix(c(tau_init), ncol = 1) nu <- 4 lambda <- 0.5 a_leaf <- 2. b_leaf <- 0.5 leaf_regression <- T feature_types <- as.integer(rep(0, p_X)) # 0 = numeric var_weights <- rep(1/p_X, p_X) alpha_init <- c(1,0) xi_init <- matrix(c(1,0,1,0),2,2) sigma_alpha_init <- diag(1,2,2) sigma_xi_init <- diag(1,2,2) sigma_xi_shape <- 1 sigma_xi_scale <- 1 # Data if (leaf_regression) {     forest_dataset <- createForestDataset(X, W)     outcome_model_type <- 1 } else {     forest_dataset <- createForestDataset(X)     outcome_model_type <- 0 } outcome <- createOutcome(resid)  # Random number generator (std::mt19937) rng <- createRNG()  # Sampling data structures forest_model <- createForestModel(forest_dataset, feature_types,                                    num_trees, n, alpha, beta,                                    min_samples_leaf, max_depth)  # Container of forest samples if (leaf_regression) {     forest_samples <- createForestContainer(num_trees, 1, F) } else {     forest_samples <- createForestContainer(num_trees, 1, T) }  # Random effects dataset rfx_basis <- as.matrix(rfx_basis) group_ids <- as.integer(group_ids) rfx_dataset <- createRandomEffectsDataset(group_ids, rfx_basis)  # Random effects details num_groups <- length(unique(group_ids)) num_components <- ncol(rfx_basis)  # Random effects tracker rfx_tracker <- createRandomEffectsTracker(group_ids)  # Random effects model rfx_model <- createRandomEffectsModel(num_components, num_groups) rfx_model$set_working_parameter(alpha_init) rfx_model$set_group_parameters(xi_init) rfx_model$set_working_parameter_cov(sigma_alpha_init) rfx_model$set_group_parameter_cov(sigma_xi_init) rfx_model$set_variance_prior_shape(sigma_xi_shape) rfx_model$set_variance_prior_scale(sigma_xi_scale)  # Random effect samples rfx_samples <- createRandomEffectSamples(num_components, num_groups, rfx_tracker) num_warmstart <- 10 num_mcmc <- 100 num_samples <- num_warmstart + num_mcmc global_var_samples <- c(global_variance_init, rep(0, num_samples)) leaf_scale_samples <- c(tau_init, rep(0, num_samples)) for (i in 1:num_warmstart) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, global_var_samples[i], cutpoint_grid_size, gfr = T     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1]          # Sample random effects model     rfx_model$sample_random_effect(rfx_dataset, outcome, rfx_tracker, rfx_samples, global_var_samples[i+1], rng) } for (i in (num_warmstart+1):num_samples) {     # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, global_var_samples[i], cutpoint_grid_size, gfr = F     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     )          # Sample leaf node variance parameter and update `leaf_prior_scale`     leaf_scale_samples[i+1] <- sample_tau_one_iteration(         forest_samples, rng, a_leaf, b_leaf, i-1     )     leaf_prior_scale[1,1] <- leaf_scale_samples[i+1]          # Sample random effects model     rfx_model$sample_random_effect(rfx_dataset, outcome, rfx_tracker, rfx_samples, global_var_samples[i+1], rng) } # Forest predictions forest_preds <- forest_samples$predict(forest_dataset)*y_std + y_bar  # Random effects predictions rfx_preds <- rfx_samples$predict(group_ids, rfx_basis)*y_std  # Overall predictions preds <- forest_preds + rfx_preds  # Global error variance sigma_samples <- sqrt(global_var_samples)*y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"results-2","dir":"Articles","previous_headings":"Demo 3: Supervised Learning with Additive Multi-Component Random Effects","what":"Results","title":"Custom Sampling Routines in StochTree","text":"Inspect initial samples obtained via grow--root additive random effects model   Inspect BART samples obtained “warm-starting” plus additive random effects model   Now inspect samples BART forest alone (without considering random effect predictions)","code":"plot(sigma_samples[1:num_warmstart], ylab=\"sigma\") plot(rowMeans(preds[,1:num_warmstart]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(sigma_samples[(num_warmstart+1):num_samples], ylab=\"sigma\") plot(rowMeans(preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(forest_preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"demo-4-supervised-learning-with-additive-linear-model","dir":"Articles","previous_headings":"","what":"Demo 4: Supervised Learning with Additive Linear Model","title":"Custom Sampling Routines in StochTree","text":"Instead group random effects, show combine stochastic forest additive linear regression term. model corresponds broadly y=Wβ+f(X)+ϵf(X)∼BART(c,d)β∼𝒩(0,τ)ϵ∼𝒩(0,σ2)σ2∼IG(,b)\\begin{equation*} \\begin{aligned} y &= W\\beta + f(X) + \\epsilon\\\\ f(X) &\\sim \\text{BART}(c,d)\\\\ \\beta &\\sim \\mathcal{N}(0, \\tau)\\\\ \\epsilon &\\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\\\\ \\sigma^2 &\\sim \\text{IG}(,b) \\end{aligned} \\end{equation*}","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"simulation-3","dir":"Articles","previous_headings":"Demo 4: Supervised Learning with Additive Linear Model","what":"Simulation","title":"Custom Sampling Routines in StochTree","text":"Simulate partitioned linear model simple additive group random effect structure","code":"# Generate the data n <- 500 p_X <- 10 p_W <- 1 X <- matrix(runif(n*p_X), ncol = p_X) W <- matrix(runif(n*p_W), ncol = p_W) beta_W <- c(5) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-3) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-1) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (1) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3) ) lm_term <- W %*% beta_W y <- lm_term + f_XW + rnorm(n, 0, 1)  # Standardize outcome y_bar <- mean(y) y_std <- sd(y) resid <- (y-y_bar)/y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"sampling-3","dir":"Articles","previous_headings":"Demo 4: Supervised Learning with Additive Linear Model","what":"Sampling","title":"Custom Sampling Routines in StochTree","text":"Set parameters inform forest variance parameter samplers Initialize R-level access C++ classes needed sample model Prepare run sampler Run grow--root sampler “warm-start” BART Pick last GFR forest (associated global variance / leaf scale parameters) MCMC sampler Predict rescale samples","code":"alpha_bart <- 0.9 beta_bart <- 1.25 min_samples_leaf <- 1 max_depth <- 10 num_trees <- 100 cutpoint_grid_size = 100 global_variance_init = 1. tau_init = 0.5 leaf_prior_scale = matrix(c(tau_init), ncol = 1) nu <- 4 lambda <- 0.5 a_leaf <- 2. b_leaf <- 0.5 leaf_regression <- F feature_types <- as.integer(rep(0, p_X)) # 0 = numeric var_weights <- rep(1/p_X, p_X) beta_tau <- 20 # Data if (leaf_regression) {     forest_dataset <- createForestDataset(X, W)     outcome_model_type <- 1 } else {     forest_dataset <- createForestDataset(X)     outcome_model_type <- 0 } outcome <- createOutcome(resid)  # Random number generator (std::mt19937) rng <- createRNG()  # Sampling data structures forest_model <- createForestModel(forest_dataset, feature_types,                                    num_trees, n, alpha_bart, beta_bart,                                    min_samples_leaf, max_depth)  # Container of forest samples if (leaf_regression) {     forest_samples <- createForestContainer(num_trees, 1, F) } else {     forest_samples <- createForestContainer(num_trees, 1, T) } num_warmstart <- 20 num_mcmc <- 100 num_samples <- num_warmstart + num_mcmc beta_init <- 0 global_var_samples <- c(global_variance_init, rep(0, num_samples)) leaf_scale_samples <- c(tau_init, rep(0, num_samples)) beta_samples <- c(beta_init, rep(0, num_samples)) for (i in 1:num_warmstart) {     # Initialize vectors needed for posterior sampling     if (i == 1) {         beta_hat <- beta_init         yhat_forest <- rep(0, n)         partial_res <- resid - yhat_forest     } else {         yhat_forest <- forest_samples$predict_raw_single_forest(forest_dataset, (i-1)-1)         partial_res <- resid - yhat_forest         outcome$add_vector(W %*% beta_hat)     }     # Sample beta from bayesian linear model with gaussian prior     sigma2 <- global_var_samples[i]     beta_posterior_mean <- sum(partial_res*W[,1])/(sigma2 + sum(W[,1]*W[,1]))     beta_posterior_var <- (sigma2*beta_tau)/(sigma2 + sum(W[,1]*W[,1]))     beta_hat <- rnorm(1, beta_posterior_mean, sqrt(beta_posterior_var))     beta_samples[i+1] <- beta_hat     # Update partial residual before sampling forest     outcome$subtract_vector(W %*% beta_hat)          # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, sigma2, cutpoint_grid_size, gfr = T     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     ) } for (i in (num_warmstart+1):num_samples) {     # Initialize vectors needed for posterior sampling     if (i == 1) {         beta_hat <- beta_init         yhat_forest <- rep(0, n)         partial_res <- resid - yhat_forest     } else {         yhat_forest <- forest_samples$predict_raw_single_forest(forest_dataset, (i-1)-1)         partial_res <- resid - yhat_forest         outcome$add_vector(W %*% beta_hat)     }     # Sample beta from bayesian linear model with gaussian prior     sigma2 <- global_var_samples[i]     beta_posterior_mean <- sum(partial_res*W[,1])/(sigma2 + sum(W[,1]*W[,1]))     beta_posterior_var <- (sigma2*beta_tau)/(sigma2 + sum(W[,1]*W[,1]))     beta_hat <- rnorm(1, beta_posterior_mean, sqrt(beta_posterior_var))     beta_samples[i+1] <- beta_hat     # Update partial residual before sampling forest     outcome$subtract_vector(W %*% beta_hat)          # Sample forest     forest_model$sample_one_iteration(         forest_dataset, outcome, forest_samples, rng, feature_types,          outcome_model_type, leaf_prior_scale, var_weights,          1, 1, global_var_samples[i], cutpoint_grid_size, gfr = F     )          # Sample global variance parameter     global_var_samples[i+1] <- sample_sigma2_one_iteration(         outcome, forest_dataset, rng, nu, lambda     ) } # Linear model predictions lm_preds <- (sapply(1:num_samples, function(x) W[,1]*beta_samples[x+1]))*y_std  # Forest predictions forest_preds <- forest_samples$predict(forest_dataset)*y_std + y_bar  # Overall predictions preds <- forest_preds + lm_preds  # Global error variance sigma_samples <- sqrt(global_var_samples)*y_std  # Regression parameter beta_samples <- beta_samples*y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"results-3","dir":"Articles","previous_headings":"Demo 4: Supervised Learning with Additive Linear Model","what":"Results","title":"Custom Sampling Routines in StochTree","text":"Inspect initial samples obtained via grow--root additive random effects model    Inspect BART samples obtained “warm-starting” plus additive random effects model    Now inspect samples BART forest alone (without considering additive linear model predictions)","code":"plot(sigma_samples[1:num_warmstart], ylab=\"sigma\") plot(beta_samples[1:num_warmstart], ylab=\"beta\") plot(rowMeans(preds[,1:num_warmstart]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(sigma_samples[(num_warmstart+1):num_samples], ylab=\"sigma\") plot(beta_samples[(num_warmstart+1):num_samples], ylab=\"beta\") plot(rowMeans(preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(forest_preds[,(num_warmstart+1):num_samples]), y, pch=16,       cex=0.75, xlab = \"pred\", ylab = \"actual\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"demo-5-causal-inference","dir":"Articles","previous_headings":"","what":"Demo 5: Causal Inference","title":"Custom Sampling Routines in StochTree","text":"show implement Bayesian Causal Forest (BCF) model Hahn, Murray, Carvalho (2020) using stochtree’s prototype API, including demoing non-trivial sampling step done R level.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"background","dir":"Articles","previous_headings":"Demo 5: Causal Inference","what":"Background","title":"Custom Sampling Routines in StochTree","text":"supervised learning case previous demo conceptually simple, motivate causal effect estimation task additional notation. Let yy refer continuous outcome interest, ZZ refer binary treatment, XX set covariates may influence YY, ZZ, . XX exhaustive set covariates influence ZZ YY, can specific YY terms causal model (see example Pearl (2009)) Y=F(Z,X,ϵY)\\begin{equation*} \\begin{aligned} Y &= F(Z, X, \\epsilon_Y) \\end{aligned} \\end{equation*} ϵY\\epsilon_Y outcome specific random noise FF function generates YY (many cases, FF can thought inverse CDF conditional XX ZZ). “potential outcomes” (see Imbens Rubin (2015)) can recovered Y1=F(1,X,ϵY)Y^1 = F(1, X, \\epsilon_Y) Y0=F(0,X,ϵY)Y^0 = F(0, X, \\epsilon_Y). causal outcome model can decomposed “mean” “error” terms Y=μ(X)+Zτ(X)+[η(X)+Zδ(X)]μ(X)=𝔼ϵY[F(0,X,ϵY)]τ(X)=𝔼ϵY[F(1,X,ϵY)−F(0,X,ϵY)]η(X)=F(0,X,ϵY)−𝔼ϵY[F(0,X,ϵY)]δ(X)=F(1,X,ϵY)−F(0,X,ϵY)−𝔼ϵY[F(1,X,ϵY)−F(0,X,ϵY)]\\begin{equation*} \\begin{aligned} Y &= \\mu(X) + Z\\tau(X) + \\left[\\eta(X) + Z\\delta(X)\\right]\\\\ \\mu(X) &= \\mathbb{E}_{\\epsilon_Y}\\left[F(0, X, \\epsilon_Y)\\right]\\\\ \\tau(X) &= \\mathbb{E}_{\\epsilon_Y}\\left[F(1, X, \\epsilon_Y) - F(0, X, \\epsilon_Y)\\right]\\\\ \\eta(X) &= F(0, X, \\epsilon_Y) - \\mathbb{E}_{\\epsilon_Y}\\left[F(0, X, \\epsilon_Y)\\right]\\\\ \\delta(X) &= F(1, X, \\epsilon_Y) - F(0, X, \\epsilon_Y) - \\mathbb{E}_{\\epsilon_Y}\\left[F(1, X, \\epsilon_Y) - F(0, X, \\epsilon_Y)\\right] \\end{aligned} \\end{equation*} τ(X)\\tau(X) precisely conditional average treatment effect (CATE) estimand. Unfortunately, functional form FF unavailable analysis, τ(X)\\tau(X) derived. flexible, regularized nonparametrics enter picture, aim estimate μ(X)\\mu(X) τ(X)\\tau(X) data.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"bayesian-causal-forest-bcf","dir":"Articles","previous_headings":"Demo 5: Causal Inference > Background","what":"Bayesian Causal Forest (BCF)","title":"Custom Sampling Routines in StochTree","text":"BCF estimates μ(X)\\mu(X) τ(X)\\tau(X) using separate BART forests term. Furthermore, rather rely common implicit coding ZZ 0 control observations 1 treated observations, consider coding control observations parameter b0b_0 treated observations parameter b1b_1. Placing N(0,1/2)N(0,1/2) prior bzb_z, essentially redefines outcome model y=μ(X)+τ(X)f(Z)+ϵf(Z)=b0(1−Z)+b1Zϵ∼N(0,σ2)b0,b1∼N(0,1/2)\\begin{equation*} \\begin{aligned} y &= \\mu(X) + \\tau(X) f(Z) + \\epsilon\\\\ f(Z) &= b_0(1-Z) + b_1 Z\\\\ \\epsilon &\\sim N\\left(0, \\sigma^2\\right)\\\\ b_0, b_1 &\\sim N\\left(0, 1/2\\right) \\end{aligned} \\end{equation*} Updating bzb_z requires additional Gibbs step, derive . Conditioning sampled forests μ\\mu τ\\tau, essentially regressing y−μ(Z)y - \\mu(Z) [(1−Z)τ(X),Zτ(X)]\\left[(1-Z)\\tau(X), Z\\tau(X)\\right] closed form posterior b0∣y,X,μ,τ∼N(syτ,0sττ,0+2σ2,σ2sττ,0+2σ2)b1∣y,X,μ,τ∼N(syτ,1sττ,1+2σ2,σ2sττ,1+2σ2)\\begin{equation*} \\begin{aligned} b_0 \\mid y, X, \\mu,\\tau &\\sim N\\left(\\frac{s_{y\\tau,0}}{s_{\\tau\\tau,0} + 2\\sigma^2}, \\frac{\\sigma^2}{s_{\\tau\\tau,0} + 2\\sigma^2}\\right)\\\\ b_1 \\mid y, X, \\mu,\\tau &\\sim N\\left(\\frac{s_{y\\tau,1}}{s_{\\tau\\tau,1} + 2\\sigma^2}, \\frac{\\sigma^2}{s_{\\tau\\tau,1} + 2\\sigma^2}\\right) \\end{aligned} \\end{equation*} syτ,z=∑:Zi=z(yi−μ(Xi))τ(Xi)s_{y\\tau,z} = \\sum_{: Z_i = z} (y_i - \\mu(X_i))\\tau(X_i) sττ,z=∑:Zi=zτ(Xi)τ(Xi)s_{\\tau\\tau,z} = \\sum_{: Z_i = z} \\tau(X_i)\\tau(X_i).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"simulation-4","dir":"Articles","previous_headings":"Demo 5: Causal Inference","what":"Simulation","title":"Custom Sampling Routines in StochTree","text":"simulated causal DGP mirrors nonlinear, heterogeneous treatment effect DGP presented Hahn, Murray, Carvalho (2020).","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 4 y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr)  # Standardize outcome y_bar <- mean(y) y_std <- sd(y) resid <- (y-y_bar)/y_std"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"sampling-4","dir":"Articles","previous_headings":"Demo 5: Causal Inference","what":"Sampling","title":"Custom Sampling Routines in StochTree","text":"Set parameters inform forest variance parameter samplers Prepare run sampler (now must specify initial values b0b_0 b1b_1, choose -1/2 1/2 instead 0 1). Initialize R-level access C++ classes needed sample model Run grow--root sampler “warm-start” BART, also updating adaptive coding parameter b0b_0 b1b_1 Pick last GFR forest (associated global variance / leaf scale parameters) MCMC sampler Predict rescale samples","code":"# Mu forest alpha_mu <- 0.95 beta_mu <- 2.0 min_samples_leaf_mu <- 5 max_depth_mu <- 10 num_trees_mu <- 250 cutpoint_grid_size_mu = 100 tau_init_mu = 1/num_trees_mu leaf_prior_scale_mu = matrix(c(tau_init_mu), ncol = 1) a_leaf_mu <- 3. b_leaf_mu <- var(resid)/(num_trees_mu) leaf_regression_mu <- F sigma_leaf_mu <- var(resid)/(num_trees_mu) current_leaf_scale_mu <- as.matrix(sigma_leaf_mu)  # Tau forest alpha_tau <- 0.25 beta_tau <- 3.0 min_samples_leaf_tau <- 5 max_depth_tau <- 10 num_trees_tau <- 50 cutpoint_grid_size_tau = 100 a_leaf_tau <- 3. b_leaf_tau <- var(resid)/(2*num_trees_tau) leaf_regression_tau <- T sigma_leaf_tau <- var(resid)/(2*num_trees_tau) current_leaf_scale_tau <- as.matrix(sigma_leaf_tau)  # Common parameters nu <- 3 sigma2hat <- (sigma(lm(resid~X)))^2 quantile_cutoff <- 0.9 if (is.null(lambda)) {     lambda <- (sigma2hat*qgamma(1-quantile_cutoff,nu))/nu } sigma2 <- sigma2hat current_sigma2 <- sigma2 # Sampling composition num_gfr <- 20 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc  # Sigma^2 samples global_var_samples <- rep(0, num_samples)  # Adaptive coding parameter samples b_0_samples <- rep(0, num_samples) b_1_samples <- rep(0, num_samples) b_0 <- -0.5 b_1 <- 0.5 current_b_0 <- b_0 current_b_1 <- b_1 tau_basis <- (1-Z)*current_b_0 + Z*current_b_1 # Data X_mu <- cbind(X, pi_x) X_tau <- X feature_types <- c(0,0,0,1,1) feature_types_mu <- as.integer(c(feature_types,0)) feature_types_tau <- as.integer(feature_types) variable_weights_mu = rep(1/ncol(X_mu), ncol(X_mu)) variable_weights_tau = rep(1/ncol(X_tau), ncol(X_tau)) forest_dataset_mu <- createForestDataset(X_mu) forest_dataset_tau <- createForestDataset(X_tau, tau_basis) outcome <- createOutcome(resid)  # Random number generator (std::mt19937) rng <- createRNG()  # Sampling data structures forest_model_mu <- createForestModel(     forest_dataset_mu, feature_types_mu, num_trees_mu, nrow(X_mu),      alpha_mu, beta_mu, min_samples_leaf_mu, max_depth_mu ) forest_model_tau <- createForestModel(     forest_dataset_tau, feature_types_tau, num_trees_tau, nrow(X_tau),      alpha_tau, beta_tau, min_samples_leaf_tau, max_depth_tau )  # Container of forest samples forest_samples_mu <- createForestContainer(num_trees_mu, 1, T) forest_samples_tau <- createForestContainer(num_trees_tau, 1, F)  # Initialize the leaves of each tree in the prognostic forest forest_samples_mu$set_root_leaves(0, mean(resid) / num_trees_mu) forest_samples_mu$adjust_residual(     forest_dataset_mu, outcome, forest_model_mu, F, 0, F )  # Initialize the leaves of each tree in the treatment effect forest forest_samples_tau$set_root_leaves(0, 0.) forest_samples_tau$adjust_residual(     forest_dataset_tau, outcome, forest_model_tau, T, 0, F ) if (num_gfr > 0){     for (i in 1:num_gfr) {         # Sample the prognostic forest         forest_model_mu$sample_one_iteration(             forest_dataset_mu, outcome, forest_samples_mu, rng,              feature_types_mu, 0, current_leaf_scale_mu, variable_weights_mu,              1, 1, current_sigma2, cutpoint_grid_size, gfr = T, pre_initialized = T         )                  # Sample variance parameters (if requested)         global_var_samples[i] <- sample_sigma2_one_iteration(             outcome, forest_dataset_mu, rng, nu, lambda         )         current_sigma2 <- global_var_samples[i]          # Sample the treatment forest         forest_model_tau$sample_one_iteration(             forest_dataset_tau, outcome, forest_samples_tau, rng,              feature_types_tau, 1, current_leaf_scale_tau, variable_weights_tau,              1, 1, current_sigma2, cutpoint_grid_size, gfr = T, pre_initialized = T         )                  # Sample adaptive coding parameters         mu_x_raw <- forest_samples_mu$predict_raw_single_forest(forest_dataset_mu, i-1)         tau_x_raw <- forest_samples_tau$predict_raw_single_forest(forest_dataset_tau, i-1)         s_tt0 <- sum(tau_x_raw*tau_x_raw*(Z==0))         s_tt1 <- sum(tau_x_raw*tau_x_raw*(Z==1))         partial_resid_mu <- resid - mu_x_raw         s_ty0 <- sum(tau_x_raw*partial_resid_mu*(Z==0))         s_ty1 <- sum(tau_x_raw*partial_resid_mu*(Z==1))         current_b_0 <- rnorm(1, (s_ty0/(s_tt0 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt0 + 2*current_sigma2)))         current_b_1 <- rnorm(1, (s_ty1/(s_tt1 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt1 + 2*current_sigma2)))         tau_basis <- (1-Z)*current_b_0 + Z*current_b_1         forest_dataset_tau$update_basis(tau_basis)         b_0_samples[i] <- current_b_0         b_1_samples[i] <- current_b_1                  # Sample variance parameters (if requested)         global_var_samples[i] <- sample_sigma2_one_iteration(outcome, forest_dataset_tau, rng, nu, lambda)         current_sigma2 <- global_var_samples[i]     } } if (num_burnin + num_mcmc > 0) {     for (i in (num_gfr+1):num_samples) {         # Sample the prognostic forest         forest_model_mu$sample_one_iteration(             forest_dataset_mu, outcome, forest_samples_mu, rng, feature_types_mu,              0, current_leaf_scale_mu, variable_weights_mu, 1, 1, current_sigma2,              cutpoint_grid_size, gfr = F, pre_initialized = T         )                  # Sample global variance parameter         global_var_samples[i] <- sample_sigma2_one_iteration(outcome, forest_dataset_mu, rng, nu, lambda)         current_sigma2 <- global_var_samples[i]          # Sample the treatment forest         forest_model_tau$sample_one_iteration(             forest_dataset_tau, outcome, forest_samples_tau, rng, feature_types_tau,              1, current_leaf_scale_tau, variable_weights_tau, 1, 1, current_sigma2,              cutpoint_grid_size, gfr = F, pre_initialized = T         )                  # Sample coding parameters         mu_x_raw <- forest_samples_mu$predict_raw_single_forest(forest_dataset_mu, i-1)         tau_x_raw <- forest_samples_tau$predict_raw_single_forest(forest_dataset_tau, i-1)         s_tt0 <- sum(tau_x_raw*tau_x_raw*(Z==0))         s_tt1 <- sum(tau_x_raw*tau_x_raw*(Z==1))         partial_resid_mu <- resid - mu_x_raw         s_ty0 <- sum(tau_x_raw*partial_resid_mu*(Z==0))         s_ty1 <- sum(tau_x_raw*partial_resid_mu*(Z==1))         current_b_0 <- rnorm(1, (s_ty0/(s_tt0 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt0 + 2*current_sigma2)))         current_b_1 <- rnorm(1, (s_ty1/(s_tt1 + 2*current_sigma2)),                               sqrt(current_sigma2/(s_tt1 + 2*current_sigma2)))         tau_basis <- (1-Z)*current_b_0 + Z*current_b_1         forest_dataset_tau$update_basis(tau_basis)         b_0_samples[i] <- current_b_0         b_1_samples[i] <- current_b_1          # Sample global variance parameter         global_var_samples[i] <- sample_sigma2_one_iteration(outcome, forest_dataset_tau, rng, nu, lambda)         current_sigma2 <- global_var_samples[i]     } } # Forest predictions mu_hat <- forest_samples_mu$predict(forest_dataset_mu)*y_std + y_bar tau_hat_raw <- forest_samples_tau$predict_raw(forest_dataset_tau) tau_hat <- t(t(tau_hat_raw) * (b_1_samples - b_0_samples))*y_std y_hat <- mu_hat + tau_hat * as.numeric(Z)  # Global error variance sigma2_samples <- global_var_samples*(y_std^2)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/CustomSamplingRoutine.html","id":"results-4","dir":"Articles","previous_headings":"Demo 5: Causal Inference","what":"Results","title":"Custom Sampling Routines in StochTree","text":"Inspect XBART results    Inspect warm start BART results    Inspect “adaptive coding” parameters b0b_0 b1b_1.","code":"plot(sigma2_samples[1:num_gfr], ylab=\"sigma^2\") plot(rowMeans(mu_hat[,1:num_gfr]), mu_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"prognostic term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(tau_hat[,1:num_gfr]), tau_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"treatment effect term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) mean((rowMeans(tau_hat[,1:num_gfr]) - tau_x)^2) #> [1] 0.3948934 plot(sigma_samples[(num_gfr+1):num_samples], ylab=\"sigma^2\") plot(rowMeans(mu_hat[,(num_gfr+1):num_samples]), mu_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"prognostic term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(tau_hat[,(num_gfr+1):num_samples]), tau_x, pch=16, cex=0.75,       xlab = \"pred\", ylab = \"actual\", main = \"treatment effect term\") abline(0,1,col=\"red\",lty=2,lwd=2.5) mean((rowMeans(tau_hat[,(num_gfr+1):num_samples]) - tau_x)^2) #> [1] 0.363731 plot(b_0_samples, col = \"blue\", ylab = \"Coding parameter draws\",       ylim = c(min(min(b_0_samples), min(b_1_samples)), max(max(b_0_samples), max(b_1_samples)))) points(b_1_samples, col = \"orange\") legend(\"topleft\", legend = c(\"b_0\", \"b_1\"), col = c(\"blue\", \"orange\"), pch = c(1,1))"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/EnsembleKernel.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"Kernel Methods from Tree Ensembles in StochTree","text":"trained tree ensemble strong --sample performance admits natural motivation “distance” two samples: shared leaf membership. number leaves ensemble 1 ss (, tree 1 3 leaves, reserves numbers 1 - 3, turn tree 2 5 leaves, reserves numbers 4 - 8 label leaves, ). dataset nn observations, construct matrix WW follows:    Initialize WW matrix zeroes nn rows many columns leaves ensemble    Let s = 0    jjIN {1,…,m}\\left\\{1,\\dots,m\\right\\}:       Let num_leaves number leaves tree jj       iiIN {1,…,n}\\left\\{1,\\dots,n\\right\\}:          Let k leaf tree jj maps observation ii          Set element Wi,k+s=1W_{,k+s} = 1       Let s = s + num_leaves sparse matrix WW matrix representation basis predictions ensemble (.e. integrating leaf parameters just analyzing leaf indices). ensemble mm trees, can determine proportion trees map observation leaf computing WWT/mW W^T / m. can form basis kernel function used Gaussian process regression, demonstrate . begin, load stochtree package tgp package serve point reference.","code":"library(stochtree) library(tgp) library(MASS) library(Matrix) library(mvtnorm)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/EnsembleKernel.html","id":"demo-1-univariate-supervised-learning","dir":"Articles","previous_headings":"","what":"Demo 1: Univariate Supervised Learning","title":"Kernel Methods from Tree Ensembles in StochTree","text":"begin simulated example tgp package (Gramacy Taddy (2010)). data generating process (DGP) non-stationary single numeric covariate. define training set test set evaluate various approaches modeling sample outcome data.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/EnsembleKernel.html","id":"traditional-gaussian-process","dir":"Articles","previous_headings":"Demo 1: Univariate Supervised Learning","what":"Traditional Gaussian Process","title":"Kernel Methods from Tree Ensembles in StochTree","text":"can use tgp package model data classical Gaussian Process.  Assess RMSE","code":"# Generate the data X_train <- seq(0,20,length=100) X_test <- seq(0,20,length=99) y_train <- (sin(pi*X_train/5) + 0.2*cos(4*pi*X_train/5)) * (X_train <= 9.6) lin_train <- X_train>9.6;  y_train[lin_train] <- -1 + X_train[lin_train]/10 y_train <- y_train + rnorm(length(y_train), sd=0.1) y_test <- (sin(pi*X_test/5) + 0.2*cos(4*pi*X_test/5)) * (X_test <= 9.6) lin_test <- X_test>9.6;  y_test[lin_test] <- -1 + X_test[lin_test]/10  # Fit the GP model_gp <- bgp(X=X_train, Z=y_train, XX=X_test) plot(model_gp$ZZ.mean, y_test, xlab = \"predicted\", ylab = \"actual\", main = \"Gaussian process\") abline(0,1,lwd=2.5,lty=3,col=\"red\") sqrt(mean((model_gp$ZZ.mean - y_test)^2)) sqrt(mean((model_gp$ZZ.mean - y_test)^2)) #> [1] 0.0466081"},{"path":"https://stochastictree.github.io/stochtree-r/articles/EnsembleKernel.html","id":"bart-based-gaussian-process","dir":"Articles","previous_headings":"Demo 1: Univariate Supervised Learning","what":"BART-based Gaussian process","title":"Kernel Methods from Tree Ensembles in StochTree","text":"Assess RMSE","code":"# Run BART on the data num_trees <- 200 sigma_leaf <- 1/num_trees X_train <- as.data.frame(X_train) X_test <- as.data.frame(X_test) colnames(X_train) <- colnames(X_test) <- \"x1\" bart_params <- list(num_trees_mean=num_trees, sigma_leaf_init=sigma_leaf) bart_model <- bart(X_train=X_train, y_train=y_train, X_test=X_test, params = bart_params)  # Extract kernels needed for kriging leaf_mat_train <- computeForestLeafIndices(bart_model, X_train, forest_type = \"mean\",                                             forest_inds = bart_model$model_params$num_samples) leaf_mat_test <- computeForestLeafIndices(bart_model, X_test, forest_type = \"mean\",                                             forest_inds = bart_model$model_params$num_samples) W_train <- sparseMatrix(i=rep(1:length(y_train),num_trees), j=leaf_mat_train + 1, x=1) W_test <- sparseMatrix(i=rep(1:length(y_test),num_trees), j=leaf_mat_test + 1, x=1) Sigma_11 <- tcrossprod(W_test) / num_trees Sigma_12 <- tcrossprod(W_test, W_train) / num_trees Sigma_22 <- tcrossprod(W_train) / num_trees Sigma_22_inv <- ginv(as.matrix(Sigma_22)) Sigma_21 <- t(Sigma_12)  # Compute mean and covariance for the test set posterior mu_tilde <- Sigma_12 %*% Sigma_22_inv %*% y_train Sigma_tilde <- as.matrix((sigma_leaf)*(Sigma_11 - Sigma_12 %*% Sigma_22_inv %*% Sigma_21))  # Sample from f(X_test) | X_test, X_train, f(X_train) gp_samples <- mvtnorm::rmvnorm(1000, mean = mu_tilde, sigma = Sigma_tilde)  # Compute posterior mean predictions for f(X_test) yhat_mean_test <- colMeans(gp_samples) plot(yhat_mean_test, y_test, xlab = \"predicted\", ylab = \"actual\", main = \"BART Gaussian process\") abline(0,1,lwd=2.5,lty=3,col=\"red\") sqrt(mean((yhat_mean_test - y_test)^2)) #> [1] 0.08974122"},{"path":"https://stochastictree.github.io/stochtree-r/articles/EnsembleKernel.html","id":"demo-2-multivariate-supervised-learning","dir":"Articles","previous_headings":"","what":"Demo 2: Multivariate Supervised Learning","title":"Kernel Methods from Tree Ensembles in StochTree","text":"proceed simulated “Friedman” dataset, implemented tgp.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/EnsembleKernel.html","id":"traditional-gaussian-process-1","dir":"Articles","previous_headings":"Demo 2: Multivariate Supervised Learning","what":"Traditional Gaussian Process","title":"Kernel Methods from Tree Ensembles in StochTree","text":"can use tgp package model data classical Gaussian Process.  Assess RMSE","code":"# Generate the data, add many \"noise variables\" n <- 100 friedman.df <- friedman.1.data(n=n) train_inds <- sort(sample(1:n, floor(0.8*n), replace = FALSE)) test_inds <- (1:n)[!((1:n) %in% train_inds)] X <- as.matrix(friedman.df)[,1:10] X <- cbind(X, matrix(runif(n*10), ncol = 10)) y <- as.matrix(friedman.df)[,12] + rnorm(n,0,1)*(sd(as.matrix(friedman.df)[,11])/2) X_train <- X[train_inds,] X_test <- X[test_inds,] y_train <- y[train_inds] y_test <- y[test_inds]  # Fit the GP model_gp <- bgp(X=X_train, Z=y_train, XX=X_test) plot(model_gp$ZZ.mean, y_test, xlab = \"predicted\", ylab = \"actual\", main = \"Gaussian process\") abline(0,1,lwd=2.5,lty=3,col=\"red\") sqrt(mean((model_gp$ZZ.mean - y_test)^2)) #> [1] 5.023593"},{"path":"https://stochastictree.github.io/stochtree-r/articles/EnsembleKernel.html","id":"bart-based-gaussian-process-1","dir":"Articles","previous_headings":"Demo 2: Multivariate Supervised Learning","what":"BART-based Gaussian process","title":"Kernel Methods from Tree Ensembles in StochTree","text":"Assess RMSE use case BART kernel classical kriging perhaps unclear without empirical investigation, see later vignette kernel approach can beneficial causal inference applications.","code":"# Run BART on the data num_trees <- 200 sigma_leaf <- 1/num_trees X_train <- as.data.frame(X_train) X_test <- as.data.frame(X_test) bart_params <- list(num_trees_mean=num_trees) bart_model <- bart(X_train=X_train, y_train=y_train, X_test=X_test, params = bart_params)  # Extract kernels needed for kriging leaf_mat_train <- computeForestLeafIndices(bart_model, X_train, forest_type = \"mean\",                                             forest_inds = bart_model$model_params$num_samples) leaf_mat_test <- computeForestLeafIndices(bart_model, X_test, forest_type = \"mean\",                                             forest_inds = bart_model$model_params$num_samples) W_train <- sparseMatrix(i=rep(1:length(y_train),num_trees), j=leaf_mat_train + 1, x=1) W_test <- sparseMatrix(i=rep(1:length(y_test),num_trees), j=leaf_mat_test + 1, x=1) Sigma_11 <- tcrossprod(W_test) / num_trees Sigma_12 <- tcrossprod(W_test, W_train) / num_trees Sigma_22 <- tcrossprod(W_train) / num_trees Sigma_22_inv <- ginv(as.matrix(Sigma_22)) Sigma_21 <- t(Sigma_12)  # Compute mean and covariance for the test set posterior mu_tilde <- Sigma_12 %*% Sigma_22_inv %*% y_train Sigma_tilde <- as.matrix((sigma_leaf)*(Sigma_11 - Sigma_12 %*% Sigma_22_inv %*% Sigma_21))  # Sample from f(X_test) | X_test, X_train, f(X_train) gp_samples <- mvtnorm::rmvnorm(1000, mean = mu_tilde, sigma = Sigma_tilde)  # Compute posterior mean predictions for f(X_test) yhat_mean_test <- colMeans(gp_samples) plot(yhat_mean_test, y_test, xlab = \"predicted\", ylab = \"actual\", main = \"BART Gaussian process\") abline(0,1,lwd=2.5,lty=3,col=\"red\") sqrt(mean((yhat_mean_test - y_test)^2)) #> [1] 5.364489"},{"path":[]},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"simulation","dir":"Articles","previous_headings":"Demo 1: Variance-Only Simulation (simple DGP)","what":"Simulation","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":", generate data constant (zero) mean relatively simple covariate-modified variance function. y=0+σ2(X)ϵσ2(X)={0.25X1≥0 X1<0.251X1≥0.25 X1<0.54X1≥0.5 X1<0.759X1≥0.75 X1<1X1,…,Xp∼U(0,1)ϵ∼𝒩(0,1)\\begin{equation*} \\begin{aligned} y &= 0 + \\sigma^2(X) \\epsilon\\\\ \\sigma^2(X) &= \\begin{cases} 0.25 & X_1 \\geq 0 \\text{ } X_1 < 0.25\\\\ 1 & X_1 \\geq 0.25 \\text{ } X_1 < 0.5\\\\ 4 & X_1 \\geq 0.5 \\text{ } X_1 < 0.75\\\\ 9 & X_1 \\geq 0.75 \\text{ } X_1 < 1\\\\ \\end{cases}\\\\ X_1,\\dots,X_p &\\sim \\text{U}\\left(0,1\\right)\\\\ \\epsilon &\\sim \\mathcal{N}\\left(0,1\\right) \\end{aligned} \\end{equation*}","code":"# Generate the data n <- 500 p_x <- 10 X <- matrix(runif(n*p_x), ncol = p_x) f_XW <- 0 s_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (0.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (1) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3) ) y <- f_XW + rnorm(n, 0, 1)*s_XW  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- NULL W_train <- NULL y_test <- y[test_inds] y_train <- y[train_inds] f_x_test <- f_XW[test_inds] f_x_train <- f_XW[train_inds] s_x_test <- s_XW[test_inds] s_x_train <- s_XW[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"warmstart","dir":"Articles","previous_headings":"Demo 1: Variance-Only Simulation (simple DGP) > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"first sample σ2(X)\\sigma^2(X) ensemble using “warm-start” initialization (Hahn (2023)). default stochtree. Inspect MCMC samples","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_trees <- 20 a_0 <- 1.5 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 0, num_trees_variance = num_trees,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_warmstart <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_warmstart$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"mcmc","dir":"Articles","previous_headings":"Demo 1: Variance-Only Simulation (simple DGP) > Sampling and Analysis","what":"MCMC","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"now sample σ2(X)\\sigma^2(X) ensemble using MCMC root initialization (Chipman, George, McCulloch (2010)). Inspect MCMC samples","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 0, num_trees_variance = 50,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_mcmc <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_mcmc$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"simulation-1","dir":"Articles","previous_headings":"Demo 2: Variance-Only Simulation (complex DGP)","what":"Simulation","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":", generate data constant (zero) mean complex covariate-modified variance function. y=0+σ2(X)ϵσ2(X)={0.25X32X1≥0 X1<0.251X32X1≥0.25 X1<0.54X32X1≥0.5 X1<0.759X32X1≥0.75 X1<1X1,…,Xp∼U(0,1)ϵ∼𝒩(0,1)\\begin{equation*} \\begin{aligned} y &= 0 + \\sigma^2(X) \\epsilon\\\\ \\sigma^2(X) &= \\begin{cases} 0.25X_3^2 & X_1 \\geq 0 \\text{ } X_1 < 0.25\\\\ 1X_3^2 & X_1 \\geq 0.25 \\text{ } X_1 < 0.5\\\\ 4X_3^2 & X_1 \\geq 0.5 \\text{ } X_1 < 0.75\\\\ 9X_3^2 & X_1 \\geq 0.75 \\text{ } X_1 < 1\\\\ \\end{cases}\\\\ X_1,\\dots,X_p &\\sim \\text{U}\\left(0,1\\right)\\\\ \\epsilon &\\sim \\mathcal{N}\\left(0,1\\right) \\end{aligned} \\end{equation*}","code":"# Generate the data n <- 500 p_x <- 10 X <- matrix(runif(n*p_x), ncol = p_x) f_XW <- 0 s_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (0.5*X[,3]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (1*X[,3]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2*X[,3]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3*X[,3]) ) y <- f_XW + rnorm(n, 0, 1)*s_XW  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- NULL W_train <- NULL y_test <- y[test_inds] y_train <- y[train_inds] f_x_test <- f_XW[test_inds] f_x_train <- f_XW[train_inds] s_x_test <- s_XW[test_inds] s_x_train <- s_XW[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"warmstart-1","dir":"Articles","previous_headings":"Demo 2: Variance-Only Simulation (complex DGP) > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"first sample σ2(X)\\sigma^2(X) ensemble using “warm-start” initialization (Hahn (2023)). default stochtree. Inspect MCMC samples","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 0, num_trees_variance = 50,                      alpha_mean = 0.95, beta_mean = 2, min_samples_leaf_mean = 5,                      alpha_variance = 0.95, beta_variance = 1.25, min_samples_leaf_variance = 1,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_warmstart <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_warmstart$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"mcmc-1","dir":"Articles","previous_headings":"Demo 2: Variance-Only Simulation (complex DGP) > Sampling and Analysis","what":"MCMC","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"now sample σ2(X)\\sigma^2(X) ensemble using MCMC root initialization (Chipman, George, McCulloch (2010)). Inspect MCMC samples","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 0, num_trees_variance = 50,                      alpha_mean = 0.95, beta_mean = 2, min_samples_leaf_mean = 5,                      alpha_variance = 0.95, beta_variance = 1.25, min_samples_leaf_variance = 5,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_mcmc <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_mcmc$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"simulation-2","dir":"Articles","previous_headings":"Demo 3: Mean and Variance Simulation (simple DGP)","what":"Simulation","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":", generate data (relatively simple) covariate-modified mean variance functions. y=f(X)+σ2(X)ϵf(X)={−6X2≥0 X2<0.25−2X2≥0.25 X2<0.52X2≥0.5 X2<0.756X2≥0.75 X2<1σ2(X)={0.25X1≥0 X1<0.251X1≥0.25 X1<0.54X1≥0.5 X1<0.759X1≥0.75 X1<1X1,…,Xp∼U(0,1)ϵ∼𝒩(0,1)\\begin{equation*} \\begin{aligned} y &= f(X) + \\sigma^2(X) \\epsilon\\\\ f(X) &= \\begin{cases} -6 & X_2 \\geq 0 \\text{ } X_2 < 0.25\\\\ -2 & X_2 \\geq 0.25 \\text{ } X_2 < 0.5\\\\ 2 & X_2 \\geq 0.5 \\text{ } X_2 < 0.75\\\\ 6 & X_2 \\geq 0.75 \\text{ } X_2 < 1\\\\ \\end{cases}\\\\ \\sigma^2(X) &= \\begin{cases} 0.25 & X_1 \\geq 0 \\text{ } X_1 < 0.25\\\\ 1 & X_1 \\geq 0.25 \\text{ } X_1 < 0.5\\\\ 4 & X_1 \\geq 0.5 \\text{ } X_1 < 0.75\\\\ 9 & X_1 \\geq 0.75 \\text{ } X_1 < 1\\\\ \\end{cases}\\\\ X_1,\\dots,X_p &\\sim \\text{U}\\left(0,1\\right)\\\\ \\epsilon &\\sim \\mathcal{N}\\left(0,1\\right) \\end{aligned} \\end{equation*}","code":"# Generate the data n <- 500 p_x <- 10 X <- matrix(runif(n*p_x), ncol = p_x) f_XW <- (     ((0 <= X[,2]) & (0.25 > X[,2])) * (-6) +      ((0.25 <= X[,2]) & (0.5 > X[,2])) * (-2) +      ((0.5 <= X[,2]) & (0.75 > X[,2])) * (2) +      ((0.75 <= X[,2]) & (1 > X[,2])) * (6) ) s_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (0.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (1) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3) ) y <- f_XW + rnorm(n, 0, 1)*s_XW  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- NULL W_train <- NULL y_test <- y[test_inds] y_train <- y[train_inds] f_x_test <- f_XW[test_inds] f_x_train <- f_XW[train_inds] s_x_test <- s_XW[test_inds] s_x_train <- s_XW[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"warmstart-2","dir":"Articles","previous_headings":"Demo 3: Mean and Variance Simulation (simple DGP) > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"first sample σ2(X)\\sigma^2(X) ensemble using “warm-start” initialization (Hahn (2023)). default stochtree. Inspect MCMC samples","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 50, num_trees_variance = 50,                      alpha_mean = 0.95, beta_mean = 2, min_samples_leaf_mean = 5,                      alpha_variance = 0.95, beta_variance = 1.25, min_samples_leaf_variance = 5,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_warmstart <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_warmstart$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"mean function\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_warmstart$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"mcmc-2","dir":"Articles","previous_headings":"Demo 3: Mean and Variance Simulation (simple DGP) > Sampling and Analysis","what":"MCMC","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"now sample σ2(X)\\sigma^2(X) ensemble using MCMC root initialization (Chipman, George, McCulloch (2010)). Inspect MCMC samples","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 50, num_trees_variance = 50,                      alpha_mean = 0.95, beta_mean = 2, min_samples_leaf_mean = 5,                      alpha_variance = 0.95, beta_variance = 1.25, min_samples_leaf_variance = 5,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_mcmc <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_mcmc$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"mean function\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_mcmc$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"simulation-3","dir":"Articles","previous_headings":"Demo 4: Mean and Variance Simulation (complex DGP)","what":"Simulation","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":", generate data complex covariate-modified mean variance functions. y=f(X)+σ2(X)ϵf(X)={−6X4X2≥0 X2<0.25−2X4X2≥0.25 X2<0.52X4X2≥0.5 X2<0.756X4X2≥0.75 X2<1σ2(X)={0.25X32X1≥0 X1<0.251X32X1≥0.25 X1<0.54X32X1≥0.5 X1<0.759X32X1≥0.75 X1<1X1,…,Xp∼U(0,1)ϵ∼𝒩(0,1)\\begin{equation*} \\begin{aligned} y &= f(X) + \\sigma^2(X) \\epsilon\\\\ f(X) &= \\begin{cases} -6X_4 & X_2 \\geq 0 \\text{ } X_2 < 0.25\\\\ -2X_4 & X_2 \\geq 0.25 \\text{ } X_2 < 0.5\\\\ 2X_4 & X_2 \\geq 0.5 \\text{ } X_2 < 0.75\\\\ 6X_4 & X_2 \\geq 0.75 \\text{ } X_2 < 1\\\\ \\end{cases}\\\\ \\sigma^2(X) &= \\begin{cases} 0.25X_3^2 & X_1 \\geq 0 \\text{ } X_1 < 0.25\\\\ 1X_3^2 & X_1 \\geq 0.25 \\text{ } X_1 < 0.5\\\\ 4X_3^2 & X_1 \\geq 0.5 \\text{ } X_1 < 0.75\\\\ 9X_3^2 & X_1 \\geq 0.75 \\text{ } X_1 < 1\\\\ \\end{cases}\\\\ X_1,\\dots,X_p &\\sim \\text{U}\\left(0,1\\right)\\\\ \\epsilon &\\sim \\mathcal{N}\\left(0,1\\right) \\end{aligned} \\end{equation*}","code":"# Generate the data n <- 500 p_x <- 10 X <- matrix(runif(n*p_x), ncol = p_x) f_XW <- (     ((0 <= X[,2]) & (0.25 > X[,2])) * (-6*X[,4]) +      ((0.25 <= X[,2]) & (0.5 > X[,2])) * (-2*X[,4]) +      ((0.5 <= X[,2]) & (0.75 > X[,2])) * (2*X[,4]) +      ((0.75 <= X[,2]) & (1 > X[,2])) * (6*X[,4]) ) s_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (0.5*X[,3]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (1*X[,3]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2*X[,3]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3*X[,3]) ) y <- f_XW + rnorm(n, 0, 1)*s_XW  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- NULL W_train <- NULL y_test <- y[test_inds] y_train <- y[train_inds] f_x_test <- f_XW[test_inds] f_x_train <- f_XW[train_inds] s_x_test <- s_XW[test_inds] s_x_train <- s_XW[train_inds]"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"warmstart-3","dir":"Articles","previous_headings":"Demo 4: Mean and Variance Simulation (complex DGP) > Sampling and Analysis","what":"Warmstart","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"first sample σ2(X)\\sigma^2(X) ensemble using “warm-start” initialization (Hahn (2023)). default stochtree. Inspect MCMC samples","code":"num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 50, num_trees_variance = 50,                      alpha_mean = 0.95, beta_mean = 2, min_samples_leaf_mean = 5,                      alpha_variance = 0.95, beta_variance = 1.25, min_samples_leaf_variance = 5,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_warmstart <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_warmstart$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"mean function\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_warmstart$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/Heteroskedasticity.html","id":"mcmc-3","dir":"Articles","previous_headings":"Demo 4: Mean and Variance Simulation (complex DGP) > Sampling and Analysis","what":"MCMC","title":"Bayesian Supervised Learning with Heteroskedasticity in StochTree","text":"now sample σ2(X)\\sigma^2(X) ensemble using MCMC root initialization (Chipman, George, McCulloch (2010)). Inspect MCMC samples","code":"num_gfr <- 0 num_burnin <- 1000 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 50, num_trees_variance = 50,                      alpha_mean = 0.95, beta_mean = 2, min_samples_leaf_mean = 5,                      alpha_variance = 0.95, beta_variance = 1.25, min_samples_leaf_variance = 5,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model_mcmc <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params ) plot(rowMeans(bart_model_mcmc$y_hat_test), y_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"mean function\") abline(0,1,col=\"red\",lty=2,lwd=2.5) plot(rowMeans(bart_model_mcmc$sigma_x_hat_test), s_x_test,       pch=16, cex=0.75, xlab = \"pred\", ylab = \"actual\", main = \"standard deviation function\") abline(0,1,col=\"red\",lty=2,lwd=2.5)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"demo-1-bayesian-causal-forest-bcf","dir":"Articles","previous_headings":"","what":"Demo 1: Bayesian Causal Forest (BCF)","title":"Model Serialization in StochTree","text":"BCF models initially sampled constructed using bcf() function. show save reload models JSON files disk.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"model-building","dir":"Articles","previous_headings":"Demo 1: Bayesian Causal Forest (BCF)","what":"Model Building","title":"Model Serialization in StochTree","text":"Draw modified version data generating process defined Hahn, Murray, Carvalho (2020). Sample BCF model.","code":"# Generate synthetic data n <- 1000 snr <- 2 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE)  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bcf_params <- list(sample_sigma_leaf_mu = F, sample_sigma_leaf_tau = F) bcf_model <- bcf(     X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,      group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,      X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,      rfx_basis_test = rfx_basis_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bcf_params )"},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"serialization","dir":"Articles","previous_headings":"Demo 1: Bayesian Causal Forest (BCF)","what":"Serialization","title":"Model Serialization in StochTree","text":"Save BCF model disk.","code":"saveBCFModelToJsonFile(bcf_model, \"bcf.json\")"},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"deserialization","dir":"Articles","previous_headings":"Demo 1: Bayesian Causal Forest (BCF)","what":"Deserialization","title":"Model Serialization in StochTree","text":"Reload BCF model disk. Check predictions align original model.","code":"bcf_model_reload <- createBCFModelFromJsonFile(\"bcf.json\") bcf_preds_reload <- predict(bcf_model_reload, X_train, Z_train, pi_train, group_ids_train, rfx_basis_train) plot(rowMeans(bcf_model$mu_hat_train), rowMeans(bcf_preds_reload$mu_hat),       xlab = \"Original\", ylab = \"Deserialized\", main = \"Prognostic forest\") abline(0,1,col=\"red\",lwd=3,lty=3) plot(rowMeans(bcf_model$tau_hat_train), rowMeans(bcf_preds_reload$tau_hat),       xlab = \"Original\", ylab = \"Deserialized\", main = \"Treatment forest\") abline(0,1,col=\"red\",lwd=3,lty=3) plot(rowMeans(bcf_model$y_hat_train), rowMeans(bcf_preds_reload$y_hat),       xlab = \"Original\", ylab = \"Deserialized\", main = \"Overall outcome\") abline(0,1,col=\"red\",lwd=3,lty=3)"},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"demo-2-bart","dir":"Articles","previous_headings":"","what":"Demo 2: BART","title":"Model Serialization in StochTree","text":"BART models initially sampled constructed using bart() function. show save reload models JSON files disk.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"model-building-1","dir":"Articles","previous_headings":"Demo 2: BART","what":"Model Building","title":"Model Serialization in StochTree","text":"Draw relatively straightforward heteroskedastic supervised learning DGP. Sample BART model.","code":"# Generate the data n <- 500 p_x <- 10 X <- matrix(runif(n*p_x), ncol = p_x) f_XW <- 0 s_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (0.5*X[,3]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (1*X[,3]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2*X[,3]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (3*X[,3]) ) y <- f_XW + rnorm(n, 0, 1)*s_XW  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- NULL W_train <- NULL y_test <- y[test_inds] y_train <- y[train_inds] f_x_test <- f_XW[test_inds] f_x_train <- f_XW[train_inds] s_x_test <- s_XW[test_inds] s_x_train <- s_XW[train_inds] num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_samples <- num_gfr + num_burnin + num_mcmc bart_params <- list(num_trees_mean = 100, num_trees_variance = 50,                      alpha_mean = 0.95, beta_mean = 2, min_samples_leaf_mean = 5,                      alpha_variance = 0.95, beta_variance = 1.25,                      min_samples_leaf_variance = 1,                      sample_sigma_global = F, sample_sigma_leaf = F) bart_model <- stochtree::bart(     X_train = X_train, y_train = y_train, X_test = X_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,      params = bart_params )"},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"serialization-1","dir":"Articles","previous_headings":"Demo 2: BART","what":"Serialization","title":"Model Serialization in StochTree","text":"Save BART model disk.","code":"saveBARTModelToJsonFile(bart_model, \"bart.json\")"},{"path":"https://stochastictree.github.io/stochtree-r/articles/ModelSerialization.html","id":"deserialization-1","dir":"Articles","previous_headings":"Demo 2: BART","what":"Deserialization","title":"Model Serialization in StochTree","text":"Reload BART model disk. Check predictions align original model.","code":"bart_model_reload <- createBARTModelFromJsonFile(\"bart.json\") bart_preds_reload <- predict(bart_model_reload, X_train) plot(rowMeans(bart_model$y_hat_train), rowMeans(bart_preds_reload$y_hat),       xlab = \"Original\", ylab = \"Deserialized\", main = \"Conditional Mean Estimates\") abline(0,1,col=\"red\",lwd=3,lty=3) plot(rowMeans(bart_model$sigma_x_hat_train), rowMeans(bart_preds_reload$variance_forest_predictions),       xlab = \"Original\", ylab = \"Deserialized\", main = \"Conditional Variance Estimates\") abline(0,1,col=\"red\",lwd=3,lty=3)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/MultiChain.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"Running Multiple Chains (Sequentially or in Parallel) in StochTree","text":"Mixing MCMC sampler perennial concern complex Bayesian models, BART exception. common way address concerns run multiple independent “chains” MCMC sampler, chain gets stuck different region posterior, combined samples attain better coverage full posterior. idea works classic “-root” MCMC sampler Chipman, George, McCulloch (2010), key insight Hahn (2023) XBART algorithm may used warm-start initialize multiple chains BART MCMC sampler. Operationally, two approaches implementation (setting num_gfr > 0 warm-start initialization desired), vignette demonstrate run multi-chain sampler sequentially parallel. begin, load stochtree necessary packages","code":"library(stochtree) library(foreach) library(doParallel) #> Loading required package: iterators #> Loading required package: parallel"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/MultiChain.html","id":"data-simulation","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Data Simulation","title":"Running Multiple Chains (Sequentially or in Parallel) in StochTree","text":"Simulate simple partitioned linear model","code":"# Generate the data n <- 500 p_x <- 10 p_w <- 1 snr <- 3 X <- matrix(runif(n*p_x), ncol = p_x) W <- matrix(runif(n*p_w), ncol = p_w) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5*W[,1]) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5*W[,1]) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5*W[,1]) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5*W[,1]) ) noise_sd <- sd(f_XW) / snr y <- f_XW + rnorm(n, 0, 1)*noise_sd  # Split data into test and train sets test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- as.data.frame(X[test_inds,]) X_train <- as.data.frame(X[train_inds,]) W_test <- W[test_inds,] W_train <- W[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds]"},{"path":"https://stochastictree.github.io/stochtree-r/articles/MultiChain.html","id":"sampling-multiple-chains-sequentially","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Sampling Multiple Chains Sequentially","title":"Running Multiple Chains (Sequentially or in Parallel) in StochTree","text":"Define high-level parameters, including number chains run number samples per chain. run 4 independent chains 5 warm-start iterations 100 MCMC iterations . Run sampler, storing resulting BART objects list Now, want combine forests BART models single forest, can follows can predict combined forest follows Compare original ŷ\\hat{y} values","code":"num_chains <- 4 num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_trees <- 100 bart_models <- list() bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T, num_trees_mean = num_trees) for (i in 1:num_chains) {     bart_models[[i]] <- stochtree::bart(         X_train = X_train, W_train = W_train, y_train = y_train, X_test = X_test,          W_test = W_test, num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc,          params = bart_params     ) } json_string_list <- list() for (i in 1:num_chains) {     json_string_list[[i]] <- saveBARTModelToJsonString(bart_models[[i]]) } combined_bart <- createBARTModelFromCombinedJsonString(json_string_list) yhat_combined <- predict(combined_bart, X_test, W_test)$y_hat par(mfrow = c(1,2)) for (i in 1:num_chains) {     offset <- (i-1)*num_mcmc     inds_start <- offset + 1     inds_end <- offset + num_mcmc     plot(rowMeans(bart_models[[i]]$y_hat_test),           rowMeans(yhat_combined[,inds_start:inds_end]),          xlab = \"original\", ylab = \"deserialized\",           main = paste0(\"Chain \", i, \"\\nPredictions\"))     abline(0,1,col=\"red\",lty=3,lwd=3) } par(mfrow = c(1,1))"},{"path":"https://stochastictree.github.io/stochtree-r/articles/MultiChain.html","id":"sampling-multiple-chains-in-parallel","dir":"Articles","previous_headings":"Demo 1: Supervised Learning","what":"Sampling Multiple Chains in Parallel","title":"Running Multiple Chains (Sequentially or in Parallel) in StochTree","text":"use high-level parameters sequential demo. order run sampler parallel, parallel backend must registered R environment. code register parallel backend access many cores available machine. Note evaluate code snippet order interact nicely CRAN / Github Actions environments. Note bartmodel object contains external pointers forests created stochtree shared object, stochtree::bart() run parallel independent subprocesses, pointers generally accessible session kicked parallel run. overcome , can return JSON representation bartmodel memory combine single -memory bartmodel object. first step process run sampler parallel, storing resulting BART JSON strings list. Close parallel cluster (evaluated , explained ). Now, want combine forests BART models single forest, can follows can predict combined forest follows Compare average predictions chain original predictions.  true yy values.","code":"num_chains <- 4 num_gfr <- 10 num_burnin <- 0 num_mcmc <- 100 num_trees <- 100 ncores <- parallel::detectCores() cl <- makeCluster(ncores) registerDoParallel(cl) bart_model_outputs <- foreach (i = 1:num_chains) %dopar% {   random_seed <- i   bart_params <- list(sample_sigma_global = T, sample_sigma_leaf = T,                        num_trees_mean = num_trees, random_seed = random_seed)   bart_model <- stochtree::bart(     X_train = X_train, W_train = W_train, y_train = y_train, X_test = X_test, W_test = W_test,      num_gfr = num_gfr, num_burnin = num_burnin, num_mcmc = num_mcmc, params = bart_params   )   bart_model_string <- stochtree::saveBARTModelToJsonString(bart_model)   y_hat_test <- bart_model$y_hat_test   list(model=bart_model_string, yhat=y_hat_test) } #> Warning: executing %dopar% sequentially: no parallel backend registered stopCluster(cl) bart_model_strings <- list() bart_model_yhats <- matrix(NA, nrow = length(y_test), ncol = num_chains) for (i in 1:length(bart_model_outputs)) {     bart_model_strings[[i]] <- bart_model_outputs[[i]]$model     bart_model_yhats[,i] <- rowMeans(bart_model_outputs[[i]]$yhat) } combined_bart <- createBARTModelFromCombinedJsonString(bart_model_strings) yhat_combined <- predict(combined_bart, X_test, W_test)$y_hat par(mfrow = c(1,2)) for (i in 1:num_chains) {     offset <- (i-1)*num_mcmc     inds_start <- offset + 1     inds_end <- offset + num_mcmc     plot(rowMeans(yhat_combined[,inds_start:inds_end]), bart_model_yhats[,i],          xlab = \"deserialized\", ylab = \"original\",           main = paste0(\"Chain \", i, \"\\nPredictions\"))     abline(0,1,col=\"red\",lty=3,lwd=3) } par(mfrow = c(1,1)) par(mfrow = c(1,2)) for (i in 1:num_chains) {     offset <- (i-1)*num_mcmc     inds_start <- offset + 1     inds_end <- offset + num_mcmc     plot(rowMeans(yhat_combined[,inds_start:inds_end]), y_test,          xlab = \"predicted\", ylab = \"actual\",           main = paste0(\"Chain \", i, \"\\nPredictions\"))     abline(0,1,col=\"red\",lty=3,lwd=3) } par(mfrow = c(1,1))"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/articles/PriorCalibration.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Prior Calibration Approaches for Parametric Components of Stochastic Tree Ensembles","text":"“classic” BART model Chipman, George, McCulloch (2010) y=f(X)+ϵf(X)∼BART(α,β)ϵ∼𝒩(0,σ2)σ2∼IG(,b)\\begin{equation*} \\begin{aligned} y &= f(X) + \\epsilon\\\\ f(X) &\\sim \\text{BART}\\left(\\alpha, \\beta\\right)\\\\ \\epsilon &\\sim \\mathcal{N}\\left(0,\\sigma^2\\right)\\\\ \\sigma^2 &\\sim \\text{IG}\\left(,b\\right) \\end{aligned} \\end{equation*} semiparametric, nonparametric tree ensemble f(X)f(X) homoskedastic error variance parameter σ2\\sigma^2. Note Chipman, George, McCulloch (2010), aa bb parameterized =ν2a = \\frac{\\nu}{2} b=νλ2b = \\frac{\\nu\\lambda}{2}.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/articles/PriorCalibration.html","id":"setting-priors-on-variance-parameters-in-stochtree","dir":"Articles","previous_headings":"","what":"Setting Priors on Variance Parameters in stochtree","title":"Prior Calibration Approaches for Parametric Components of Stochastic Tree Ensembles","text":"default, stochtree employs Jeffreys’ prior σ2\\sigma^2σ2∝1σ2\\begin{equation*} \\begin{aligned} \\sigma^2 &\\propto \\frac{1}{\\sigma^2} \\end{aligned} \\end{equation*} corresponds improper prior =0a = 0 b=0b = 0. provide convenience functions users wishing set σ2\\sigma^2 prior Chipman, George, McCulloch (2010). case, ν\\nu set default 3 λ\\lambda calibrated follows: “overestimate,” σ̂2\\hat{\\sigma}^2, σ2\\sigma^2 obtained via simple linear regression yy XX λ\\lambda chosen ensure p(σ2<σ̂2)=qp(\\sigma^2 < \\hat{\\sigma}^2) = q value qq, typically set default value 0.9. done stochtree via calibrate_inverse_gamma_error_variance function. Now run BART model variance parameterization Inspect --sample predictions model  Inspect posterior samples σ2\\sigma^2","code":"# Load library library(stochtree)  # Generate data n <- 500 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd)  # Test/train split test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds]  # Calibrate the scale parameter for the variance term as in Chipman et al (2010) nu <- 3 lambda <- calibrate_inverse_gamma_error_variance(y_train, X_train, nu = nu) bart_params <- list(a_global = nu/2, b_global = (nu*lambda)/2) bart_model <- bart(X_train = X_train, y_train = y_train, X_test = X_test,                     num_gfr = 0, num_burnin = 1000, num_mcmc = 100,                     params = bart_params) plot(rowMeans(bart_model$y_hat_test), y_test, xlab = \"predicted\", ylab = \"actual\") abline(0,1,col=\"red\",lty=3,lwd=3) plot(bart_model$sigma2_global_samples, ylab = \"sigma^2\", xlab = \"iteration\") abline(h = noise_sd^2, col = \"red\", lty = 3, lwd = 3)"},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Drew Herren. Author, maintainer. Richard Hahn. Author. Jared Murray. Author. Carlos Carvalho. Author. Jingyu . Author.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Herren D, Hahn R, Murray J, Carvalho C, J (2024). stochtree: Stochastic tree ensembles (XBART BART) supervised learning causal inference. R package version 0.0.0.9000, https://stochastictree.github.io/stochtree-r/.","code":"@Manual{,   title = {stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference},   author = {Drew Herren and Richard Hahn and Jared Murray and Carlos Carvalho and Jingyu He},   year = {2024},   note = {R package version 0.0.0.9000},   url = {https://stochastictree.github.io/stochtree-r/}, }"},{"path":"https://stochastictree.github.io/stochtree-r/index.html","id":"stochtree-r-package","dir":"","previous_headings":"","what":"Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference","title":"Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference","text":"NOTE: process refactoring project R, Python, C++ source code sits repo.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference","text":"package can installed R via","code":"remotes::install_github(\"StochasticTree/stochtree\", ref=\"r-dev\")"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that stores draws from an random ensemble of decision trees — CppJson","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Wrapper around C++ container tree ensembles","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"json_ptr External pointer C++ nlohmann::json object num_forests Number forests nlohmann::json object forest_labels Names forest objects overall nlohmann::json object num_rfx Number random effects terms nlohman::json object rfx_container_labels Names rfx container objects overall nlohmann::json object rfx_mapper_labels Names rfx label mapper objects overall nlohmann::json object rfx_groupid_labels Names rfx group id objects overall nlohmann::json object","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"CppJson$new() CppJson$add_forest() CppJson$add_random_effects() CppJson$add_scalar() CppJson$add_boolean() CppJson$add_string() CppJson$add_vector() CppJson$add_string_vector() CppJson$add_list() CppJson$add_string_list() CppJson$get_scalar() CppJson$get_boolean() CppJson$get_string() CppJson$get_vector() CppJson$get_string_vector() CppJson$get_numeric_list() CppJson$get_string_list() CppJson$return_json_string() CppJson$save_file() CppJson$load_from_file() CppJson$load_from_string()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Create new CppJson object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$new()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"new CppJson object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-forest-","dir":"Reference","previous_headings":"","what":"Method add_forest()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Convert forest container json add current CppJson object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_forest(forest_samples)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"forest_samples ForestSamples R class","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-random-effects-","dir":"Reference","previous_headings":"","what":"Method add_random_effects()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Convert random effects container json add current CppJson object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_random_effects(rfx_samples)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"rfx_samples RandomEffectSamples R class","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-scalar-","dir":"Reference","previous_headings":"","what":"Method add_scalar()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Add scalar json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_scalar(field_name, field_value, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json field_value Numeric value field added json subfolder_name (Optional) Name subfolder / hierarchy place value","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-boolean-","dir":"Reference","previous_headings":"","what":"Method add_boolean()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Add boolean value json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_boolean(field_name, field_value, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json field_value Numeric value field added json subfolder_name (Optional) Name subfolder / hierarchy place value","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-string-","dir":"Reference","previous_headings":"","what":"Method add_string()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Add string value json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_string(field_name, field_value, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json field_value Numeric value field added json subfolder_name (Optional) Name subfolder / hierarchy place value","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-vector-","dir":"Reference","previous_headings":"","what":"Method add_vector()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Add array json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_vector(field_name, field_vector, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json field_vector Vector stored json subfolder_name (Optional) Name subfolder / hierarchy place value","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-string-vector-","dir":"Reference","previous_headings":"","what":"Method add_string_vector()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Add array json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_string_vector(field_name, field_vector, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json field_vector Character vector stored json subfolder_name (Optional) Name subfolder / hierarchy place value","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-list-","dir":"Reference","previous_headings":"","what":"Method add_list()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Add list vectors (object map arrays) json object name \"field_name\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_list(field_name, field_list)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json field_list List stored json","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-add-string-list-","dir":"Reference","previous_headings":"","what":"Method add_string_list()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Add list vectors (object map arrays) json object name \"field_name\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$add_string_list(field_name, field_list)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json field_list List stored json","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-get-scalar-","dir":"Reference","previous_headings":"","what":"Method get_scalar()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Retrieve scalar value json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$get_scalar(field_name, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field accessed json subfolder_name (Optional) Name subfolder / hierarchy field stored","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-get-boolean-","dir":"Reference","previous_headings":"","what":"Method get_boolean()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Retrieve boolean value json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$get_boolean(field_name, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field accessed json subfolder_name (Optional) Name subfolder / hierarchy field stored","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-get-string-","dir":"Reference","previous_headings":"","what":"Method get_string()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Retrieve string value json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$get_string(field_name, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field accessed json subfolder_name (Optional) Name subfolder / hierarchy field stored","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-get-vector-","dir":"Reference","previous_headings":"","what":"Method get_vector()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Retrieve vector json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$get_vector(field_name, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field accessed json subfolder_name (Optional) Name subfolder / hierarchy field stored","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-get-string-vector-","dir":"Reference","previous_headings":"","what":"Method get_string_vector()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Retrieve character vector json object name \"field_name\" (optional subfolder \"subfolder_name\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$get_string_vector(field_name, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field accessed json subfolder_name (Optional) Name subfolder / hierarchy field stored","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-get-numeric-list-","dir":"Reference","previous_headings":"","what":"Method get_numeric_list()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Reconstruct list numeric vectors json object stored \"field_name\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$get_numeric_list(field_name, key_names)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json key_names Vector names list elements (vector)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-get-string-list-","dir":"Reference","previous_headings":"","what":"Method get_string_list()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Reconstruct list string vectors json object stored \"field_name\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$get_string_list(field_name, key_names)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"field_name name field added json key_names Vector names list elements (vector)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-16","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-return-json-string-","dir":"Reference","previous_headings":"","what":"Method return_json_string()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Convert JSON object -memory string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$return_json_string()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-17","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"JSON string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-save-file-","dir":"Reference","previous_headings":"","what":"Method save_file()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Save json object file","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$save_file(filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"filename String filepath, must end \".json\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-18","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-load-from-file-","dir":"Reference","previous_headings":"","what":"Method load_from_file()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Load json object file","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$load_from_file(filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"filename String filepath, must end \".json\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-19","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"method-load-from-string-","dir":"Reference","previous_headings":"","what":"Method load_from_string()","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"Load json object string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"","code":"CppJson$load_from_string(json_string)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"json_string JSON string dump","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppJson.html","id":"returns-20","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — CppJson","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"Persists C++ random number generator throughout R session ensure reproducibility given random seed. seed provided, C++ random number generator initialized using std::random_device.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"rng_ptr External pointer C++ std::mt19937 class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"CppRNG$new()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"Create new CppRNG object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"","code":"CppRNG$new(random_seed = -1)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"random_seed (Optional) random seed sampling","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/CppRNG.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps a C++ random number generator (for reproducibility) — CppRNG","text":"new CppRNG object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataset used to sample a forest — ForestDataset","title":"Dataset used to sample a forest — ForestDataset","text":"dataset consists three matrices / vectors: covariates, bases, variance weights. basis vector variance weights optional.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Dataset used to sample a forest — ForestDataset","text":"data_ptr External pointer C++ ForestDataset class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Dataset used to sample a forest — ForestDataset","text":"ForestDataset$new() ForestDataset$update_basis() ForestDataset$num_observations() ForestDataset$num_covariates() ForestDataset$num_basis() ForestDataset$has_basis() ForestDataset$has_variance_weights()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Dataset used to sample a forest — ForestDataset","text":"Create new ForestDataset object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$new(covariates, basis = NULL, variance_weights = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataset used to sample a forest — ForestDataset","text":"covariates Matrix covariates basis (Optional) Matrix bases used define leaf regression variance_weights (Optional) Vector observation-specific variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"new ForestDataset object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-update-basis-","dir":"Reference","previous_headings":"","what":"Method update_basis()","title":"Dataset used to sample a forest — ForestDataset","text":"Update basis matrix dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$update_basis(basis)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataset used to sample a forest — ForestDataset","text":"basis Updated matrix bases used define leaf regression","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-num-observations-","dir":"Reference","previous_headings":"","what":"Method num_observations()","title":"Dataset used to sample a forest — ForestDataset","text":"Return number observations ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$num_observations()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"Observation count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-num-covariates-","dir":"Reference","previous_headings":"","what":"Method num_covariates()","title":"Dataset used to sample a forest — ForestDataset","text":"Return number covariates ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$num_covariates()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"Covariate count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-num-basis-","dir":"Reference","previous_headings":"","what":"Method num_basis()","title":"Dataset used to sample a forest — ForestDataset","text":"Return number bases ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$num_basis()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"Basis count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-has-basis-","dir":"Reference","previous_headings":"","what":"Method has_basis()","title":"Dataset used to sample a forest — ForestDataset","text":"Whether dataset basis matrix","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$has_basis()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"True basis matrix loaded, false otherwise","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"method-has-variance-weights-","dir":"Reference","previous_headings":"","what":"Method has_variance_weights()","title":"Dataset used to sample a forest — ForestDataset","text":"Whether dataset variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a forest — ForestDataset","text":"","code":"ForestDataset$has_variance_weights()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestDataset.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a forest — ForestDataset","text":"True variance weights loaded, false otherwise","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that defines and samples a forest model — ForestModel","title":"Class that defines and samples a forest model — ForestModel","text":"Hosts C++ data structures needed sample ensemble decision trees, exposes functionality run forest sampler (using either MCMC grow--root algorithm).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that defines and samples a forest model — ForestModel","text":"tracker_ptr External pointer C++ ForestTracker class tree_prior_ptr External pointer C++ TreePrior class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that defines and samples a forest model — ForestModel","text":"ForestModel$new() ForestModel$sample_one_iteration() ForestModel$propagate_basis_update() ForestModel$propagate_residual_update()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that defines and samples a forest model — ForestModel","text":"Create new ForestModel object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that defines and samples a forest model — ForestModel","text":"","code":"ForestModel$new(   forest_dataset,   feature_types,   num_trees,   n,   alpha,   beta,   min_samples_leaf,   max_depth = -1 )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that defines and samples a forest model — ForestModel","text":"forest_dataset ForestDataset object, used initialize forest sampling data structures feature_types Feature types (integers 0 = numeric, 1 = ordered categorical, 2 = unordered categorical) num_trees Number trees forest sampled n Number observations forest_dataset alpha Root node split probability tree prior beta Depth prior penalty tree prior min_samples_leaf Minimum number samples tree leaf max_depth Maximum depth tree can reach","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that defines and samples a forest model — ForestModel","text":"new ForestModel object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"method-sample-one-iteration-","dir":"Reference","previous_headings":"","what":"Method sample_one_iteration()","title":"Class that defines and samples a forest model — ForestModel","text":"Run single iteration forest sampling algorithm (MCMC GFR)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that defines and samples a forest model — ForestModel","text":"","code":"ForestModel$sample_one_iteration(   forest_dataset,   residual,   forest_samples,   rng,   feature_types,   leaf_model_int,   leaf_model_scale,   variable_weights,   a_forest,   b_forest,   global_scale,   cutpoint_grid_size = 500,   gfr = T,   pre_initialized = F )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that defines and samples a forest model — ForestModel","text":"forest_dataset Dataset used sample forest residual Outcome used sample forest forest_samples Container forest samples rng Wrapper around C++ random number generator feature_types Vector specifying type p covariates forest_dataset (0 = numeric, 1 = ordered categorical, 2 = unordered categorical) leaf_model_int Integer specifying leaf model type (0 = constant leaf, 1 = univariate leaf regression, 2 = multivariate leaf regression) leaf_model_scale Scale parameter used leaf node model (q x q matrix q dimensionality basis >1 leaf_model_int = 2) variable_weights Vector specifying sampling probability p covariates forest_dataset a_forest Shape parameter variance forest model (applicable) b_forest Scale parameter variance forest model (applicable) global_scale Global variance parameter cutpoint_grid_size (Optional) Number unique cutpoints consider (default: 500, currently used GFR = TRUE) gfr (Optional) Whether forest sampled using \"grow--root\" (GFR) algorithm pre_initialized (Optional) Whether leaves pre-initialized outside sampling loop (samples drawn). multi-forest implementations like BCF, true, though single-forest supervised learning implementation, can let C++ initialization. Default: F.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"method-propagate-basis-update-","dir":"Reference","previous_headings":"","what":"Method propagate_basis_update()","title":"Class that defines and samples a forest model — ForestModel","text":"Propagates basis update (full/partial) residual iteratively () adding back previous prediction tree, (b) recomputing predictions tree (caching C++ side), (c) subtracting new predictions residual. useful cases basis (e.g. leaf regression) updated outside tree sampler (e.g. adaptive coding binary treatment BCF). basis updated, overall \"function\" represented tree model changed reflected residual next sampling loop run.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that defines and samples a forest model — ForestModel","text":"","code":"ForestModel$propagate_basis_update(   dataset,   outcome,   forest_samples,   forest_num )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that defines and samples a forest model — ForestModel","text":"dataset ForestDataset object storing covariates bases given forest outcome Outcome object storing residuals updated based forest predictions forest_samples ForestSamples object storing draws tree ensembles forest_num Index forest used update residuals (starting 1, R style)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"method-propagate-residual-update-","dir":"Reference","previous_headings":"","what":"Method propagate_residual_update()","title":"Class that defines and samples a forest model — ForestModel","text":"Update current state outcome (.e. partial residual) data subtracting current predictions tree. function run Outcome class's update_data method, overwrites partial residual entirely new stream outcome data.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that defines and samples a forest model — ForestModel","text":"","code":"ForestModel$propagate_residual_update(residual)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that defines and samples a forest model — ForestModel","text":"residual Outcome used sample forest","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestModel.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that defines and samples a forest model — ForestModel","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that stores draws from an random ensemble of decision trees — ForestSamples","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Wrapper around C++ container tree ensembles","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_container_ptr External pointer C++ ForestContainer class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"ForestSamples$new() ForestSamples$load_from_json() ForestSamples$append_from_json() ForestSamples$load_from_json_string() ForestSamples$append_from_json_string() ForestSamples$predict() ForestSamples$predict_raw() ForestSamples$predict_raw_single_forest() ForestSamples$set_root_leaves() ForestSamples$prepare_for_sampler() ForestSamples$adjust_residual() ForestSamples$save_json() ForestSamples$load_json() ForestSamples$num_samples() ForestSamples$num_trees() ForestSamples$output_dimension() ForestSamples$add_forest_with_constant_leaves() ForestSamples$add_numeric_split_tree() ForestSamples$get_tree_leaves() ForestSamples$get_tree_split_counts() ForestSamples$get_forest_split_counts() ForestSamples$get_aggregate_split_counts() ForestSamples$get_granular_split_counts() ForestSamples$ensemble_tree_max_depth() ForestSamples$average_ensemble_max_depth() ForestSamples$average_max_depth()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Create new ForestContainer object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$new(   num_trees,   output_dimension = 1,   is_leaf_constant = F,   is_exponentiated = F )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"num_trees Number trees output_dimension Dimensionality outcome model is_leaf_constant Whether leaf constant is_exponentiated Whether forest predictions exponentiated returned","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"new ForestContainer object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-load-from-json-","dir":"Reference","previous_headings":"","what":"Method load_from_json()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Create new ForestContainer object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$load_from_json(json_object, json_forest_label)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_object Object class CppJson json_forest_label Label referring particular forest (.e. \"forest_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"new ForestContainer object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-append-from-json-","dir":"Reference","previous_headings":"","what":"Method append_from_json()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Append ForestContainer object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$append_from_json(json_object, json_forest_label)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_object Object class CppJson json_forest_label Label referring particular forest (.e. \"forest_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-load-from-json-string-","dir":"Reference","previous_headings":"","what":"Method load_from_json_string()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Create new ForestContainer object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$load_from_json_string(json_string, json_forest_label)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_string JSON string parses object class CppJson json_forest_label Label referring particular forest (.e. \"forest_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"new ForestContainer object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-append-from-json-string-","dir":"Reference","previous_headings":"","what":"Method append_from_json_string()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Append ForestContainer object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$append_from_json_string(json_string, json_forest_label)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_string JSON string parses object class CppJson json_forest_label Label referring particular forest (.e. \"forest_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Predict every tree ensemble every sample forest_dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$predict(forest_dataset)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_dataset ForestDataset R class","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"matrix predictions many rows forest_dataset many columns samples ForestContainer","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-predict-raw-","dir":"Reference","previous_headings":"","what":"Method predict_raw()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Predict \"raw\" leaf values (without multiplied basis) every tree ensemble every sample forest_dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$predict_raw(forest_dataset)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_dataset ForestDataset R class","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Array predictions observation forest_dataset sample ForestSamples class prediction dimensionality forests' leaf model. case constant leaf model univariate leaf regression, array two-dimensional (number observations, number forest samples). case multivariate leaf regression, array three-dimension (number observations, leaf model dimension, number samples).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-predict-raw-single-forest-","dir":"Reference","previous_headings":"","what":"Method predict_raw_single_forest()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Predict \"raw\" leaf values (without multiplied basis) specific forest every sample forest_dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$predict_raw_single_forest(forest_dataset, forest_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_dataset ForestDataset R class forest_num Index forest sample within container","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"matrix predictions many rows forest_dataset many columns samples ForestContainer","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-set-root-leaves-","dir":"Reference","previous_headings":"","what":"Method set_root_leaves()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Set constant predicted value every tree ensemble. Stops program tree root node.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$set_root_leaves(forest_num, leaf_value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_num Index forest sample within container. leaf_value Constant leaf value(s) fixed tree ensemble indexed forest_num. Can either single number vector, depending forest's leaf dimension.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-prepare-for-sampler-","dir":"Reference","previous_headings":"","what":"Method prepare_for_sampler()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Set constant predicted value every tree ensemble. Stops program tree root node.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$prepare_for_sampler(   dataset,   outcome,   forest_model,   leaf_model_int,   leaf_value )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"dataset ForestDataset Dataset class (covariates, basis, etc...) outcome Outcome Outcome class (residual / partial residual) forest_model ForestModel object storing tracking structures used training / sampling leaf_model_int Integer value encoding leaf model type (0 = constant gaussian, 1 = univariate gaussian, 2 = multivariate gaussian, 3 = log linear variance). leaf_value Constant leaf value(s) fixed tree ensemble indexed forest_num. Can either single number vector, depending forest's leaf dimension.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-adjust-residual-","dir":"Reference","previous_headings":"","what":"Method adjust_residual()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Adjusts residual based predictions forest typically run just beginning forest sampling algorithm. trees initialized constant root node predictions, root predictions subtracted residual.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$adjust_residual(   dataset,   outcome,   forest_model,   requires_basis,   forest_num,   add )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"dataset ForestDataset object storing covariates bases given forest outcome Outcome object storing residuals updated based forest predictions forest_model ForestModel object storing tracking structures used training / sampling requires_basis Whether forest requires basis prediction forest_num Index forest used update residuals add Whether forest predictions added subtracted residuals","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-save-json-","dir":"Reference","previous_headings":"","what":"Method save_json()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Store trees metadata ForestDataset class json file","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$save_json(json_filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_filename Name output json file (must end \".json\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-load-json-","dir":"Reference","previous_headings":"","what":"Method load_json()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Load trees metadata ensemble json file. Note trees metadata already present ForestDataset class overwritten.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$load_json(json_filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"json_filename Name model input json file (must end \".json\")","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-num-samples-","dir":"Reference","previous_headings":"","what":"Method num_samples()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Return number samples ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$num_samples()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Sample count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-num-trees-","dir":"Reference","previous_headings":"","what":"Method num_trees()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Return number trees ensemble ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$num_trees()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Tree count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-output-dimension-","dir":"Reference","previous_headings":"","what":"Method output_dimension()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Return output dimension trees ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$output_dimension()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Leaf node parameter size","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-add-forest-with-constant-leaves-","dir":"Reference","previous_headings":"","what":"Method add_forest_with_constant_leaves()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Add new -root ensemble container, leaves set value / vector provided","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$add_forest_with_constant_leaves(leaf_value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"leaf_value Value (vector values) initialize root nodes tree","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-add-numeric-split-tree-","dir":"Reference","previous_headings":"","what":"Method add_numeric_split_tree()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Add numeric (.e. X,<= c) split given tree ensemble","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$add_numeric_split_tree(   forest_num,   tree_num,   leaf_num,   feature_num,   split_threshold,   left_leaf_value,   right_leaf_value )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_num Index forest contains tree split tree_num Index tree split leaf_num Leaf split feature_num Feature defines new split split_threshold Value defines cutoff new split left_leaf_value Value (vector values) assign newly created left node right_leaf_value Value (vector values) assign newly created right node","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-get-tree-leaves-","dir":"Reference","previous_headings":"","what":"Method get_tree_leaves()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Retrieve vector indices leaf nodes given tree given forest","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$get_tree_leaves(forest_num, tree_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_num Index forest contains tree tree_num tree_num Index tree leaf indices retrieved","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-get-tree-split-counts-","dir":"Reference","previous_headings":"","what":"Method get_tree_split_counts()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Retrieve vector split counts every training set variable given tree given forest","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$get_tree_split_counts(forest_num, tree_num, num_features)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_num Index forest contains tree tree_num tree_num Index tree split counts retrieved num_features Total number features training set","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-get-forest-split-counts-","dir":"Reference","previous_headings":"","what":"Method get_forest_split_counts()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Retrieve vector split counts every training set variable given forest","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$get_forest_split_counts(forest_num, num_features)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"forest_num Index forest split counts retrieved num_features Total number features training set","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-get-aggregate-split-counts-","dir":"Reference","previous_headings":"","what":"Method get_aggregate_split_counts()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Retrieve vector split counts every training set variable given forest, aggregated across ensembles trees","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$get_aggregate_split_counts(num_features)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"num_features Total number features training set","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-get-granular-split-counts-","dir":"Reference","previous_headings":"","what":"Method get_granular_split_counts()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Retrieve vector split counts every training set variable given forest, reported separately ensemble tree","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$get_granular_split_counts(num_features)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-19","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"num_features Total number features training set","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-ensemble-tree-max-depth-","dir":"Reference","previous_headings":"","what":"Method ensemble_tree_max_depth()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Maximum depth specific tree specific ensemble ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$ensemble_tree_max_depth(ensemble_num, tree_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-20","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"ensemble_num Ensemble number tree_num Tree index within ensemble ensemble_num","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Maximum leaf depth","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-average-ensemble-max-depth-","dir":"Reference","previous_headings":"","what":"Method average_ensemble_max_depth()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Average maximum depth tree given ensemble ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-24","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$average_ensemble_max_depth(ensemble_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"arguments-21","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"ensemble_num Ensemble number","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Average maximum depth","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"method-average-max-depth-","dir":"Reference","previous_headings":"","what":"Method average_max_depth()","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Average maximum depth tree ensemble ForestContainer object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"usage-25","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"","code":"ForestSamples$average_max_depth()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/ForestSamples.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that stores draws from an random ensemble of decision trees — ForestSamples","text":"Average maximum depth","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":null,"dir":"Reference","previous_headings":"","what":"Outcome / partial residual used to sample an additive model. — Outcome","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"outcome class wrapper around vector (mutable) outcomes ML tasks (supervised learning, causal inference). additive tree ensemble sampled, outcome used sample specific model term \"partial residual\" consisting outcome minus predictions every model term (trees, group random effects, etc...).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"data_ptr External pointer C++ Outcome class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Outcome$new() Outcome$get_data() Outcome$add_vector() Outcome$subtract_vector() Outcome$update_data()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Create new Outcome object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"","code":"Outcome$new(outcome)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"outcome Vector outcome values","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"new Outcome object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"method-get-data-","dir":"Reference","previous_headings":"","what":"Method get_data()","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Extract raw data R underlying C++ object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"","code":"Outcome$get_data()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"R vector containing (copy ) values Outcome object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"method-add-vector-","dir":"Reference","previous_headings":"","what":"Method add_vector()","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Update current state outcome (.e. partial residual) data adding values update_vector","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"","code":"Outcome$add_vector(update_vector)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"update_vector Vector added outcome","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"method-subtract-vector-","dir":"Reference","previous_headings":"","what":"Method subtract_vector()","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Update current state outcome (.e. partial residual) data subtracting values update_vector","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"","code":"Outcome$subtract_vector(update_vector)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"update_vector Vector subtracted outcome","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"method-update-data-","dir":"Reference","previous_headings":"","what":"Method update_data()","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"Update current state outcome (.e. partial residual) data replacing element elements new_vector","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"","code":"Outcome$update_data(new_vector)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"new_vector Vector overwrite current data","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/Outcome.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Outcome / partial residual used to sample an additive model. — Outcome","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that wraps the ","title":"Class that wraps the ","text":"Coordinates various C++ random effects classes persists needed prediction / serialization","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that wraps the ","text":"rfx_container_ptr External pointer C++ StochTree::RandomEffectsContainer class label_mapper_ptr External pointer C++ StochTree::LabelMapper class training_group_ids Unique vector group IDs training dataset","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that wraps the ","text":"RandomEffectSamples$new() RandomEffectSamples$load_in_session() RandomEffectSamples$load_from_json() RandomEffectSamples$append_from_json() RandomEffectSamples$load_from_json_string() RandomEffectSamples$append_from_json_string() RandomEffectSamples$predict() RandomEffectSamples$extract_parameter_samples() RandomEffectSamples$extract_label_mapping()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that wraps the ","text":"Create new RandomEffectSamples object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$new()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"new RandomEffectSamples object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-load-in-session-","dir":"Reference","previous_headings":"","what":"Method load_in_session()","title":"Class that wraps the ","text":"Construct RandomEffectSamples object \"-session\" R objects","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$load_in_session(   num_components,   num_groups,   random_effects_tracker )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps the ","text":"num_components Number \"components\" bases defining random effects regression num_groups Number random effects groups random_effects_tracker Object type RandomEffectsTracker","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"NULL","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-load-from-json-","dir":"Reference","previous_headings":"","what":"Method load_from_json()","title":"Class that wraps the ","text":"Construct RandomEffectSamples object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$load_from_json(   json_object,   json_rfx_container_label,   json_rfx_mapper_label,   json_rfx_groupids_label )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps the ","text":"json_object Object class CppJson json_rfx_container_label Label referring particular rfx sample container (.e. \"random_effect_container_0\") overall json hierarchy json_rfx_mapper_label Label referring particular rfx label mapper (.e. \"random_effect_label_mapper_0\") overall json hierarchy json_rfx_groupids_label Label referring particular set rfx group IDs (.e. \"random_effect_groupids_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"new RandomEffectSamples object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-append-from-json-","dir":"Reference","previous_headings":"","what":"Method append_from_json()","title":"Class that wraps the ","text":"Append random effect draws RandomEffectSamples object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$append_from_json(   json_object,   json_rfx_container_label,   json_rfx_mapper_label,   json_rfx_groupids_label )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps the ","text":"json_object Object class CppJson json_rfx_container_label Label referring particular rfx sample container (.e. \"random_effect_container_0\") overall json hierarchy json_rfx_mapper_label Label referring particular rfx label mapper (.e. \"random_effect_label_mapper_0\") overall json hierarchy json_rfx_groupids_label Label referring particular set rfx group IDs (.e. \"random_effect_groupids_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"NULL (updates object -place)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-load-from-json-string-","dir":"Reference","previous_headings":"","what":"Method load_from_json_string()","title":"Class that wraps the ","text":"Construct RandomEffectSamples object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$load_from_json_string(   json_string,   json_rfx_container_label,   json_rfx_mapper_label,   json_rfx_groupids_label )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps the ","text":"json_string JSON string parses object class CppJson json_rfx_container_label Label referring particular rfx sample container (.e. \"random_effect_container_0\") overall json hierarchy json_rfx_mapper_label Label referring particular rfx label mapper (.e. \"random_effect_label_mapper_0\") overall json hierarchy json_rfx_groupids_label Label referring particular set rfx group IDs (.e. \"random_effect_groupids_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"new RandomEffectSamples object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-append-from-json-string-","dir":"Reference","previous_headings":"","what":"Method append_from_json_string()","title":"Class that wraps the ","text":"Append random effect draws RandomEffectSamples object json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$append_from_json_string(   json_string,   json_rfx_container_label,   json_rfx_mapper_label,   json_rfx_groupids_label )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps the ","text":"json_string JSON string parses object class CppJson json_rfx_container_label Label referring particular rfx sample container (.e. \"random_effect_container_0\") overall json hierarchy json_rfx_mapper_label Label referring particular rfx label mapper (.e. \"random_effect_label_mapper_0\") overall json hierarchy json_rfx_groupids_label Label referring particular set rfx group IDs (.e. \"random_effect_groupids_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"NULL (updates object -place)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"Class that wraps the ","text":"Predict random effects observation implied rfx_group_ids rfx_basis. random effects model \"intercept-\" rfx_basis vector ones size length(rfx_group_ids).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$predict(rfx_group_ids, rfx_basis = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that wraps the ","text":"rfx_group_ids Indices random effects groups prediction set rfx_basis (Optional ) Basis used random effects prediction","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"Matrix many rows observations provided many columns samples drawn model.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-extract-parameter-samples-","dir":"Reference","previous_headings":"","what":"Method extract_parameter_samples()","title":"Class that wraps the ","text":"Extract random effects parameters sampled. \"redundant parameterization\" Gelman et al (2008), includes four parameters: alpha (\"working parameter\" shared across every group), xi (\"group parameter\" sampled separately group), beta (product alpha xi, corresponds overall group-level random effects), sigma (group-independent prior variance component xi).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$extract_parameter_samples()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"List arrays. alpha array dimension (num_components, num_samples) simply vector num_components = 1. xi beta arrays dimension (num_components, num_groups, num_samples) simply matrix num_components = 1. sigma array dimension (num_components, num_samples) simply vector num_components = 1.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"method-extract-label-mapping-","dir":"Reference","previous_headings":"","what":"Method extract_label_mapping()","title":"Class that wraps the ","text":"Convert mapping group IDs random effect components indices C++ R native format","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that wraps the ","text":"","code":"RandomEffectSamples$extract_label_mapping()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectSamples.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that wraps the ","text":"List mapping group ID random effect components.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataset used to sample a random effects model — RandomEffectsDataset","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"dataset consists three matrices / vectors: group labels, bases, variance weights. Variance weights optional.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"data_ptr External pointer C++ RandomEffectsDataset class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"RandomEffectsDataset$new() RandomEffectsDataset$num_observations() RandomEffectsDataset$has_group_labels() RandomEffectsDataset$has_basis() RandomEffectsDataset$has_variance_weights()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"Create new RandomEffectsDataset object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"","code":"RandomEffectsDataset$new(group_labels, basis, variance_weights = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"group_labels Vector group labels basis Matrix bases used define random effects regression (intercept-model, pass array ones) variance_weights (Optional) Vector observation-specific variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"new RandomEffectsDataset object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"method-num-observations-","dir":"Reference","previous_headings":"","what":"Method num_observations()","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"Return number observations RandomEffectsDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"","code":"RandomEffectsDataset$num_observations()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"Observation count","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"method-has-group-labels-","dir":"Reference","previous_headings":"","what":"Method has_group_labels()","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"Whether dataset group label indices","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"","code":"RandomEffectsDataset$has_group_labels()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"True group label vector loaded, false otherwise","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"method-has-basis-","dir":"Reference","previous_headings":"","what":"Method has_basis()","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"Whether dataset basis matrix","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"","code":"RandomEffectsDataset$has_basis()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"True basis matrix loaded, false otherwise","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"method-has-variance-weights-","dir":"Reference","previous_headings":"","what":"Method has_variance_weights()","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"Whether dataset variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"","code":"RandomEffectsDataset$has_variance_weights()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsDataset.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Dataset used to sample a random effects model — RandomEffectsDataset","text":"True variance weights loaded, false otherwise","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":null,"dir":"Reference","previous_headings":"","what":"The core ","title":"The core ","text":"Stores current model state, prior parameters, procedures sampling conditional posterior parameter.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"The core ","text":"rfx_model_ptr External pointer C++ StochTree::RandomEffectsModel class num_groups Number groups random effects model num_components Number components (.e. dimension basis) random effects model","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"The core ","text":"RandomEffectsModel$new() RandomEffectsModel$sample_random_effect() RandomEffectsModel$predict() RandomEffectsModel$set_working_parameter() RandomEffectsModel$set_group_parameters() RandomEffectsModel$set_working_parameter_cov() RandomEffectsModel$set_group_parameter_cov() RandomEffectsModel$set_variance_prior_shape() RandomEffectsModel$set_variance_prior_scale()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"The core ","text":"Create new RandomEffectsModel object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$new(num_components, num_groups)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"num_components Number \"components\" bases defining random effects regression num_groups Number random effects groups","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"new RandomEffectsModel object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-sample-random-effect-","dir":"Reference","previous_headings":"","what":"Method sample_random_effect()","title":"The core ","text":"Sample random effects model.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$sample_random_effect(   rfx_dataset,   residual,   rfx_tracker,   rfx_samples,   global_variance,   rng )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"rfx_dataset Object type RandomEffectsDataset residual Object type Outcome rfx_tracker Object type RandomEffectsTracker rfx_samples Object type RandomEffectSamples global_variance Scalar global variance parameter rng Object type CppRNG","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"None","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-predict-","dir":"Reference","previous_headings":"","what":"Method predict()","title":"The core ","text":"Predict (single sample ) random effects model.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$predict(rfx_dataset, rfx_tracker)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"rfx_dataset Object type RandomEffectsDataset rfx_tracker Object type RandomEffectsTracker","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"Vector predictions size matching number observations rfx_dataset","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-set-working-parameter-","dir":"Reference","previous_headings":"","what":"Method set_working_parameter()","title":"The core ","text":"Set value \"working parameter.\" typically used initialization, also used interrupt override sampler.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$set_working_parameter(value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"value Parameter input","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"None","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-set-group-parameters-","dir":"Reference","previous_headings":"","what":"Method set_group_parameters()","title":"The core ","text":"Set value \"group parameters.\" typically used initialization, also used interrupt override sampler.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$set_group_parameters(value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"value Parameter input","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"None","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-set-working-parameter-cov-","dir":"Reference","previous_headings":"","what":"Method set_working_parameter_cov()","title":"The core ","text":"Set value working parameter covariance. typically used initialization, also used interrupt override sampler.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$set_working_parameter_cov(value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"value Parameter input","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"None","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-set-group-parameter-cov-","dir":"Reference","previous_headings":"","what":"Method set_group_parameter_cov()","title":"The core ","text":"Set value group parameter covariance. typically used initialization, also used interrupt override sampler.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$set_group_parameter_cov(value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"value Parameter input","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"None","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-set-variance-prior-shape-","dir":"Reference","previous_headings":"","what":"Method set_variance_prior_shape()","title":"The core ","text":"Set shape parameter group parameter variance prior.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$set_variance_prior_shape(value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"value Parameter input","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"None","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"method-set-variance-prior-scale-","dir":"Reference","previous_headings":"","what":"Method set_variance_prior_scale()","title":"The core ","text":"Set shape parameter group parameter variance prior.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"The core ","text":"","code":"RandomEffectsModel$set_variance_prior_scale(value)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"The core ","text":"value Parameter input","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsModel.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"The core ","text":"None","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsTracker.html","id":null,"dir":"Reference","previous_headings":"","what":"Class that defines a ","title":"Class that defines a ","text":"Stores mapping every observation group index, mapping group indices training sample observations available group, predictions observation.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsTracker.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Class that defines a ","text":"rfx_tracker_ptr External pointer C++ StochTree::RandomEffectsTracker class","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsTracker.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Class that defines a ","text":"RandomEffectsTracker$new()","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsTracker.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Class that defines a ","text":"Create new RandomEffectsTracker object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsTracker.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class that defines a ","text":"","code":"RandomEffectsTracker$new(rfx_group_indices)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsTracker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class that defines a ","text":"rfx_group_indices Integer indices indicating groups used define random effects","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/RandomEffectsTracker.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Class that defines a ","text":"new RandomEffectsTracker object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bart.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the BART algorithm for supervised learning. — bart","title":"Run the BART algorithm for supervised learning. — bart","text":"Run BART algorithm supervised learning.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the BART algorithm for supervised learning. — bart","text":"","code":"bart(   X_train,   y_train,   W_train = NULL,   group_ids_train = NULL,   rfx_basis_train = NULL,   X_test = NULL,   W_test = NULL,   group_ids_test = NULL,   rfx_basis_test = NULL,   num_gfr = 5,   num_burnin = 0,   num_mcmc = 100,   params = list() )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/bart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the BART algorithm for supervised learning. — bart","text":"X_train Covariates used split trees ensemble. May provided either dataframe matrix. Matrix covariates assumed numeric. Covariates passed dataframe preprocessed based variable types (e.g. categorical columns stored unordered factors one-hot encoded, categorical columns stored ordered factors passed integers core algorithm, along metadata column ordered categorical). y_train Outcome modeled ensemble. W_train (Optional) Bases used define regression model y ~ W leaf regression tree. default, BART assumes constant leaf node parameters, implicitly regressing constant basis ones (.e. y ~ 1). group_ids_train (Optional) Group labels used additive random effects model. rfx_basis_train (Optional) Basis \"random-slope\" regression additive random effects model. group_ids_train provided regression basis, intercept-random effects model estimated. X_test (Optional) Test set covariates used define \"sample\" evaluation data. May provided either dataframe matrix, format X_test must consistent X_train. W_test (Optional) Test set bases used define \"sample\" evaluation data. test set optional, structure provided test set must match training set (.e. X_train W_train provided, test set must consist X_test W_test number columns). group_ids_test (Optional) Test set group labels used additive random effects model. currently support (plan near future), test set evaluation group labels training set. rfx_basis_test (Optional) Test set basis \"random-slope\" regression additive random effects model. num_gfr Number \"warm-start\" iterations run using grow--root algorithm (Hahn, 2021). Default: 5. num_burnin Number \"burn-\" iterations MCMC sampler. Default: 0. num_mcmc Number \"retained\" iterations MCMC sampler. Default: 100. params list model parameters, default value. 1. Global Parameters cutpoint_grid_size Maximum size \"grid\" potential cutpoints consider. Default: 100. sigma2_init Starting value global error variance parameter. Calibrated internally pct_var_sigma2_init*var((y-mean(y))/sd(y)) set. pct_var_sigma2_init Percentage standardized outcome variance used initialize global error variance parameter. Default: 1. Superseded sigma2_init. variance_scale Variance data scaled. Default: 1. a_global Shape parameter IG(a_global, b_global) global error variance model. Default: 0. b_global Scale parameter IG(a_global, b_global) global error variance model. Default: 0. random_seed Integer parameterizing C++ random number generator. specified, C++ random number generator seeded according std::random_device. sample_sigma_global Whether update sigma^2 global error variance parameter based IG(a_global, b_global). Default: TRUE. keep_burnin Whether \"burnin\" samples included cached predictions. Default FALSE. Ignored num_mcmc = 0. keep_gfr Whether \"grow--root\" samples included cached predictions. Default TRUE. Ignored num_mcmc = 0. verbose Whether print progress sampling loops. Default: FALSE. 2. Mean Forest Parameters num_trees_mean Number trees ensemble conditional mean model. Default: 200. num_trees_mean = 0, conditional mean modeled using forest, function proceed num_trees_variance > 0. sample_sigma_leaf Whether update tau leaf scale variance parameter based IG(a_leaf, b_leaf). (currently) set true ncol(W_train)>1. Default: FALSE. 2.1. Tree Prior Parameters alpha_mean Prior probability splitting tree depth 0 mean model. Tree split prior combines alpha_mean beta_mean via alpha_mean*(1+node_depth)^-beta_mean. Default: 0.95. beta_mean Exponent decreases split probabilities nodes depth > 0 mean model. Tree split prior combines alpha_mean beta_mean via alpha_mean*(1+node_depth)^-beta_mean. Default: 2. min_samples_leaf_mean Minimum allowable size leaf, terms training samples, mean model. Default: 5. max_depth_mean Maximum depth tree ensemble mean model. Default: 10. Can overridden -1 enforce depth limits trees. 2.2. Leaf Model Parameters variable_weights_mean Numeric weights reflecting relative probability splitting variable mean forest. need sum 1 negative. Defaults rep(1/ncol(X_train), ncol(X_train)) set . sigma_leaf_init Starting value leaf node scale parameter. Calibrated internally 1/num_trees_mean set . a_leaf Shape parameter IG(a_leaf, b_leaf) leaf node parameter variance model. Default: 3. b_leaf Scale parameter IG(a_leaf, b_leaf) leaf node parameter variance model. Calibrated internally 0.5/num_trees_mean set . 3. Conditional Variance Forest Parameters num_trees_variance Number trees ensemble conditional variance model. Default: 0. Variance modeled using tree / forest num_trees_variance > 0. variance_forest_init Starting value root forest prediction conditional (heteroskedastic) error variance model. Calibrated internally log(pct_var_variance_forest_init*var((y-mean(y))/sd(y)))/num_trees_variance set. pct_var_variance_forest_init Percentage standardized outcome variance used initialize global error variance parameter. Default: 1. Superseded variance_forest_init. 3.1. Tree Prior Parameters alpha_variance Prior probability splitting tree depth 0 variance model. Tree split prior combines alpha_variance beta_variance via alpha_variance*(1+node_depth)^-beta_variance. Default: 0.95. beta_variance Exponent decreases split probabilities nodes depth > 0 variance model. Tree split prior combines alpha_variance beta_variance via alpha_variance*(1+node_depth)^-beta_variance. Default: 2. min_samples_leaf_variance Minimum allowable size leaf, terms training samples, variance model. Default: 5. max_depth_variance Maximum depth tree ensemble variance model. Default: 10. Can overridden -1 enforce depth limits trees. 3.2. Leaf Model Parameters variable_weights_variance Numeric weights reflecting relative probability splitting variable variance forest. need sum 1 negative. Defaults rep(1/ncol(X_train), ncol(X_train)) set . sigma_leaf_init Starting value leaf node scale parameter. Calibrated internally 1/num_trees_mean set . a_forest Shape parameter IG(a_forest, b_forest) conditional error variance model (sampled num_trees_variance > 0). Calibrated internally num_trees_variance / 1.5^2 + 0.5 set. b_forest Scale parameter IG(a_forest, b_forest) conditional error variance model (sampled num_trees_variance > 0). Calibrated internally num_trees_variance / 1.5^2 set.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bart.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the BART algorithm for supervised learning. — bart","text":"List sampling outputs wrapper around sampled forests (can used -memory prediction new data, serialized JSON disk).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run the BART algorithm for supervised learning. — bart","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train, X_test = X_test) # plot(rowMeans(bart_model$y_hat_test), y_test, xlab = \"predicted\", ylab = \"actual\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"Run Bayesian Causal Forest (BCF) algorithm regularized causal effect estimation.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"","code":"bcf(   X_train,   Z_train,   y_train,   pi_train = NULL,   group_ids_train = NULL,   rfx_basis_train = NULL,   X_test = NULL,   Z_test = NULL,   pi_test = NULL,   group_ids_test = NULL,   rfx_basis_test = NULL,   num_gfr = 5,   num_burnin = 0,   num_mcmc = 100,   params = list() )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"X_train Covariates used split trees ensemble. May provided either dataframe matrix. Matrix covariates assumed numeric. Covariates passed dataframe preprocessed based variable types (e.g. categorical columns stored unordered factors one-hot encoded, categorical columns stored ordered factors passed integers core algorithm, along metadata column ordered categorical). Z_train Vector (continuous binary) treatment assignments. y_train Outcome modeled ensemble. pi_train (Optional) Vector propensity scores. provided, estimated data. group_ids_train (Optional) Group labels used additive random effects model. rfx_basis_train (Optional) Basis \"random-slope\" regression additive random effects model. group_ids_train provided regression basis, intercept-random effects model estimated. X_test (Optional) Test set covariates used define \"sample\" evaluation data. May provided either dataframe matrix, format X_test must consistent X_train. Z_test (Optional) Test set (continuous binary) treatment assignments. pi_test (Optional) Vector propensity scores. provided, estimated data. group_ids_test (Optional) Test set group labels used additive random effects model. currently support (plan near future), test set evaluation group labels training set. rfx_basis_test (Optional) Test set basis \"random-slope\" regression additive random effects model. num_gfr Number \"warm-start\" iterations run using grow--root algorithm (Hahn, 2021). Default: 5. num_burnin Number \"burn-\" iterations MCMC sampler. Default: 0. num_mcmc Number \"retained\" iterations MCMC sampler. Default: 100. params list model parameters, default value. 1. Global Parameters cutpoint_grid_size Maximum size \"grid\" potential cutpoints consider. Default: 100. a_global Shape parameter IG(a_global, b_global) global error variance model. Default: 0. b_global Scale parameter IG(a_global, b_global) global error variance model. Default: 0. sigma2_init Starting value global error variance parameter. Calibrated internally pct_var_sigma2_init*var((y-mean(y))/sd(y)) set. pct_var_sigma2_init Percentage standardized outcome variance used initialize global error variance parameter. Default: 1. Superseded sigma2_init. variable_weights Numeric weights reflecting relative probability splitting variable. need sum 1 negative. Defaults rep(1/ncol(X_train), ncol(X_train)) set . Note propensity score included covariate either forest, weight default 1/ncol(X_train). workaround wish provide custom weight propensity score include column X_train set propensity_covariate 'none' adjust keep_vars_mu, keep_vars_tau keep_vars_variance accordingly. propensity_covariate Whether include propensity score covariate either forests. Enter \"none\" neither, \"mu\" prognostic forest, \"tau\" treatment forest, \"\" forests. \"none\" propensity score provided, estimated (X_train, Z_train) using stochtree::bart(). Default: \"mu\". adaptive_coding Whether use \"adaptive coding\" scheme binary treatment variable coded manually (0,1) (-1,1) learned via parameters b_0 b_1 attach outcome model [b_0 (1-Z) + b_1 Z] tau(X). ignored Z binary. Default: TRUE. b_0 Initial value \"control\" group coding parameter. ignored Z binary. Default: -0.5. b_1 Initial value \"treatment\" group coding parameter. ignored Z binary. Default: 0.5. random_seed Integer parameterizing C++ random number generator. specified, C++ random number generator seeded according std::random_device. keep_burnin Whether \"burnin\" samples included cached predictions. Default FALSE. Ignored num_mcmc = 0. keep_gfr Whether \"grow--root\" samples included cached predictions. Default FALSE. Ignored num_mcmc = 0. verbose Whether print progress sampling loops. Default: FALSE. sample_sigma_global Whether update sigma^2 global error variance parameter based IG(a_global, b_global). Default: TRUE. 2. Prognostic Forest Parameters num_trees_mu Number trees prognostic forest. Default: 200. sample_sigma_leaf_mu Whether update sigma_leaf_mu leaf scale variance parameter prognostic forest based IG(a_leaf_mu, b_leaf_mu). Default: TRUE. 2.1. Tree Prior Parameters alpha_mu Prior probability splitting tree depth 0 prognostic forest. Tree split prior combines alpha beta via alpha_mu*(1+node_depth)^-beta_mu. Default: 0.95. beta_mu Exponent decreases split probabilities nodes depth > 0 prognostic forest. Tree split prior combines alpha beta via alpha_mu*(1+node_depth)^-beta_mu. Default: 2.0. min_samples_leaf_mu Minimum allowable size leaf, terms training samples, prognostic forest. Default: 5. max_depth_mu Maximum depth tree mu ensemble. Default: 10. Can overridden -1 enforce depth limits trees. 2.2. Leaf Model Parameters keep_vars_mu Vector variable names column indices denoting variables included prognostic (mu(X)) forest. Default: NULL. drop_vars_mu Vector variable names column indices denoting variables excluded prognostic (mu(X)) forest. Default: NULL. drop_vars_mu keep_vars_mu set, drop_vars_mu ignored. sigma_leaf_mu Starting value leaf node scale parameter prognostic forest. Calibrated internally 1/num_trees_mu set . a_leaf_mu Shape parameter IG(a_leaf_mu, b_leaf_mu) leaf node parameter variance model prognostic forest. Default: 3. b_leaf_mu Scale parameter IG(a_leaf_mu, b_leaf_mu) leaf node parameter variance model prognostic forest. Calibrated internally 0.5/num_trees set . 3. Treatment Effect Forest Parameters num_trees_tau Number trees treatment effect forest. Default: 50. sample_sigma_leaf_tau Whether update sigma_leaf_tau leaf scale variance parameter treatment effect forest based IG(a_leaf_tau, b_leaf_tau). Default: TRUE. 3.1. Tree Prior Parameters alpha_tau Prior probability splitting tree depth 0 treatment effect forest. Tree split prior combines alpha beta via alpha_tau*(1+node_depth)^-beta_tau. Default: 0.25. beta_tau Exponent decreases split probabilities nodes depth > 0 treatment effect forest. Tree split prior combines alpha beta via alpha_tau*(1+node_depth)^-beta_tau. Default: 3.0. min_samples_leaf_tau Minimum allowable size leaf, terms training samples, treatment effect forest. Default: 5. max_depth_tau Maximum depth tree tau ensemble. Default: 5. Can overridden -1 enforce depth limits trees. 3.2. Leaf Model Parameters a_leaf_tau Shape parameter IG(a_leaf, b_leaf) leaf node parameter variance model treatment effect forest. Default: 3. b_leaf_tau Scale parameter IG(a_leaf, b_leaf) leaf node parameter variance model treatment effect forest. Calibrated internally 0.5/num_trees set . keep_vars_tau Vector variable names column indices denoting variables included treatment effect (tau(X)) forest. Default: NULL. drop_vars_tau Vector variable names column indices denoting variables excluded treatment effect (tau(X)) forest. Default: NULL. drop_vars_tau keep_vars_tau set, drop_vars_tau ignored. 4. Conditional Variance Forest Parameters num_trees_variance Number trees (optional) conditional variance forest model. Default: 0. variance_forest_init Starting value root forest prediction conditional (heteroskedastic) error variance model. Calibrated internally log(pct_var_variance_forest_init*var((y-mean(y))/sd(y)))/num_trees_variance set. pct_var_variance_forest_init Percentage standardized outcome variance used initialize global error variance parameter. Default: 1. Superseded variance_forest_init. 4.1. Tree Prior Parameters alpha_variance Prior probability splitting tree depth 0 (optional) conditional variance model. Tree split prior combines alpha_variance beta_variance via alpha_variance*(1+node_depth)^-beta_variance. Default: 0.95. beta_variance Exponent decreases split probabilities nodes depth > 0 (optional) conditional variance model. Tree split prior combines alpha_variance beta_variance via alpha_variance*(1+node_depth)^-beta_variance. Default: 2.0. min_samples_leaf_variance Minimum allowable size leaf, terms training samples, (optional) conditional variance model. Default: 5. max_depth_variance Maximum depth tree ensemble (optional) conditional variance model. Default: 10. Can overridden -1 enforce depth limits trees. 4.2. Leaf Model Parameters a_forest Shape parameter IG(a_forest, b_forest) conditional error variance model (sampled num_trees_variance > 0). Calibrated internally num_trees_variance / 1.5^2 + 0.5 set. b_forest Scale parameter IG(a_forest, b_forest) conditional error variance model (sampled num_trees_variance > 0). Calibrated internally num_trees_variance / 1.5^2 set. keep_vars_variance Vector variable names column indices denoting variables included (optional) conditional variance forest. Default: NULL. drop_vars_variance Vector variable names column indices denoting variables excluded (optional) conditional variance forest. Default: NULL. drop_vars_variance keep_vars_variance set, drop_vars_variance ignored. 5. Random Effects Parameters rfx_prior_var Prior (diagonals ) covariance additive group-level random regression coefficients. Must vector length ncol(rfx_basis_train). Default: rep(1, ncol(rfx_basis_train))","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"List sampling outputs wrapper around sampled forests (can used -memory prediction new data, serialized JSON disk).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/bcf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run the Bayesian Causal Forest (BCF) algorithm for regularized causal effect estimation. — bcf","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 4 y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train,                   X_test = X_test, Z_test = Z_test, pi_test = pi_test) # plot(rowMeans(bcf_model$mu_hat_test), mu_test, xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") # abline(0,1,col=\"red\",lty=3,lwd=3) # plot(rowMeans(bcf_model$tau_hat_test), tau_test, xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/calibrate_inverse_gamma_error_variance.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibrate the scale parameter on an inverse gamma prior for the global error variance as in Chipman et al (2022) 1 — calibrate_inverse_gamma_error_variance","title":"Calibrate the scale parameter on an inverse gamma prior for the global error variance as in Chipman et al (2022) 1 — calibrate_inverse_gamma_error_variance","text":"1 Chipman, H., George, E., Hahn, R., McCulloch, R., Pratola, M. Sparapani, R. (2022). Bayesian Additive Regression Trees, Computational Approaches. Wiley StatsRef: Statistics Reference Online (eds N. Balakrishnan, T. Colton, B. Everitt, W. Piegorsch, F. Ruggeri J.L. Teugels). https://doi.org/10.1002/9781118445112.stat08288","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/calibrate_inverse_gamma_error_variance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibrate the scale parameter on an inverse gamma prior for the global error variance as in Chipman et al (2022) 1 — calibrate_inverse_gamma_error_variance","text":"","code":"calibrate_inverse_gamma_error_variance(   y,   X,   W = NULL,   nu = 3,   quant = 0.9,   standardize = TRUE )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/calibrate_inverse_gamma_error_variance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibrate the scale parameter on an inverse gamma prior for the global error variance as in Chipman et al (2022) 1 — calibrate_inverse_gamma_error_variance","text":"y Outcome modeled using BART, BCF another nonparametric ensemble method. X Covariates used partition trees ensemble series ensemble. W Optional Basis used define \"leaf regression\" model decision tree. \"classic\" BART model assumes constant leaf parameter, equivalent \"leaf regression\" basis ones, though necessary pass vector ones, BART function. Default: NULL. nu shape parameter global error variance's IG prior. scale parameter Sparapani et al (2021) parameterization defined nu*lambda lambda output function. Default: 3. quant Optional Quantile inverse gamma prior distribution represented linear-regression-based overestimate sigma^2. Default: 0.9. standardize Optional Whether outcome standardized ((y-mean(y))/sd(y)) calibration lambda. Default: TRUE.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/calibrate_inverse_gamma_error_variance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibrate the scale parameter on an inverse gamma prior for the global error variance as in Chipman et al (2022) 1 — calibrate_inverse_gamma_error_variance","text":"Value lambda determines scale parameter global error variance prior (sigma^2 ~ IG(nu,nu*lambda))","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/calibrate_inverse_gamma_error_variance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibrate the scale parameter on an inverse gamma prior for the global error variance as in Chipman et al (2022) 1 — calibrate_inverse_gamma_error_variance","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) y <- 10*X[,1] - 20*X[,2] + rnorm(n) nu <- 3 lambda <- calibrate_inverse_gamma_error_variance(y, X, nu = nu) sigma2hat <- mean(resid(lm(y~X))^2) mean(var(y)/rgamma(100000, nu, rate = nu*lambda) < sigma2hat) #> [1] 0.90021"},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafIndices.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute vector of forest leaf indices — computeForestLeafIndices","title":"Compute vector of forest leaf indices — computeForestLeafIndices","text":"Compute return vector representation forest's leaf predictions every observation dataset. vector \"row-major\" format can easily re-represented CSR sparse matrix: elements organized first n elements correspond leaf predictions n observations dataset first tree ensemble, next n elements correspond predictions second tree . \"data\" element corresponds uniquely mapped column index corresponds single leaf single tree (.e. tree 1 3 leaves, column indices range 0 2, tree 2's leaf indices begin 3, etc...).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafIndices.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute vector of forest leaf indices — computeForestLeafIndices","text":"","code":"computeForestLeafIndices(   model_object,   covariates,   forest_type,   forest_inds = NULL )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafIndices.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute vector of forest leaf indices — computeForestLeafIndices","text":"model_object Object type bartmodel bcf corresponding BART / BCF model least one forest sample covariates Covariates use prediction. Must dimensions / column types data used train forest. forest_type forest use model_object. Valid inputs depend model type, whether given forest sampled model. 1. BART 'mean': Extracts leaf indices mean forest 'variance': Extracts leaf indices variance forest 2. BCF 'prognostic': Extracts leaf indices prognostic forest 'treatment': Extracts leaf indices treatment effect forest 'variance': Extracts leaf indices variance forest forest_inds (Optional) Indices forest sample(s) compute leaf indices. provided, function return leaf indices every sample forest. function uses 1-indexing, first forest sample corresponds forest_num = 1, .","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafIndices.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute vector of forest leaf indices — computeForestLeafIndices","text":"List vectors. vector size num_obs * num_trees, num_obs = nrow(covariates) num_trees number trees relevant forest model_object.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafVariances.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute vector of forest leaf scale parameters — computeForestLeafVariances","title":"Compute vector of forest leaf scale parameters — computeForestLeafVariances","text":"Return forest's leaf node scale parameters. leaf scale sampled forest question, throws error leaf model stochastic scale parameter.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafVariances.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute vector of forest leaf scale parameters — computeForestLeafVariances","text":"","code":"computeForestLeafVariances(model_object, forest_type, forest_inds = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafVariances.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute vector of forest leaf scale parameters — computeForestLeafVariances","text":"model_object Object type bartmodel bcf corresponding BART / BCF model least one forest sample forest_type forest use model_object. Valid inputs depend model type, whether given forest sampled model. 1. BART 'mean': Extracts leaf indices mean forest 'variance': Extracts leaf indices variance forest 2. BCF 'prognostic': Extracts leaf indices prognostic forest 'treatment': Extracts leaf indices treatment effect forest 'variance': Extracts leaf indices variance forest forest_inds (Optional) Indices forest sample(s) compute leaf indices. provided, function return leaf indices every sample forest. function uses 1-indexing, first forest sample corresponds forest_num = 1, .","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeForestLeafVariances.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute vector of forest leaf scale parameters — computeForestLeafVariances","text":"Vector size length(forest_inds) leaf scale parameter requested forest.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeMaxLeafIndex.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute and return the largest possible leaf index computable by computeForestLeafIndices for the forests in a designated forest sample container. — computeMaxLeafIndex","title":"Compute and return the largest possible leaf index computable by computeForestLeafIndices for the forests in a designated forest sample container. — computeMaxLeafIndex","text":"Compute return largest possible leaf index computable computeForestLeafIndices forests designated forest sample container.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeMaxLeafIndex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute and return the largest possible leaf index computable by computeForestLeafIndices for the forests in a designated forest sample container. — computeMaxLeafIndex","text":"","code":"computeMaxLeafIndex(model_object, covariates, forest_type, forest_inds = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeMaxLeafIndex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute and return the largest possible leaf index computable by computeForestLeafIndices for the forests in a designated forest sample container. — computeMaxLeafIndex","text":"model_object Object type bartmodel bcf corresponding BART / BCF model least one forest sample covariates Covariates use prediction. Must dimensions / column types data used train forest. forest_type forest use model_object. Valid inputs depend model type, whether 1. BART 'mean': Extracts leaf indices mean forest 'variance': Extracts leaf indices variance forest 2. BCF 'prognostic': Extracts leaf indices prognostic forest 'treatment': Extracts leaf indices treatment effect forest 'variance': Extracts leaf indices variance forest forest_inds (Optional) Indices forest sample(s) compute leaf indices. provided, function return leaf indices every sample forest. function uses 1-indexing, first forest sample corresponds forest_num = 1, .","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/computeMaxLeafIndex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute and return the largest possible leaf index computable by computeForestLeafIndices for the forests in a designated forest sample container. — computeMaxLeafIndex","text":"Vector containing largest possible leaf index computable computeForestLeafIndices forests designated forest sample container.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBARTModelToJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the persistent aspects of a BART model to (in-memory) JSON — convertBARTModelToJson","title":"Convert the persistent aspects of a BART model to (in-memory) JSON — convertBARTModelToJson","text":"Convert persistent aspects BART model (-memory) JSON","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBARTModelToJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the persistent aspects of a BART model to (in-memory) JSON — convertBARTModelToJson","text":"","code":"convertBARTModelToJson(object)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBARTModelToJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the persistent aspects of a BART model to (in-memory) JSON — convertBARTModelToJson","text":"object Object type bartmodel containing draws BART model associated sampling outputs.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBARTModelToJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert the persistent aspects of a BART model to (in-memory) JSON — convertBARTModelToJson","text":"Object type CppJson","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBARTModelToJson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert the persistent aspects of a BART model to (in-memory) JSON — convertBARTModelToJson","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # bart_json <- convertBARTModelToJson(bart_model)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBCFModelToJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the persistent aspects of a BCF model to (in-memory) JSON — convertBCFModelToJson","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON — convertBCFModelToJson","text":"Convert persistent aspects BCF model (-memory) JSON","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBCFModelToJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON — convertBCFModelToJson","text":"","code":"convertBCFModelToJson(object)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBCFModelToJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON — convertBCFModelToJson","text":"object Object type bcf containing draws Bayesian causal forest model associated sampling outputs.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBCFModelToJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON — convertBCFModelToJson","text":"Object type CppJson","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/convertBCFModelToJson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON — convertBCFModelToJson","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,                   pi_train = pi_train, group_ids_train = group_ids_train,                   rfx_basis_train = rfx_basis_train, X_test = X_test,                   Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,                  rfx_basis_test = rfx_basis_test,                   num_gfr = 100, num_burnin = 0, num_mcmc = 100,                   sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) #> Error in bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,     pi_train = pi_train, group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,     X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = 100, num_burnin = 0,     num_mcmc = 100, sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE): unused arguments (sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) # bcf_json <- convertBCFModelToJson(bcf_model)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a list of (in-memory) JSON representations of a BART model to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJson","title":"Convert a list of (in-memory) JSON representations of a BART model to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJson","text":"Convert list (-memory) JSON representations BART model single combined BART model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a list of (in-memory) JSON representations of a BART model to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJson","text":"","code":"createBARTModelFromCombinedJson(json_object_list)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a list of (in-memory) JSON representations of a BART model to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJson","text":"json_object_list List objects type CppJson containing Json representation BART model","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a list of (in-memory) JSON representations of a BART model to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJson","text":"Object type bartmodel","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a list of (in-memory) JSON representations of a BART model to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJson","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # bart_json <- list(convertBARTModelToJson(bart_model)) # bart_model_roundtrip <- createBARTModelFromCombinedJson(bart_json)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a list of (in-memory) JSON strings that represent BART models to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJsonString","title":"Convert a list of (in-memory) JSON strings that represent BART models to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJsonString","text":"Convert list (-memory) JSON strings represent BART models single combined BART model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a list of (in-memory) JSON strings that represent BART models to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJsonString","text":"","code":"createBARTModelFromCombinedJsonString(json_string_list)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a list of (in-memory) JSON strings that represent BART models to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJsonString","text":"json_string_list List JSON strings can parsed objects type CppJson containing Json representation BART model","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a list of (in-memory) JSON strings that represent BART models to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJsonString","text":"Object type bartmodel","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromCombinedJsonString.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a list of (in-memory) JSON strings that represent BART models to a single combined BART model object which can be used for prediction, etc... — createBARTModelFromCombinedJsonString","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # bart_json_string_list <- list(saveBARTModelToJsonString(bart_model)) # bart_model_roundtrip <- createBARTModelFromCombinedJsonString(bart_json_string_list)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an (in-memory) JSON representation of a BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJson","title":"Convert an (in-memory) JSON representation of a BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJson","text":"Convert (-memory) JSON representation BART model BART model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an (in-memory) JSON representation of a BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJson","text":"","code":"createBARTModelFromJson(json_object)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an (in-memory) JSON representation of a BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJson","text":"json_object Object type CppJson containing Json representation BART model","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an (in-memory) JSON representation of a BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJson","text":"Object type bartmodel","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an (in-memory) JSON representation of a BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJson","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # bart_json <- convertBARTModelToJson(bart_model) # bart_model_roundtrip <- createBARTModelFromJson(bart_json)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonFile.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a JSON file containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonFile","title":"Convert a JSON file containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonFile","text":"Convert JSON file containing sample information trained BART model BART model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonFile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a JSON file containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonFile","text":"","code":"createBARTModelFromJsonFile(json_filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonFile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a JSON file containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonFile","text":"json_filename String filepath, must end \".json\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonFile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a JSON file containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonFile","text":"Object type bartmodel","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonFile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a JSON file containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonFile","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # saveBARTModelToJsonFile(bart_model, \"test.json\") # bart_model_roundtrip <- createBARTModelFromJsonFile(\"test.json\")"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a JSON string containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonString","title":"Convert a JSON string containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonString","text":"Convert JSON string containing sample information trained BART model BART model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a JSON string containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonString","text":"","code":"createBARTModelFromJsonString(json_string)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a JSON string containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonString","text":"json_string JSON string dump","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a JSON string containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonString","text":"Object type bartmodel","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBARTModelFromJsonString.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a JSON string containing sample information on a trained BART model to a BART model object which can be used for prediction, etc... — createBARTModelFromJsonString","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # bart_json <- saveBARTModelToJsonString(bart_model) # bart_model_roundtrip <- createBARTModelFromJsonString(bart_json) # y_hat_mean_roundtrip <- rowMeans(predict(bart_model_roundtrip, X_train)$y_hat) # plot(rowMeans(bart_model$y_hat_train), y_hat_mean_roundtrip,  #      xlab = \"original\", ylab = \"roundtrip\")"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert an (in-memory) JSON representation of a BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJson","title":"Convert an (in-memory) JSON representation of a BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJson","text":"Convert (-memory) JSON representation BCF model BCF model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert an (in-memory) JSON representation of a BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJson","text":"","code":"createBCFModelFromJson(json_object)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert an (in-memory) JSON representation of a BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJson","text":"json_object Object type CppJson containing Json representation BCF model","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert an (in-memory) JSON representation of a BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJson","text":"Object type bcf","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert an (in-memory) JSON representation of a BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJson","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,                   pi_train = pi_train, group_ids_train = group_ids_train,                   rfx_basis_train = rfx_basis_train, X_test = X_test,                   Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,                  rfx_basis_test = rfx_basis_test,                   num_gfr = 100, num_burnin = 0, num_mcmc = 100,                   sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) #> Error in bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,     pi_train = pi_train, group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,     X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = 100, num_burnin = 0,     num_mcmc = 100, sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE): unused arguments (sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) # bcf_json <- convertBCFModelToJson(bcf_model) # bcf_model_roundtrip <- createBCFModelFromJson(bcf_json)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonFile.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a JSON file containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonFile","title":"Convert a JSON file containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonFile","text":"Convert JSON file containing sample information trained BCF model BCF model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonFile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a JSON file containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonFile","text":"","code":"createBCFModelFromJsonFile(json_filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonFile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a JSON file containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonFile","text":"json_filename String filepath, must end \".json\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonFile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a JSON file containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonFile","text":"Object type bcf","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonFile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a JSON file containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonFile","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,                   pi_train = pi_train, group_ids_train = group_ids_train,                   rfx_basis_train = rfx_basis_train, X_test = X_test,                   Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,                  rfx_basis_test = rfx_basis_test,                   num_gfr = 100, num_burnin = 0, num_mcmc = 100,                   sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) #> Error in bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,     pi_train = pi_train, group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,     X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = 100, num_burnin = 0,     num_mcmc = 100, sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE): unused arguments (sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) # saveBCFModelToJsonFile(bcf_model, \"test.json\") # bcf_model_roundtrip <- createBCFModelFromJsonFile(\"test.json\")"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a JSON string containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonString","title":"Convert a JSON string containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonString","text":"Convert JSON string containing sample information trained BCF model BCF model object can used prediction, etc...","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a JSON string containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonString","text":"","code":"createBCFModelFromJsonString(json_string)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a JSON string containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonString","text":"json_string JSON string dump","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a JSON string containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonString","text":"Object type bcf","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createBCFModelFromJsonString.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a JSON string containing sample information on a trained BCF model to a BCF model object which can be used for prediction, etc... — createBCFModelFromJsonString","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,                   pi_train = pi_train, group_ids_train = group_ids_train,                   rfx_basis_train = rfx_basis_train, X_test = X_test,                   Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,                  rfx_basis_test = rfx_basis_test,                   num_gfr = 100, num_burnin = 0, num_mcmc = 100,                   sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) #> Error in bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,     pi_train = pi_train, group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,     X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = 100, num_burnin = 0,     num_mcmc = 100, sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE): unused arguments (sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) # bcf_json <- saveBCFModelToJsonString(bcf_model) # bcf_model_roundtrip <- createBCFModelFromJsonString(bcf_json)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a new (empty) C++ Json object — createCppJson","title":"Create a new (empty) C++ Json object — createCppJson","text":"Create new (empty) C++ Json object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a new (empty) C++ Json object — createCppJson","text":"","code":"createCppJson()"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a new (empty) C++ Json object — createCppJson","text":"CppJson object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonFile.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a C++ Json object from a Json file — createCppJsonFile","title":"Create a C++ Json object from a Json file — createCppJsonFile","text":"Create C++ Json object Json file","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonFile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a C++ Json object from a Json file — createCppJsonFile","text":"","code":"createCppJsonFile(json_filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonFile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a C++ Json object from a Json file — createCppJsonFile","text":"json_filename Name file read. Must end .json.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonFile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a C++ Json object from a Json file — createCppJsonFile","text":"CppJson object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a C++ Json object from a Json string — createCppJsonString","title":"Create a C++ Json object from a Json string — createCppJsonString","text":"Create C++ Json object Json string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a C++ Json object from a Json string — createCppJsonString","text":"","code":"createCppJsonString(json_string)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a C++ Json object from a Json string — createCppJsonString","text":"json_string JSON string dump","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createCppJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a C++ Json object from a Json string — createCppJsonString","text":"CppJson object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a container of forest samples — createForestContainer","title":"Create a container of forest samples — createForestContainer","text":"Create container forest samples","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a container of forest samples — createForestContainer","text":"","code":"createForestContainer(   num_trees,   output_dimension = 1,   is_leaf_constant = F,   is_exponentiated = F )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a container of forest samples — createForestContainer","text":"num_trees Number trees output_dimension Dimensionality outcome model is_leaf_constant Whether leaf constant is_exponentiated Whether forest predictions exponentiated returned","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestContainer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a container of forest samples — createForestContainer","text":"ForestSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariates.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariates","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariates","text":"Preprocess dataframe covariate values, converting categorical variables integers one-hot encoding need . Returns list including matrix preprocessed covariate values associated tracking.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariates","text":"","code":"createForestCovariates(   input_data,   ordered_cat_vars = NULL,   unordered_cat_vars = NULL )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariates","text":"input_data Dataframe matrix covariates. Users may pre-process categorical variables factors necessary. ordered_cat_vars (Optional) Vector names ordered categorical variables, vector column indices input_data matrix. unordered_cat_vars (Optional) Vector names unordered categorical variables, vector column indices input_data matrix.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariates","text":"List preprocessed data details number type variable, unique categories associated categorical variables, vector feature types needed calls BART BCF.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariates","text":"","code":"cov_df <- data.frame(x1 = 1:5, x2 = 5:1, x3 = 6:10) preprocess_list <- createForestCovariates(cov_df) X <- preprocess_list$X"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariatesFromMetadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariatesFromMetadata","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariatesFromMetadata","text":"Preprocess dataframe covariate values, converting categorical variables integers one-hot encoding need . Returns list including matrix preprocessed covariate values associated tracking.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariatesFromMetadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariatesFromMetadata","text":"","code":"createForestCovariatesFromMetadata(input_data, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariatesFromMetadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariatesFromMetadata","text":"input_data Dataframe matrix covariates. Users may pre-process categorical variables factors necessary. metadata List containing information variables, including train set categories categorical variables","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariatesFromMetadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariatesFromMetadata","text":"Preprocessed data categorical variables appropriately preprocessed","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestCovariatesFromMetadata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — createForestCovariatesFromMetadata","text":"","code":"cov_df <- data.frame(x1 = 1:5, x2 = 5:1, x3 = 6:10) metadata <- list(num_ordered_cat_vars = 0, num_unordered_cat_vars = 0,                   num_numeric_vars = 3, numeric_vars = c(\"x1\", \"x2\", \"x3\")) X_preprocessed <- createForestCovariatesFromMetadata(cov_df, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a forest dataset object — createForestDataset","title":"Create a forest dataset object — createForestDataset","text":"Create forest dataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a forest dataset object — createForestDataset","text":"","code":"createForestDataset(covariates, basis = NULL, variance_weights = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a forest dataset object — createForestDataset","text":"covariates Matrix covariates basis (Optional) Matrix bases used define leaf regression variance_weights (Optional) Vector observation-specific variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestDataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a forest dataset object — createForestDataset","text":"ForestDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a forest model object — createForestModel","title":"Create a forest model object — createForestModel","text":"Create forest model object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a forest model object — createForestModel","text":"","code":"createForestModel(   forest_dataset,   feature_types,   num_trees,   n,   alpha,   beta,   min_samples_leaf,   max_depth )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a forest model object — createForestModel","text":"forest_dataset ForestDataset object, used initialize forest sampling data structures feature_types Feature types (integers 0 = numeric, 1 = ordered categorical, 2 = unordered categorical) num_trees Number trees forest sampled n Number observations forest_dataset alpha Root node split probability tree prior beta Depth prior penalty tree prior min_samples_leaf Minimum number samples tree leaf","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createForestModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a forest model object — createForestModel","text":"ForestModel object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an outcome object — createOutcome","title":"Create an outcome object — createOutcome","text":"Create outcome object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an outcome object — createOutcome","text":"","code":"createOutcome(outcome)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an outcome object — createOutcome","text":"outcome Vector outcome values","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createOutcome.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an outcome object — createOutcome","text":"Outcome object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an R class that wraps a C++ random number generator — createRNG","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"Create R class wraps C++ random number generator","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"","code":"createRNG(random_seed = -1)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"random_seed (Optional) random seed sampling","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRNG.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an R class that wraps a C++ random number generator — createRNG","text":"CppRng object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectSamples.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a RandomEffectSamples object — createRandomEffectSamples","title":"Create a RandomEffectSamples object — createRandomEffectSamples","text":"Create RandomEffectSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectSamples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a RandomEffectSamples object — createRandomEffectSamples","text":"","code":"createRandomEffectSamples(num_components, num_groups, random_effects_tracker)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectSamples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a RandomEffectSamples object — createRandomEffectSamples","text":"num_components Number \"components\" bases defining random effects regression num_groups Number random effects groups random_effects_tracker Object type RandomEffectsTracker","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectSamples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a RandomEffectSamples object — createRandomEffectSamples","text":"RandomEffectSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsDataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a random effects dataset object — createRandomEffectsDataset","title":"Create a random effects dataset object — createRandomEffectsDataset","text":"Create random effects dataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsDataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a random effects dataset object — createRandomEffectsDataset","text":"","code":"createRandomEffectsDataset(group_labels, basis, variance_weights = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsDataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a random effects dataset object — createRandomEffectsDataset","text":"group_labels Vector group labels basis Matrix bases used define random effects regression (intercept-model, pass array ones) variance_weights (Optional) Vector observation-specific variance weights","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsDataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a random effects dataset object — createRandomEffectsDataset","text":"RandomEffectsDataset object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsModel.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a RandomEffectsModel object — createRandomEffectsModel","title":"Create a RandomEffectsModel object — createRandomEffectsModel","text":"Create RandomEffectsModel object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsModel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a RandomEffectsModel object — createRandomEffectsModel","text":"","code":"createRandomEffectsModel(num_components, num_groups)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsModel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a RandomEffectsModel object — createRandomEffectsModel","text":"num_components Number \"components\" bases defining random effects regression num_groups Number random effects groups","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsModel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a RandomEffectsModel object — createRandomEffectsModel","text":"RandomEffectsModel object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsTracker.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a RandomEffectsTracker object — createRandomEffectsTracker","title":"Create a RandomEffectsTracker object — createRandomEffectsTracker","text":"Create RandomEffectsTracker object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsTracker.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a RandomEffectsTracker object — createRandomEffectsTracker","text":"","code":"createRandomEffectsTracker(rfx_group_indices)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsTracker.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a RandomEffectsTracker object — createRandomEffectsTracker","text":"rfx_group_indices Integer indices indicating groups used define random effects","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/createRandomEffectsTracker.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a RandomEffectsTracker object — createRandomEffectsTracker","text":"RandomEffectsTracker object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bartmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bartmodel","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bartmodel","text":"Extract raw sample values random effect parameter terms.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bartmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bartmodel","text":"","code":"# S3 method for class 'bartmodel' getRandomEffectSamples(object, ...)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bartmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bartmodel","text":"object Object type bcf containing draws Bayesian causal forest model associated sampling outputs. ... parameters used random effects extraction","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bartmodel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bartmodel","text":"List arrays. alpha array dimension (num_components, num_samples) simply vector num_components = 1. xi beta arrays dimension (num_components, num_groups, num_samples) simply matrix num_components = 1. sigma array dimension (num_components, num_samples) simply vector num_components = 1.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bartmodel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bartmodel","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) E_y <- f_XW + rfx_term y <- E_y + rnorm(n, 0, 1)*(sd(E_y)/snr) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train,                     group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,                     X_test = X_test, group_ids_test = group_ids_test, rfx_basis_test = rfx_basis_test,                     num_gfr = 100, num_burnin = 0, num_mcmc = 100) rfx_samples <- getRandomEffectSamples(bart_model)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bcf.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bcf","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bcf","text":"Extract raw sample values random effect parameter terms.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bcf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bcf","text":"","code":"# S3 method for class 'bcf' getRandomEffectSamples(object, ...)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bcf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bcf","text":"object Object type bcf containing draws Bayesian causal forest model associated sampling outputs. ... parameters used random effects extraction","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bcf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bcf","text":"List arrays. alpha array dimension (num_components, num_samples) simply vector num_components = 1. xi beta arrays dimension (num_components, num_groups, num_samples) simply matrix num_components = 1. sigma array dimension (num_components, num_samples) simply vector num_components = 1.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.bcf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract raw sample values for each of the random effect parameter terms. — getRandomEffectSamples.bcf","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,                   pi_train = pi_train, group_ids_train = group_ids_train,                   rfx_basis_train = rfx_basis_train, X_test = X_test,                   Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,                  rfx_basis_test = rfx_basis_test,                   num_gfr = 100, num_burnin = 0, num_mcmc = 100,                   sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) #> Error in bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,     pi_train = pi_train, group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,     X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = 100, num_burnin = 0,     num_mcmc = 100, sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE): unused arguments (sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) rfx_samples <- getRandomEffectSamples(bcf_model) #> Error: object 'bcf_model' not found"},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic function for extracting random effect samples from a model object (BCF, BART, etc...) — getRandomEffectSamples","title":"Generic function for extracting random effect samples from a model object (BCF, BART, etc...) — getRandomEffectSamples","text":"Generic function extracting random effect samples model object (BCF, BART, etc...)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic function for extracting random effect samples from a model object (BCF, BART, etc...) — getRandomEffectSamples","text":"","code":"getRandomEffectSamples(object, ...)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic function for extracting random effect samples from a model object (BCF, BART, etc...) — getRandomEffectSamples","text":"object Fitted model object extract random effects ... parameters used random effects extraction","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/getRandomEffectSamples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic function for extracting random effect samples from a model object (BCF, BART, etc...) — getRandomEffectSamples","text":"List random effect samples","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine multiple JSON model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJson","title":"Combine multiple JSON model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJson","text":"Combine multiple JSON model objects containing forests (hierarchy / schema) single forest_container","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine multiple JSON model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJson","text":"","code":"loadForestContainerCombinedJson(json_object_list, json_forest_label)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine multiple JSON model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJson","text":"json_object_list List objects class CppJson json_forest_label Label referring particular forest (.e. \"forest_0\") overall json hierarchy (must exist every json object list)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine multiple JSON model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJson","text":"ForestSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine multiple JSON strings representing model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJsonString","title":"Combine multiple JSON strings representing model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJsonString","text":"Combine multiple JSON strings representing model objects containing forests (hierarchy / schema) single forest_container","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine multiple JSON strings representing model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJsonString","text":"","code":"loadForestContainerCombinedJsonString(json_string_list, json_forest_label)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine multiple JSON strings representing model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJsonString","text":"json_string_list List strings parse objects type CppJson json_forest_label Label referring particular forest (.e. \"forest_0\") overall json hierarchy (must exist every json object list)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerCombinedJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine multiple JSON strings representing model objects containing forests (with the same hierarchy / schema) into a single forest_container — loadForestContainerCombinedJsonString","text":"ForestSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a container of forest samples from json — loadForestContainerJson","title":"Load a container of forest samples from json — loadForestContainerJson","text":"Load container forest samples json","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a container of forest samples from json — loadForestContainerJson","text":"","code":"loadForestContainerJson(json_object, json_forest_label)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a container of forest samples from json — loadForestContainerJson","text":"json_object Object class CppJson json_forest_label Label referring particular forest (.e. \"forest_0\") overall json hierarchy","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadForestContainerJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a container of forest samples from json — loadForestContainerJson","text":"ForestSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine multiple JSON model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJson","title":"Combine multiple JSON model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJson","text":"Combine multiple JSON model objects containing random effects (hierarchy / schema) single container","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine multiple JSON model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJson","text":"","code":"loadRandomEffectSamplesCombinedJson(json_object_list, json_rfx_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine multiple JSON model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJson","text":"json_object_list List objects class CppJson json_rfx_num Integer index indicating position random effects term unpacked","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine multiple JSON model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJson","text":"RandomEffectSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine multiple JSON strings representing model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJsonString","title":"Combine multiple JSON strings representing model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJsonString","text":"Combine multiple JSON strings representing model objects containing random effects (hierarchy / schema) single container","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine multiple JSON strings representing model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJsonString","text":"","code":"loadRandomEffectSamplesCombinedJsonString(json_string_list, json_rfx_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine multiple JSON strings representing model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJsonString","text":"json_string_list List objects class CppJson json_rfx_num Integer index indicating position random effects term unpacked","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesCombinedJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine multiple JSON strings representing model objects containing random effects (with the same hierarchy / schema) into a single container — loadRandomEffectSamplesCombinedJsonString","text":"RandomEffectSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a container of random effect samples from json — loadRandomEffectSamplesJson","title":"Load a container of random effect samples from json — loadRandomEffectSamplesJson","text":"Load container random effect samples json","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a container of random effect samples from json — loadRandomEffectSamplesJson","text":"","code":"loadRandomEffectSamplesJson(json_object, json_rfx_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a container of random effect samples from json — loadRandomEffectSamplesJson","text":"json_object Object class CppJson json_rfx_num Integer index indicating position random effects term unpacked","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadRandomEffectSamplesJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a container of random effect samples from json — loadRandomEffectSamplesJson","text":"RandomEffectSamples object","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadScalarJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a scalar from json — loadScalarJson","title":"Load a scalar from json — loadScalarJson","text":"Load scalar json","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadScalarJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a scalar from json — loadScalarJson","text":"","code":"loadScalarJson(json_object, json_scalar_label, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadScalarJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a scalar from json — loadScalarJson","text":"json_object Object class CppJson json_scalar_label Label referring particular scalar / string value (.e. \"num_samples\") overall json hierarchy subfolder_name (Optional) Name subfolder / hierarchy vector sits","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadScalarJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a scalar from json — loadScalarJson","text":"R vector","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadVectorJson.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a vector from json — loadVectorJson","title":"Load a vector from json — loadVectorJson","text":"Load vector json","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadVectorJson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a vector from json — loadVectorJson","text":"","code":"loadVectorJson(json_object, json_vector_label, subfolder_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadVectorJson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a vector from json — loadVectorJson","text":"json_object Object class CppJson json_vector_label Label referring particular vector (.e. \"sigma2_samples\") overall json hierarchy subfolder_name (Optional) Name subfolder / hierarchy vector sits","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/loadVectorJson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a vector from json — loadVectorJson","text":"R vector","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotEncode.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"procedure assumes reference set observations variable (typically training set used sample forest) already one-hot encoded unique levels training set variable available (passed unique_levels). Test set observations contain categories unique_levels mapped last column matrix","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotEncode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"","code":"oneHotEncode(x_input, unique_levels)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotEncode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"x_input Vector unordered categorical data (typically either strings integers, function also accepts floating point data). unique_levels Unique values categorical variable used create initial one-hot matrix (typically training set)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotEncode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"Binary one-hot matrix","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotEncode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"","code":"x <- sample(1:8, 100, TRUE) x_test <- sample(1:9, 10, TRUE) x_onehot <- oneHotEncode(x_test, levels(factor(x)))"},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotInitializeAndEncode.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"allow prediction \"unseen\" categories test dataset, procedure pads one-hot matrix blank \"\" column. Test set observations contain categories levels(factor(x_input)) mapped column.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotInitializeAndEncode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"","code":"oneHotInitializeAndEncode(x_input)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotInitializeAndEncode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"x_input Vector unordered categorical data (typically either strings integers, function also accepts floating point data).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotInitializeAndEncode.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"List containing binary one-hot matrix unique levels input variable. unique levels used BCF BART functions.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/oneHotInitializeAndEncode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert a vector of unordered categorical data (either numeric or character labels) to a ","text":"","code":"x <- c(\"a\",\"c\",\"b\",\"c\",\"d\",\"a\",\"c\",\"a\",\"b\",\"d\") x_onehot <- oneHotInitializeAndEncode(x)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatInitializeAndPreprocess.html","id":null,"dir":"Reference","previous_headings":"","what":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatInitializeAndPreprocess","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatInitializeAndPreprocess","text":"Run simple preprocessing ordered categorical variables, converting ordered levels integers necessary, storing unique levels variable.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatInitializeAndPreprocess.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatInitializeAndPreprocess","text":"","code":"orderedCatInitializeAndPreprocess(x_input)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatInitializeAndPreprocess.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatInitializeAndPreprocess","text":"x_input Vector ordered categorical data. data already stored ordered factor, converted one using default sort order.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatInitializeAndPreprocess.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatInitializeAndPreprocess","text":"List containing preprocessed vector integer-converted ordered categorical observations unique level original ordered categorical feature.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatInitializeAndPreprocess.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatInitializeAndPreprocess","text":"","code":"x <- c(\"1. Strongly disagree\", \"3. Neither agree nor disagree\", \"2. Disagree\", \"4. Agree\", \"3. Neither agree nor disagree\", \"5. Strongly agree\", \"4. Agree\") preprocess_list <- orderedCatInitializeAndPreprocess(x) x_preprocessed <- preprocess_list$x_preprocessed"},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatPreprocess.html","id":null,"dir":"Reference","previous_headings":"","what":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatPreprocess","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatPreprocess","text":"Run simple preprocessing ordered categorical variables, converting ordered levels integers necessary, storing unique levels variable.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatPreprocess.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatPreprocess","text":"","code":"orderedCatPreprocess(x_input, unique_levels, var_name = NULL)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatPreprocess.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatPreprocess","text":"x_input Vector ordered categorical data. data already stored ordered factor, converted one using default sort order. unique_levels Vector unique levels categorical feature. var_name (Optional) Name variable.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatPreprocess.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatPreprocess","text":"List containing preprocessed vector integer-converted ordered categorical observations unique level original ordered categorical feature.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/orderedCatPreprocess.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run some simple preprocessing of ordered categorical variables, converting ordered levels to integers if necessary, and storing the unique levels of a variable. — orderedCatPreprocess","text":"","code":"x_levels <- c(\"1. Strongly disagree\", \"2. Disagree\", \"3. Neither agree nor disagree\", \"4. Agree\", \"5. Strongly agree\") x <- c(\"1. Strongly disagree\", \"3. Neither agree nor disagree\", \"2. Disagree\", \"4. Agree\", \"3. Neither agree nor disagree\", \"5. Strongly agree\", \"4. Agree\") x_processed <- orderedCatPreprocess(x, x_levels)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict from a sampled BART model on new data — predict.bartmodel","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"Predict sampled BART model new data","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"","code":"# S3 method for class 'bartmodel' predict(   bart,   X_test,   W_test = NULL,   group_ids_test = NULL,   rfx_basis_test = NULL )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"bart Object type bart containing draws regression forest associated sampling outputs. X_test Covariates used determine tree leaf predictions observation. Must passed matrix dataframe. W_test (Optional) Bases used prediction (e.g. dot product leaf values). Default: NULL. group_ids_test (Optional) Test set group labels used additive random effects model. currently support (plan near future), test set evaluation group labels training set. rfx_basis_test (Optional) Test set basis \"random-slope\" regression additive random effects model.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"List prediction matrices. model random effects, list one element – predictions forest. model random effects, list three elements – forest predictions, random effects predictions, sum (y_hat).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bartmodel.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict from a sampled BART model on new data — predict.bartmodel","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) y_hat_test <- predict(bart_model, X_test) # plot(rowMeans(y_hat_test), y_test, xlab = \"predicted\", ylab = \"actual\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict from a sampled BCF model on new data — predict.bcf","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"Predict sampled BCF model new data","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"","code":"# S3 method for class 'bcf' predict(   bcf,   X_test,   Z_test,   pi_test = NULL,   group_ids_test = NULL,   rfx_basis_test = NULL )"},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"bcf Object type bcf containing draws Bayesian causal forest model associated sampling outputs. X_test Covariates used determine tree leaf predictions observation. Must passed matrix dataframe. Z_test Treatments used prediction. pi_test (Optional) Propensities used prediction. group_ids_test (Optional) Test set group labels used additive random effects model. currently support (plan near future), test set evaluation group labels training set. rfx_basis_test (Optional) Test set basis \"random-slope\" regression additive random effects model.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"List 3-5 nrow(X_test) bcf$num_samples matrices: prognostic function estimates, treatment effect estimates, (optionally) random effects predictions, (optionally) variance forest predictions, outcome predictions.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/predict.bcf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict from a sampled BCF model on new data — predict.bcf","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 4 y <- E_XZ + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train, pi_train = pi_train) preds <- predict(bcf_model, X_test, Z_test, pi_test) # plot(rowMeans(preds$mu_hat), mu_test, xlab = \"predicted\", ylab = \"actual\", main = \"Prognostic function\") # abline(0,1,col=\"red\",lty=3,lwd=3) # plot(rowMeans(preds$tau_hat), tau_test, xlab = \"predicted\", ylab = \"actual\", main = \"Treatment effect\") # abline(0,1,col=\"red\",lty=3,lwd=3)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBartParams.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess BART parameter list. Override defaults with any provided parameters. — preprocessBartParams","title":"Preprocess BART parameter list. Override defaults with any provided parameters. — preprocessBartParams","text":"Preprocess BART parameter list. Override defaults provided parameters.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBartParams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess BART parameter list. Override defaults with any provided parameters. — preprocessBartParams","text":"","code":"preprocessBartParams(params)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBartParams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess BART parameter list. Override defaults with any provided parameters. — preprocessBartParams","text":"params Parameter list","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBartParams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess BART parameter list. Override defaults with any provided parameters. — preprocessBartParams","text":"Parameter list defaults overriden values supplied params","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBcfParams.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess BCF parameter list. Override defaults with any provided parameters. — preprocessBcfParams","title":"Preprocess BCF parameter list. Override defaults with any provided parameters. — preprocessBcfParams","text":"Preprocess BCF parameter list. Override defaults provided parameters.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBcfParams.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess BCF parameter list. Override defaults with any provided parameters. — preprocessBcfParams","text":"","code":"preprocessBcfParams(params)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBcfParams.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess BCF parameter list. Override defaults with any provided parameters. — preprocessBcfParams","text":"params Parameter list","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessBcfParams.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess BCF parameter list. Override defaults with any provided parameters. — preprocessBcfParams","text":"Parameter list defaults overriden values supplied params","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionData.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessPredictionData","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessPredictionData","text":"Preprocess covariates. DataFrames preprocessed based column types. Matrices passed assuming columns numeric.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessPredictionData","text":"","code":"preprocessPredictionData(input_data, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessPredictionData","text":"input_data Covariates, provided either dataframe matrix metadata List containing information variables, including train set categories categorical variables","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessPredictionData","text":"Preprocessed data categorical variables appropriately handled","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessPredictionData","text":"","code":"cov_df <- data.frame(x1 = 1:5, x2 = 5:1, x3 = 6:10) metadata <- list(num_ordered_cat_vars = 0, num_unordered_cat_vars = 0,                   num_numeric_vars = 3, numeric_vars = c(\"x1\", \"x2\", \"x3\")) X_preprocessed <- preprocessPredictionData(cov_df, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionDataFrame.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. — preprocessPredictionDataFrame","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. — preprocessPredictionDataFrame","text":"Preprocess dataframe covariate values, converting categorical variables integers one-hot encoding need .","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionDataFrame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. — preprocessPredictionDataFrame","text":"","code":"preprocessPredictionDataFrame(input_df, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionDataFrame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. — preprocessPredictionDataFrame","text":"input_df Dataframe covariates. Users must pre-process categorical variables factors (ordered ordered categorical). metadata List containing information variables, including train set categories categorical variables","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionDataFrame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. — preprocessPredictionDataFrame","text":"Preprocessed data categorical variables appropriately preprocessed","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionDataFrame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. — preprocessPredictionDataFrame","text":"","code":"cov_df <- data.frame(x1 = 1:5, x2 = 5:1, x3 = 6:10) metadata <- list(num_ordered_cat_vars = 0, num_unordered_cat_vars = 0,                   num_numeric_vars = 3, numeric_vars = c(\"x1\", \"x2\", \"x3\")) X_preprocessed <- preprocessPredictionDataFrame(cov_df, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess a matrix of covariate values, assuming all columns are numeric. — preprocessPredictionMatrix","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. — preprocessPredictionMatrix","text":"Preprocess matrix covariate values, assuming columns numeric.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. — preprocessPredictionMatrix","text":"","code":"preprocessPredictionMatrix(input_matrix, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. — preprocessPredictionMatrix","text":"input_matrix Covariate matrix. metadata List containing information variables, including train set categories categorical variables","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionMatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. — preprocessPredictionMatrix","text":"Preprocessed data categorical variables appropriately preprocessed","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessPredictionMatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. — preprocessPredictionMatrix","text":"","code":"cov_mat <- matrix(c(1:5, 5:1, 6:10), ncol = 3) metadata <- list(num_ordered_cat_vars = 0, num_unordered_cat_vars = 0,                   num_numeric_vars = 3, numeric_vars = c(\"x1\", \"x2\", \"x3\")) X_preprocessed <- preprocessPredictionMatrix(cov_mat, metadata)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainData.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessTrainData","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessTrainData","text":"Preprocess covariates. DataFrames preprocessed based column types. Matrices passed assuming columns numeric.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessTrainData","text":"","code":"preprocessTrainData(input_data)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessTrainData","text":"input_data Covariates, provided either dataframe matrix variable_weights Numeric weights reflecting relative probability splitting variable","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessTrainData","text":"List preprocessed (unmodified) data details number type variable, unique categories associated categorical variables, vector feature types needed calls BART BCF.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainData.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess covariates. DataFrames will be preprocessed based on their column types. Matrices will be passed through assuming all columns are numeric. — preprocessTrainData","text":"","code":"cov_mat <- matrix(1:12, ncol = 3) preprocess_list <- preprocessTrainData(cov_mat) X <- preprocess_list$X"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainDataFrame.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainDataFrame","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainDataFrame","text":"Preprocess dataframe covariate values, converting categorical variables integers one-hot encoding need . Returns list including matrix preprocessed covariate values associated tracking.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainDataFrame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainDataFrame","text":"","code":"preprocessTrainDataFrame(input_df)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainDataFrame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainDataFrame","text":"input_df Dataframe covariates. Users must pre-process categorical variables factors (ordered ordered categorical).","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainDataFrame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainDataFrame","text":"List preprocessed data details number type variable, unique categories associated categorical variables, vector feature types needed calls BART BCF.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainDataFrame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess a dataframe of covariate values, converting categorical variables to integers and one-hot encoding if need be. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainDataFrame","text":"","code":"cov_df <- data.frame(x1 = 1:5, x2 = 5:1, x3 = 6:10) preprocess_list <- preprocessTrainDataFrame(cov_df) X <- preprocess_list$X"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Preprocess a matrix of covariate values, assuming all columns are numeric. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainMatrix","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainMatrix","text":"Preprocess matrix covariate values, assuming columns numeric. Returns list including matrix preprocessed covariate values associated tracking.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainMatrix","text":"","code":"preprocessTrainMatrix(input_matrix)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainMatrix","text":"input_matrix Covariate matrix.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainMatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainMatrix","text":"List preprocessed (unmodified) data details number type variable, unique categories associated categorical variables, vector feature types needed calls BART BCF.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/preprocessTrainMatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Preprocess a matrix of covariate values, assuming all columns are numeric. Returns a list including a matrix of preprocessed covariate values and associated tracking. — preprocessTrainMatrix","text":"","code":"cov_mat <- matrix(1:12, ncol = 3) preprocess_list <- preprocessTrainMatrix(cov_mat) X <- preprocess_list$X"},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_sigma2_one_iteration.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample one iteration of the (inverse gamma) global variance model — sample_sigma2_one_iteration","title":"Sample one iteration of the (inverse gamma) global variance model — sample_sigma2_one_iteration","text":"Sample one iteration (inverse gamma) global variance model","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_sigma2_one_iteration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample one iteration of the (inverse gamma) global variance model — sample_sigma2_one_iteration","text":"","code":"sample_sigma2_one_iteration(residual, dataset, rng, a, b)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_sigma2_one_iteration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample one iteration of the (inverse gamma) global variance model — sample_sigma2_one_iteration","text":"residual Outcome class dataset ForestDataset class rng C++ random number generator Global variance shape parameter b Global variance scale parameter","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_tau_one_iteration.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","title":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","text":"Sample one iteration leaf parameter variance model (univariate basis constant leaf!)","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_tau_one_iteration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","text":"","code":"sample_tau_one_iteration(forest_samples, rng, a, b, sample_num)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/sample_tau_one_iteration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample one iteration of the leaf parameter variance model (only for univariate basis and constant leaf!) — sample_tau_one_iteration","text":"forest_samples Container forest samples rng C++ random number generator Leaf variance shape parameter b Leaf variance scale parameter sample_num Sample index","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonFile.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the persistent aspects of a BART model to (in-memory) JSON and save to a file — saveBARTModelToJsonFile","title":"Convert the persistent aspects of a BART model to (in-memory) JSON and save to a file — saveBARTModelToJsonFile","text":"Convert persistent aspects BART model (-memory) JSON save file","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonFile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the persistent aspects of a BART model to (in-memory) JSON and save to a file — saveBARTModelToJsonFile","text":"","code":"saveBARTModelToJsonFile(object, filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonFile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the persistent aspects of a BART model to (in-memory) JSON and save to a file — saveBARTModelToJsonFile","text":"object Object type bartmodel containing draws BART model associated sampling outputs. filename String filepath, must end \".json\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonFile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert the persistent aspects of a BART model to (in-memory) JSON and save to a file — saveBARTModelToJsonFile","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # saveBARTModelToJsonFile(bart_model, \"test.json\")"},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the persistent aspects of a BART model to (in-memory) JSON string — saveBARTModelToJsonString","title":"Convert the persistent aspects of a BART model to (in-memory) JSON string — saveBARTModelToJsonString","text":"Convert persistent aspects BART model (-memory) JSON string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the persistent aspects of a BART model to (in-memory) JSON string — saveBARTModelToJsonString","text":"","code":"saveBARTModelToJsonString(object)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the persistent aspects of a BART model to (in-memory) JSON string — saveBARTModelToJsonString","text":"object Object type bartmodel containing draws BART model associated sampling outputs.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert the persistent aspects of a BART model to (in-memory) JSON string — saveBARTModelToJsonString","text":"JSON string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBARTModelToJsonString.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert the persistent aspects of a BART model to (in-memory) JSON string — saveBARTModelToJsonString","text":"","code":"n <- 100 p <- 5 X <- matrix(runif(n*p), ncol = p) f_XW <- (     ((0 <= X[,1]) & (0.25 > X[,1])) * (-7.5) +      ((0.25 <= X[,1]) & (0.5 > X[,1])) * (-2.5) +      ((0.5 <= X[,1]) & (0.75 > X[,1])) * (2.5) +      ((0.75 <= X[,1]) & (1 > X[,1])) * (7.5) ) noise_sd <- 1 y <- f_XW + rnorm(n, 0, noise_sd) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] y_test <- y[test_inds] y_train <- y[train_inds] bart_model <- bart(X_train = X_train, y_train = y_train) # saveBARTModelToJsonString(bart_model)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonFile.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the persistent aspects of a BCF model to (in-memory) JSON and save to a file — saveBCFModelToJsonFile","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON and save to a file — saveBCFModelToJsonFile","text":"Convert persistent aspects BCF model (-memory) JSON save file","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonFile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON and save to a file — saveBCFModelToJsonFile","text":"","code":"saveBCFModelToJsonFile(object, filename)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonFile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON and save to a file — saveBCFModelToJsonFile","text":"object Object type bcf containing draws Bayesian causal forest model associated sampling outputs. filename String filepath, must end \".json\"","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonFile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON and save to a file — saveBCFModelToJsonFile","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,                   pi_train = pi_train, group_ids_train = group_ids_train,                   rfx_basis_train = rfx_basis_train, X_test = X_test,                   Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,                  rfx_basis_test = rfx_basis_test,                   num_gfr = 100, num_burnin = 0, num_mcmc = 100,                   sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) #> Error in bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,     pi_train = pi_train, group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,     X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = 100, num_burnin = 0,     num_mcmc = 100, sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE): unused arguments (sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) # saveBCFModelToJsonFile(bcf_model, \"test.json\")"},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonString.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert the persistent aspects of a BCF model to (in-memory) JSON string — saveBCFModelToJsonString","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON string — saveBCFModelToJsonString","text":"Convert persistent aspects BCF model (-memory) JSON string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonString.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON string — saveBCFModelToJsonString","text":"","code":"saveBCFModelToJsonString(object)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonString.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON string — saveBCFModelToJsonString","text":"object Object type bcf containing draws Bayesian causal forest model associated sampling outputs.","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonString.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON string — saveBCFModelToJsonString","text":"JSON string","code":""},{"path":"https://stochastictree.github.io/stochtree-r/reference/saveBCFModelToJsonString.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert the persistent aspects of a BCF model to (in-memory) JSON string — saveBCFModelToJsonString","text":"","code":"n <- 500 x1 <- rnorm(n) x2 <- rnorm(n) x3 <- rnorm(n) x4 <- as.numeric(rbinom(n,1,0.5)) x5 <- as.numeric(sample(1:3,n,replace=TRUE)) X <- cbind(x1,x2,x3,x4,x5) p <- ncol(X) g <- function(x) {ifelse(x[,5]==1,2,ifelse(x[,5]==2,-1,4))} mu1 <- function(x) {1+g(x)+x[,1]*x[,3]} mu2 <- function(x) {1+g(x)+6*abs(x[,3]-1)} tau1 <- function(x) {rep(3,nrow(x))} tau2 <- function(x) {1+2*x[,2]*x[,4]} mu_x <- mu1(X) tau_x <- tau2(X) pi_x <- 0.8*pnorm((3*mu_x/sd(mu_x)) - 0.5*X[,1]) + 0.05 + runif(n)/10 Z <- rbinom(n,1,pi_x) E_XZ <- mu_x + Z*tau_x snr <- 3 group_ids <- rep(c(1,2), n %/% 2) rfx_coefs <- matrix(c(-1, -1, 1, 1), nrow=2, byrow=TRUE) rfx_basis <- cbind(1, runif(n, -1, 1)) rfx_term <- rowSums(rfx_coefs[group_ids,] * rfx_basis) y <- E_XZ + rfx_term + rnorm(n, 0, 1)*(sd(E_XZ)/snr) X <- as.data.frame(X) X$x4 <- factor(X$x4, ordered = TRUE) X$x5 <- factor(X$x5, ordered = TRUE) test_set_pct <- 0.2 n_test <- round(test_set_pct*n) n_train <- n - n_test test_inds <- sort(sample(1:n, n_test, replace = FALSE)) train_inds <- (1:n)[!((1:n) %in% test_inds)] X_test <- X[test_inds,] X_train <- X[train_inds,] pi_test <- pi_x[test_inds] pi_train <- pi_x[train_inds] Z_test <- Z[test_inds] Z_train <- Z[train_inds] y_test <- y[test_inds] y_train <- y[train_inds] mu_test <- mu_x[test_inds] mu_train <- mu_x[train_inds] tau_test <- tau_x[test_inds] tau_train <- tau_x[train_inds] group_ids_test <- group_ids[test_inds] group_ids_train <- group_ids[train_inds] rfx_basis_test <- rfx_basis[test_inds,] rfx_basis_train <- rfx_basis[train_inds,] rfx_term_test <- rfx_term[test_inds] rfx_term_train <- rfx_term[train_inds] bcf_model <- bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,                   pi_train = pi_train, group_ids_train = group_ids_train,                   rfx_basis_train = rfx_basis_train, X_test = X_test,                   Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,                  rfx_basis_test = rfx_basis_test,                   num_gfr = 100, num_burnin = 0, num_mcmc = 100,                   sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) #> Error in bcf(X_train = X_train, Z_train = Z_train, y_train = y_train,     pi_train = pi_train, group_ids_train = group_ids_train, rfx_basis_train = rfx_basis_train,     X_test = X_test, Z_test = Z_test, pi_test = pi_test, group_ids_test = group_ids_test,     rfx_basis_test = rfx_basis_test, num_gfr = 100, num_burnin = 0,     num_mcmc = 100, sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE): unused arguments (sample_sigma_leaf_mu = TRUE, sample_sigma_leaf_tau = FALSE) # saveBCFModelToJsonString(bcf_model)"},{"path":"https://stochastictree.github.io/stochtree-r/reference/stochtree-package.html","id":null,"dir":"Reference","previous_headings":"","what":"stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference — stochtree-package","title":"stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference — stochtree-package","text":"Stochastic tree ensembles (XBART BART) supervised learning causal inference.","code":""},{"path":[]},{"path":"https://stochastictree.github.io/stochtree-r/reference/stochtree-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"stochtree: Stochastic tree ensembles (XBART and BART) for supervised learning and causal inference — stochtree-package","text":"Maintainer: Drew Herren drewherrenopensource@gmail.com (ORCID) Authors: Richard Hahn Jared Murray Carlos Carvalho Jingyu ","code":""}]
